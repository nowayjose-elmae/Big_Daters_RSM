---
title: "Greedy and best performance based on full and aggregated data - VS"
output: html_document
date: "2024-10-18"
editor_options: 
  markdown: 
    wrap: 72
---

### 1. LOAD LIBRARY

```{r setup, include=FALSE}
 if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )
```

### 2. DATASET

In this chunk, we are loading the full dataset with 20 items (arms) and
create an index column that we are using in our analysis later.

```{r, full dataset}
ZOZOdf <-  read.csv("C:/Users/Lenovo/Desktop/zozo_noContext_20items_tiny.csv")
ZOZO_for_sim <- ZOZOdf %>%
  select(item_id, click)

ZOZO_for_sim$index <- 1:nrow(ZOZO_for_sim)
```

### 3. SUMMARY STATS

```{r, summary stats}
summary <- summary(ZOZOdf)
print(summary)
```

Our research uses the dataset zozo_noContext_20items.csv, which involved
10000 user-item interactions gathered during a 7-day experiment on the
ZOZO platform. Each observation is an interaction that includes
information like item identification, timestamp, position of the item,
click status, propensity score and user features.

**Variables overview:**

1)  *item_id* *(item identification)* with a range from 1 to 20. The
    distribution is uniform suggesting that items were frequently
    recommended.

2)  *timestamp* which collects the date and time in which the user
    interaction was made

3)  *position*, represent the left, center, right positions -\> 1,2,3
    position respectively

4)  *click* - binary variable, represents if the user clicks (1) or not
    click (0)

5)  *Propensity score*: the likelihood that the item will be selected

6)  *user feature variables (0 to 3)*: represents sensitive user feature
    variables

### 4. DATA AGGREGATION TECHNIQUE

In this research, we decide to aggregate the data. In Part 1 of the
aggregation, we create user feature combinations then , in Part 2 we
filter and merge the agents and, in Part 3 we create the final
aggregation and merge with the original dataset.

#### PART 1 AGGREGATION - create user feature aggregation

```{r, aggregation part 1}
unique_user_features <- list(
  user_feature_0 = unique(ZOZOdf$user_feature_0),
  user_feature_1 = unique(ZOZOdf$user_feature_1),
  user_feature_2 = unique(ZOZOdf$user_feature_2),
  user_feature_3 = unique(ZOZOdf$user_feature_3)
)

# Display the number of unique values and their respective lists
for (i in names(unique_user_features)) {
  cat("\n", i, "- Number of unique values:", length(unique_user_features[[i]]), "\n")
  print(unique_user_features[[i]])
}

# Extract unique values for each user feature column
user_feature_0 <- unique(ZOZOdf$user_feature_0)
user_feature_1 <- unique(ZOZOdf$user_feature_1)
user_feature_2 <- unique(ZOZOdf$user_feature_2)
user_feature_3 <- unique(ZOZOdf$user_feature_3)

# Create all possible combinations of the user features
all_combinations <- expand.grid(
  user_feature_0 = user_feature_0,
  user_feature_1 = user_feature_1,
  user_feature_2 = user_feature_2,
  user_feature_3 = user_feature_3
)

all_combinations <- all_combinations %>%
  mutate(Agent = paste0("Agent_", row_number()))

print(all_combinations)

write.csv(all_combinations, "agent_combinations.csv", row.names = FALSE)

```

#### PART 2 AGGREGATION - merge and filter the data

```{r, aggregation part 2}
# Merge agents with the original dataset based on the user features
df_with_agents <- ZOZOdf %>%
  left_join(all_combinations, by = c("user_feature_0", "user_feature_1", "user_feature_2", "user_feature_3"))

# Group by Agent and filter agents that have at least one click (reward)
agents_with_clicks <- df_with_agents %>%
  group_by(Agent) %>%
  summarize(total_clicks = sum(click, na.rm = TRUE)) %>%
  filter(total_clicks > 0)  

# Eliminate agents with no clicks and return the final dataset
df_filtered_agents <- df_with_agents %>%
  filter(Agent %in% agents_with_clicks$Agent)

df_sorted_by_agent <- df_filtered_agents %>%
  arrange(Agent)

print(df_sorted_by_agent)

write.csv(df_sorted_by_agent, "sorted_agents_with_clicks.csv", row.names = FALSE)

unique_agent_count <- n_distinct(df_filtered_agents$Agent)
```

#### PART 3 AGGREGATION

```{r, aggregation part 3 final aggregated data}

agents_data <- read.csv("sorted_agents_with_clicks.csv", stringsAsFactors = FALSE)

columns_to_convert <- 6:10  

# Function to label encode specific columns
convert_to_numeric_labels <- function(df, cols) {
  df[, cols] <- lapply(df[, cols], function(x) {
    # Apply Label Encoding for non-numeric features
    return(as.numeric(as.factor(x)))
  })
  return(df)
}

agents_data_encoded <- convert_to_numeric_labels(agents_data, columns_to_convert)

print(agents_data_encoded)

write.csv(agents_data_encoded, "agents_data_encoded.csv", row.names = FALSE)

```

### 4. Distribution on full data

In this part of the paper, we aim to look more detailed over the
distribution of the full dataset provided.

#### 4.1 Click distribution based on full data

The first distribution is by number of clicks (graph below) and it aims
to show how many interactions result in click or not. Visibly, the
non-click is 99.8% (9980 obs.) of all observations while the clicked
items are only 0.2% (20 obs.).

```{r, distribution clicks }
click_distribution <- ZOZO_for_sim %>%
  group_by(click) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)

print(click_distribution)

ggplot(click_distribution, aes(x = factor(click), y = count, fill = factor(click))) +
  geom_bar(stat = "identity", width = 0.6) +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("0"=  "blue", "1"=  "red")) +
  scale_x_discrete(labels = c("0" = "0 = No Click", "1" = "1 = Click")) +
  labs(x = "Click Status", y = "Count", title = "Distribution of Clicks", fill = "Click Status") +
  theme_minimal(base_size = 10) +
  theme(legend.position = "top", plot.title = element_text(hjust = 0.5, face = "bold")) +
  ylim(0, max(click_distribution$count) * 1.1)


```

#### 4.2 Click rate per item based on full data

The second distribution helps us understand which is the most and least
clicked items based on historical data. The distribution presented below
shows that not all items were being clicked. The most clicked item was
number 11, while item 1,3 to 6, 12 and 17 to 20 have no visible bars
meaning that they received no clicks.

```{r, click rate per item}
item_click <- ZOZOdf %>%
  group_by(item_id) %>%
  summarise(click_rate = mean(click))

ggplot(item_click, aes(x = factor(item_id), y = click_rate)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(x = "Item ID (arm)", y = "Click Rate", title = "Click Rate by Item ID (arms)") +
  theme_minimal()

```

#### 4.3 Clicks by position based on full data

The third histogram shows which position is the best one based on
received clicks. This is position 2 with 8 clicks, followed by position
1 and 3 with 6 clicks.

```{r, clicks per position}
clicks_per_position <- ZOZOdf %>%
  group_by(position) %>%
  summarise(click_count = sum(click), .groups = "drop")

ggplot(clicks_per_position, aes(x = factor(position), y = click_count)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(x = "Position", y = "Number of Clicks", title = "Distribution of Clicks per Position") +
  theme_minimal()
```

### 4.4 Clicks by Item ID (arms) and Position on full data

The final histogram/map shows the combination of Arms and Positions.

```{r, combo map}
click_heatmap_data <- ZOZOdf%>%
  group_by(item_id, position) %>%
  summarise(click_count = sum(click))

# Create the heatmap
ggplot(click_heatmap_data, aes(x = factor(item_id), y = factor(position), fill = click_count)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red", name = "Click Count") +
  labs(x = "Item ID (arm)", y = "Position", title = "Clicks by Item ID and Position") +
  theme_minimal()

```

## I. EPSILON GREEDY ANALYSIS

## *I.I EPSILON GREEDY ANALYSIS ON FULL DATA*

### 1. epsilon-Greedy Algorithm: notation

In this part, we are implementing the epsilon-greedy algorithm which
choose the best performing arm(item_id) from our dataset. The arm that
is chosen is 11, proving that histogram 2 ("Click rate per item") allign
with the greedy algorithm.

```{r, epsilon greedy notation}
df_practice <- ZOZO_for_sim
eps <- 0 

policy_greedy <- function(df, eps, n_arms=20){
  random_uniform_variable <- runif(1, min=0, max=1)
  
  # in epsilon % of cases, pick a random arm (exploration)
  if (random_uniform_variable < eps){
    # Sample uniform random arm between 1:n_arms
    chosen_arm <- sample(1:n_arms, 1)
  } else {
     df_reward_overview <- df %>%
      drop_na() %>%
      group_by(item_id) %>%
      summarise(reward_size = sum(click, na.rm=TRUE),
                sample_size = n(),
                success_rate = reward_size / sample_size)
    
    # Pick the arm with the highest success_rate from df_reward_overview
    chosen_arm <- df_reward_overview$item_id[which.max(df_reward_overview$success_rate)]
  }
  
  # Return the chosen arm
  return(chosen_arm)
}
# Get the chosen arm from the function
chosen_arm_practice <- policy_greedy(df_practice, eps)
print(paste0('The chosen item_id/arm is: ', chosen_arm_practice))

```

### 2. sim greedy - simulates perfomance of an epsilon-greedy policy

##### Part 1: create two dataframes, one with data before start of policy, and one with data after

##### Part 2: apply epsilon-greedy algorithm, updating at interval

```{r,epsilon-greedy policy }
##PART 1
sim_greedy <- function(df, n_before_sim, n_sim, epsilon, interval=1){
  
  # define the number of observations of all data available
  n_obs <- nrow(df)

  if(n_sim > (n_obs-n_before_sim)){
    stop("The indicated size of the experiment is bigger than the data provided - shrink the size ")
  }

  # find n_before_sim random observations to be used before start policy
  index_before_sim <- sample(1:n_obs, n_before_sim)
  
  df_before_policy <- df[index_before_sim,]
  
  # save dataframe with all the results at t - to begin with those before the policy
  df_results_at_t <- df_before_policy %>%
    select(item_id, click)
  
  # create dataframe with data that we can sample from during policy
  df_during_policy <- df[-index_before_sim,]
  
  # dataframe where the results of storing the policy are stored
  df_results_policy <- data.frame(matrix(NA, nrow = n_sim, ncol = 2))
  colnames(df_results_policy) <- c('item_id', 'click')


##Part 2
  for (i in 1:n_sim){
    # update at interval
    if((i==1) || ((i %% interval)==0)){
      # Select the arm with your policy_greedy function and define the current arm
      chosen_arm <- policy_greedy(df_results_at_t, epsilon)
      current_arm <- chosen_arm
    } else {
      # In the case that we do not update, take the current arm
      chosen_arm <- current_arm
    }

    # select from the data for experiment the arm chosen
    df_during_policy_arm <- df_during_policy %>%
      filter(item_id == chosen_arm)
    
    # randomly sample from this arm and observe the reward
    if (nrow(df_during_policy_arm) == 0) {
      warning("You have run out of observations from a chosen arm")
      sampled_arm <- sample(1:n_obs, 1, replace = TRUE)
      reward <- df$click[sampled_arm]
    } else {
      sampled_arm <- sample(1:nrow(df_during_policy_arm), 1)
      reward <- df_during_policy_arm$click[sampled_arm]
    }
    
    # get a vector of results from chosen arm (item_id, reward)
    result_policy_i <- c(chosen_arm, reward)
    
    # add to dataframe to save the result
    df_results_policy[i,] <- result_policy_i
    # update the data.frame 
    df_results_at_t <- rbind(df_results_at_t, result_policy_i)
  }

  # save results in list
  results <- list(df_results_of_policy = df_results_policy,
                  df_sample_of_policy = df[-index_before_sim,])
  
  return(results)
}
```

### 3. Parameters

We decide what our parameters for epsalon-greedy analysis are. Fisrtly,
n_before_sim indicates the number of observations before the starting of
epsalon-greedy policy, we choose 300, while n_sim is times to simulate,
we choose 4000. Additionally set.seed(0) ensure that we have the same
results each time when the code is run.

```{r, parameters epsalon greedy}
n_before_sim <- 300
n_sim <- 4000

set.seed(0)
```

### 4. epsilons calculation on full data

In this part, we are caluclating the epsilon exploration values of 0.05
until 0.45 by 0.05. By analysiszng the different results, we can decide
which epsalon value have the best performance and lead to the highest
clicks (reward).

```{r, results for diffrent epsalons on full data}
result_sim_eps_5 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.05,
                                interval=1)

result_sim_eps_10 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.1,
                                interval=1)

result_sim_eps_15 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.15,
                                interval=1)

result_sim_eps_20 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.2,
                                interval=1)

result_sim_eps_25 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.25,
                                interval = 1)
result_sim_eps_30 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.3,
                                interval = 1)
result_sim_eps_35 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.35,
                                interval = 1)
result_sim_eps_40 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.40,
                                interval = 1)
result_sim_eps_45 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.45,
                                interval = 1)


```

Then, we calculate the cumulative average reward for the epsilon values
we choose. This part of the code is a crucial step in evaluating the
epsilon value performance.

```{R, cumulative average rewrad calculation on full data}

#Calculate cumulative average reward for epsilon = 0.05
cumulative_reward_5 <- cumsum(result_sim_eps_5$df_results_of_policy$click)
average_reward_5 <- cumulative_reward_5 / (n_sim)
df_eps_5 <- data.frame(t = 1:n_sim, epsilon = 0.05, avg_reward = average_reward_5)

#Calculate cumulative average reward for epsilon = 0.10
cumulative_reward_10 <- cumsum(result_sim_eps_10$df_results_of_policy$click)
average_reward_10 <- cumulative_reward_10 / (n_sim)
df_eps_10 <- data.frame(t = 1:n_sim, epsilon = 0.10, avg_reward = average_reward_10)

#Calculate cumulative average reward for epsilon = 0.15
cumulative_reward_15 <- cumsum(result_sim_eps_15$df_results_of_policy$click)
average_reward_15 <- cumulative_reward_15 / (n_sim)
df_eps_15 <- data.frame(t = 1:n_sim, epsilon = 0.15, avg_reward = average_reward_15)

#Calculate cumulative average reward for epsilon = 0.20
cumulative_reward_20 <- cumsum(result_sim_eps_20$df_results_of_policy$click)
average_reward_20 <- cumulative_reward_20 / (n_sim)
df_eps_20 <- data.frame(t = 1:n_sim, epsilon = 0.20, avg_reward = average_reward_20)

#Calculate cumulative average reward for epsilon = 0.25
cumulative_reward_25 <- cumsum(result_sim_eps_25$df_results_of_policy$click)
average_reward_25 <- cumulative_reward_25 / (n_sim)
df_eps_25 <- data.frame(t = 1:n_sim, epsilon = 0.25, avg_reward = average_reward_25)

# Calculate cumulative average reward for epsilon = 0.30
cumulative_reward_30 <- cumsum(result_sim_eps_30$df_results_of_policy$click)
average_reward_30 <- cumulative_reward_30 / (n_sim)
df_eps_30 <- data.frame(t = 1:n_sim, epsilon = 0.3, avg_reward = average_reward_30)

#Calculate cumulative average reward for epsilon = 0.35
cumulative_reward_35 <- cumsum(result_sim_eps_35$df_results_of_policy$click)
average_reward_35 <- cumulative_reward_35 / (n_sim)
df_eps_35 <- data.frame(t = 1:n_sim, epsilon = 0.35, avg_reward = average_reward_35)

#Calculate cumulative average reward for epsilon = 0.40
cumulative_reward_40 <- cumsum(result_sim_eps_40$df_results_of_policy$click)
average_reward_40 <- cumulative_reward_40 / (n_sim)
df_eps_40 <- data.frame(t = 1:n_sim, epsilon = 0.4, avg_reward = average_reward_40)

#Calculate cumulative average reward for epsilon = 0.45
cumulative_reward_45 <- cumsum(result_sim_eps_45$df_results_of_policy$click)
average_reward_45 <- cumulative_reward_45 / (n_sim)
df_eps_45 <- data.frame(t = 1:n_sim, epsilon = 0.45, avg_reward = average_reward_45)

```

Finally, we analyze the performance of the epsilons and compare them. We
use the average reward over time because this is an effective way to
evaluate which epsilon have the better performance over times.

### 5. best performed epsilon visualisation on full data

Based on the graph presented below,at time step 4000, epsilon value 0.1
reaches the highest average reward follow by 0.15, 0.25 and 0.4.

```{r, epsalons on full data}

df_all_eps <- rbind(df_eps_5,df_eps_10,df_eps_15,df_eps_20, df_eps_25, df_eps_30,df_eps_35, df_eps_40, df_eps_45)

ggplot(df_all_eps, aes(x = t, y = avg_reward, color = factor(epsilon), group = factor(epsilon))) +
  geom_line(size = 1) +
  scale_color_manual(values = c("black","purple","#2ca02c", "red", "pink","orange", "blue","yellow","brown")) +
  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
  labs(title = "Average Reward Over Time for Different Epsilon-Greedy Policies on Full Data",
       x = "Time Steps",
       y = "Average Reward",
       color = "Epsilon Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Then, we create a separate histogram with its confidence interval for
the best 4 performed epsalons.

```{r, confidence interval epasolns full data}
df_all_eps <- rbind( df_eps_10, df_eps_15, df_eps_25, df_eps_40)

ggplot(df_all_eps, aes(x = t, y = avg_reward, color = factor(epsilon), group = factor(epsilon))) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = avg_reward - 1.96 * sd(avg_reward), ymax = avg_reward + 1.96 * sd(avg_reward), fill = factor(epsilon)), alpha = 0.2, color = NA) +
  scale_color_manual(values = c( "purple", "green", "pink", "yellow")) +
  scale_fill_manual(values = c( "purple", "green", "pink","yellow")) +
  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
  labs(title = "Average Reward Over Time for Different Epsilon-Greedy Policies on Full data",
       x = "Time Steps",
       y = "Average Reward",
       color = "Epsilon Value",
       fill = "Epsilon Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

In order to check if the results are statistically significant, we
performed ANOVA which shows if there is any significant differences
between the average rewards of four best performed epsilons values.

Based on the results, we can conclude that at 5 % significance level,
there are statistically significant differences in average rewards.

```{r, ANOVA statistically significant}
df_all_rewards <- rbind(df_eps_10, df_eps_15, df_eps_25, df_eps_40) 
df_all_rewards$epsilon <- as.factor(df_all_rewards$epsilon)

anova_result <- aov(avg_reward ~ epsilon, data = df_all_rewards)
summary(anova_result)

```

## *I.II EPSILON GREEDY FOR AGGREGATED DATA*

Now, in this part of the analysis, we performed the epsilon greedy but
on aggregated data.

### 1. epsilon-Greedy Algorithm: notation on aggregated data

```{r}
ZOZOdf_Aggregate <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

ZOZO_for_sim_aggregate <- ZOZOdf_Aggregate %>%
  select(item_id, click)

ZOZO_for_sim_aggregate$index <- 1:nrow(ZOZO_for_sim_aggregate)

df_practice_aggregate <- ZOZO_for_sim_aggregate
eps <- 0 

policy_greedy <- function(df, eps, n_arms=20){
  random_uniform_variable <- runif(1, min=0, max=1)
  
  # in epsilon % of cases, pick a random arm (exploration)
  if (random_uniform_variable < eps){
    # Sample uniform random arm between 1:n_arms
    chosen_arm <- sample(1:n_arms, 1)
  } else {
     df_reward_overview <- df %>%
      drop_na() %>%
      group_by(item_id) %>%
      summarise(reward_size = sum(click, na.rm=TRUE),
                sample_size = n(),
                success_rate = reward_size / sample_size)
    
    # Pick the arm with the highest success_rate from df_reward_overview
    chosen_arm <- df_reward_overview$item_id[which.max(df_reward_overview$success_rate)]
  }
  
  # Return the chosen arm
  return(chosen_arm)
}
# Get the chosen arm from the function
chosen_arm_practice <- policy_greedy(df_practice_aggregate, eps)
print(paste0('The chosen item_id/arm is: ', chosen_arm_practice))

```

### 2. sim greedy - simulates perfomance of an epsilon-greedy policy on aggregated data

##### Part 1: create two dataframes, one with data before start of policy, and one with data after

##### Part 2: apply epsilon-greedy algorithm, updating at interval

```{r, aggregated data epsilon}
##PART 1
sim_greedy <- function(df, n_before_sim, n_sim, epsilon, interval=1){
  
  # define the number of observations of all data available
  n_obs <- nrow(df)

  if(n_sim > (n_obs-n_before_sim)){
    stop("The indicated size of the experiment is bigger than the data provided - shrink the size ")
  }

  # find n_before_sim random observations to be used before start policy
  index_before_sim <- sample(1:n_obs, n_before_sim)
  
  df_before_policy <- df[index_before_sim,]
  
  # save dataframe with all the results at t - to begin with those before the policy
  df_results_at_t <- df_before_policy %>%
    select(item_id, click)
  
  # create dataframe with data that we can sample from during policy
  df_during_policy <- df[-index_before_sim,]
  
  # dataframe where the results of storing the policy are stored
  df_results_policy <- data.frame(matrix(NA, nrow = n_sim, ncol = 2))
  colnames(df_results_policy) <- c('item_id', 'click')


##Part 2
  for (i in 1:n_sim){
    # update at interval
    if((i==1) || ((i %% interval)==0)){
      # Select the arm with your policy_greedy function and define the current arm
      chosen_arm <- policy_greedy(df_results_at_t, epsilon)
      current_arm <- chosen_arm
    } else {
      # In the case that we do not update, take the current arm
      chosen_arm <- current_arm
    }

    # select from the data for experiment the arm chosen
    df_during_policy_arm <- df_during_policy %>%
      filter(item_id == chosen_arm)
    
    # randomly sample from this arm and observe the reward
    if (nrow(df_during_policy_arm) == 0) {
      warning("You have run out of observations from a chosen arm")
      sampled_arm <- sample(1:n_obs, 1, replace = TRUE)
      reward <- df$click[sampled_arm]
    } else {
      sampled_arm <- sample(1:nrow(df_during_policy_arm), 1)
      reward <- df_during_policy_arm$click[sampled_arm]
    }
    
    # get a vector of results from chosen arm (item_id, reward)
    result_policy_i <- c(chosen_arm, reward)
    
    # add to dataframe to save the result
    df_results_policy[i,] <- result_policy_i
    # update the data.frame 
    df_results_at_t <- rbind(df_results_at_t, result_policy_i)
  }

  # save results in list
  results <- list(df_results_of_policy = df_results_policy,
                  df_sample_of_policy = df[-index_before_sim,])
  
  return(results)
}
  
 

```

### 3. Parameters

We keep the same parameters as the first epsilon analysis.

```{r, parameters}
n_before_sim <- 300
n_sim <- 4000

set.seed(0)
```

### 4. epsilons calculation on aggregated data

In this part, we are caluclating the epsilon exploration values of 0.05
until 0.45 by 0.05 but on aggregated data.

```{r, results for diffrent epsalons with aggregated data}
result_sim_agg_eps_5 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.05,
                                interval=1)

result_sim_agg_eps_10 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.1,
                                interval=1)

result_sim_agg_eps_15 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.15,
                                interval=1)

result_sim_agg_eps_20 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.2,
                                interval=1)

result_sim_agg_eps_25 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.25,
                                interval = 1)
result_sim_agg_eps_30 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.3,
                                interval = 1)
result_sim_agg_eps_35 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.35,
                                interval = 1)
result_sim_agg_eps_40 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.40,
                                interval = 1)
result_sim_agg_eps_45 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.45,
                                interval = 1)


```

Then, we calculate the cumulative average reward for the epsilon values
we choose. This part of the code is a crucial step in evaluating the
epsilon value performance.

```{R, cumulative average rewrad calculation on aggregated data}
# Calculate cumulative average reward for epsilon = 0.05
cumulative_reward_5 <- cumsum(result_sim_agg_eps_5$df_results_of_policy$click)
average_reward_5 <- cumulative_reward_5 / (n_sim)
df_eps_5_agg <- data.frame(t = 1:n_sim, epsilon = 0.05, avg_reward = average_reward_5)

# Calculate cumulative average reward for epsilon = 0.10
cumulative_reward_10 <- cumsum(result_sim_agg_eps_10$df_results_of_policy$click)
average_reward_10 <- cumulative_reward_10 / (n_sim)
df_eps_10_agg <- data.frame(t = 1:n_sim, epsilon = 0.10, avg_reward = average_reward_10)

# Calculate cumulative average reward for epsilon = 0.15
cumulative_reward_15 <- cumsum(result_sim_agg_eps_15$df_results_of_policy$click)
average_reward_15 <- cumulative_reward_15 / (n_sim)
df_eps_15_agg <- data.frame(t = 1:n_sim, epsilon = 0.15, avg_reward = average_reward_15)

# Calculate cumulative average reward for epsilon = 0.20
cumulative_reward_20 <- cumsum(result_sim_agg_eps_20$df_results_of_policy$click)
average_reward_20 <- cumulative_reward_20 / (n_sim)
df_eps_20_agg <- data.frame(t = 1:n_sim, epsilon = 0.20, avg_reward = average_reward_20)

# Calculate cumulative average reward for epsilon = 0.25
cumulative_reward_25 <- cumsum(result_sim_agg_eps_25$df_results_of_policy$click)
average_reward_25 <- cumulative_reward_25 / (n_sim)
df_eps_25_agg <- data.frame(t = 1:n_sim, epsilon = 0.25, avg_reward = average_reward_25)

# Calculate cumulative average reward for epsilon = 0.30
cumulative_reward_30 <- cumsum(result_sim_agg_eps_30$df_results_of_policy$click)
average_reward_30 <- cumulative_reward_30 / (n_sim)
df_eps_30_agg <- data.frame(t = 1:n_sim, epsilon = 0.3, avg_reward = average_reward_30)

# Calculate cumulative average reward for epsilon = 0.35
cumulative_reward_35 <- cumsum(result_sim_agg_eps_35$df_results_of_policy$click)
average_reward_35 <- cumulative_reward_35 / (n_sim)
df_eps_35_agg <- data.frame(t = 1:n_sim, epsilon = 0.35, avg_reward = average_reward_35)

# Calculate cumulative average reward for epsilon = 0.40
cumulative_reward_40 <- cumsum(result_sim_agg_eps_40$df_results_of_policy$click)
average_reward_40 <- cumulative_reward_40 / (n_sim)
df_eps_40_agg <- data.frame(t = 1:n_sim, epsilon = 0.4, avg_reward = average_reward_40)

# Calculate cumulative average reward for epsilon = 0.45
cumulative_reward_45 <- cumsum(result_sim_agg_eps_45$df_results_of_policy$click)
average_reward_45 <- cumulative_reward_45 / (n_sim)
df_eps_45_agg <- data.frame(t = 1:n_sim, epsilon = 0.45, avg_reward = average_reward_45)

```

### 5. best performed epsilon visualization on aggregated data

Finally, we analyze the performance of the epsilons and compare them
based on aggregated data. We use the average reward over time because
this is an effective way to evaluate which epsilon have the better
perdomance over times.

```{r, visualisation on aggregated data}
# Combine all data frames into one
df_all_eps <- rbind(df_eps_5_agg,df_eps_10_agg,df_eps_15_agg,df_eps_20_agg, df_eps_25_agg, df_eps_30_agg,df_eps_35_agg, df_eps_40_agg, df_eps_45_agg)

ggplot(df_all_eps, aes(x = t, y = avg_reward, color = factor(epsilon), group = factor(epsilon))) +
  geom_line(size = 1) +
  scale_color_manual(values = c("black","purple","#2ca02c", "red", "pink","orange", "blue","yellow","brown")) +
  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
  labs(title = "Average Reward Over Time for Different Epsilon-Greedy Policies with Agrregated Data",
       x = "Time Steps",
       y = "Average Reward",
       color = "Epsilon Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Based on the graph presented above ,at time step 4000, epsilon value 0.4
reaches the highest average reward follow by 0.2, 0.25 and 0.35.

### 6. BEST ITEM, BEST POSITION based on aggregated data

##### 6.1 Best Item Recommendation with Epsilon-Greedy

In this part of the epsilon analysis based on aggregated data, we aim to
recommend the best item for each user on the historical data.

```{r, Best Item Recommendation with Epsilon-Greedy}
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))

# Initialize matrices to store rewards (clicks) and trials (attempts)
rewards <- matrix(0, nrow = n_users, ncol = n_items)
trials <- matrix(0, nrow = n_users, ncol = n_items)

# Create a mapping from user and item to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))
item_map <- as.numeric(factor(agents_data$item_id))

# Implement the epsilon-greedy function
epsilon_greedy <- function(epsilon, rewards, trials, n_items) {
  if (runif(1) < epsilon) {
    # Explore: Choose a random item
    return(sample(1:n_items, 1))
  } else {
    # Exploit: Choose the item with the highest average reward
    mean_rewards <- rewards / pmax(trials, 1)  # Avoid division by zero
    return(which.max(mean_rewards))
  }
}

# Step 4: Simulate epsilon-greedy for each user-item interaction
n_rounds <- 1000
epsilon <- 0.4 #this is the best performed epsilon) 

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Choose an item using epsilon-greedy
    chosen_item <- epsilon_greedy(epsilon, rewards[user, ], trials[user, ], n_items)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    click_data <- user_data[user_data$item_id == unique(agents_data$item_id)[chosen_item], "click"]
    
    if (length(click_data) > 0) {
      # Update rewards and trials based on clicks
      reward <- sum(click_data)
      rewards[user, chosen_item] <- rewards[user, chosen_item] + reward
      trials[user, chosen_item] <- trials[user, chosen_item] + 1
    }
  }
}

best_item_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Display the results
results <- data.frame(
  Agent = unique(agents_data$Agent), 
  Best_Item = unique(agents_data$item_id)[best_item_for_user]
)
print(results)
```

The interpretation of the table above suggest that different users have
different preferences for best item based on episilon greedy analysis.

##### **6.2 Best Position Recommendation with Epsilon-Greedy**

This part suggests the best position for each agent based on historical
data

```{r, Best Position Recommendation with Epsilon-Greedy}
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

n_users <- length(unique(agents_data$Agent))
n_positions <- length(unique(agents_data$position))

# Initialize matrices to store rewards (clicks) and trials (attempts)
rewards <- matrix(0, nrow = n_users, ncol = n_positions)
trials <- matrix(0, nrow = n_users, ncol = n_positions)

# Create a mapping from user and position to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))
position_map <- as.numeric(factor(agents_data$position))

#Simulate epsilon-greedy for each user-position interaction
n_rounds <- 1000
epsilon <- 0.4

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Choose a position using epsilon-greedy
    chosen_position <- epsilon_greedy(epsilon, rewards[user, ], trials[user, ], n_positions)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    click_data <- user_data[user_data$position == unique(agents_data$position)[chosen_position], "click"]
    
    if (length(click_data) > 0) {
      reward <- sum(click_data)
      rewards[user, chosen_position] <- rewards[user, chosen_position] + reward
      trials[user, chosen_position] <- trials[user, chosen_position] + 1
    }
  }
}

#  Identify the best position for each user
best_position_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

#  Display the results
results <- data.frame(
  Agent = unique(agents_data$Agent), 
  Best_Position = unique(agents_data$position)[best_position_for_user]
)
print(results)

```

The table above present the best position for each agent. Overall, we
can see that the position 2 is the most common one (similarly to the
distribution per position histogram).

##### **6.3 Best Item-Position Combination with Epsilon-Greedy**

```{r,  Best Item-Position Combination with Epsilon-Greedy}

agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

#Initialize variables
n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))
n_positions <- length(unique(agents_data$position))

# Create all possible combinations of item_id and position
item_position_combinations <- expand.grid(item_id = unique(agents_data$item_id), position = unique(agents_data$position))
n_combinations <- nrow(item_position_combinations)

# Initialize matrices to store rewards and trials
rewards <- matrix(0, nrow = n_users, ncol = n_combinations)
trials <- matrix(0, nrow = n_users, ncol = n_combinations)

# Function to map a specific item_id and position to its combination index
get_combination_index <- function(item_id, position) {
  which(item_position_combinations$item_id == item_id & item_position_combinations$position == position)
}

#Simulate epsilon-greedy for each user-item_position combination interaction
n_rounds <- 1000
epsilon <- 0.4

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    chosen_combination <- epsilon_greedy(epsilon, rewards[user, ], trials[user, ], n_combinations)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    chosen_item <- item_position_combinations$item_id[chosen_combination]
    chosen_position <- item_position_combinations$position[chosen_combination]
    
    click_data <- user_data[user_data$item_id == chosen_item & user_data$position == chosen_position, "click"]
    
    if (length(click_data) > 0) {
      reward <- sum(click_data)
      rewards[user, chosen_combination] <- rewards[user, chosen_combination] + reward
      trials[user, chosen_combination] <- trials[user, chosen_combination] + 1
    }
  }
}

# Identify the best item-position combination for each user
best_combination_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Display the results
best_item_position <- item_position_combinations[best_combination_for_user, ]
results <- data.frame(Agent = unique(agents_data$Agent), Best_Item = best_item_position$item_id, Best_Position = best_item_position$position)

print(results)

```
