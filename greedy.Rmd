---
title: "final try"
output: html_document
date: "2024-10-06"
---

```{r setup, include=FALSE}
# Step 1: Load Libraries
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, tidyr, ggplot2, reshape2, latex2exp)

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
##using already the big dataset -> you can replace with smaller one but then 
ZOZOdf <- "zozo_Context_80items.csv"
ZOZOdf <- read.csv(ZOZOdf)

ZOZO_for_sim <- ZOZOdf %>%
  select(item_id, click)

ZOZO_for_sim$index <- 1:nrow(ZOZO_for_sim)
```

###check distribution
```{r }
click_distribution <- ZOZO_for_sim %>%
  group_by(click) %>%
  summarise(count = n())

```

##epsalon greedy notation

```{r}
df_practice <- ZOZO_for_sim[sample(1:nrow(ZOZO_for_sim), 5000), ] #get random observations

# Set value of epsilon parameter
eps <- 0

###if you use another dataset -> change the arms
policy_greedy <- function(df, eps, n_arms=80){
  # Draws a random float between 0 and 1 from a uniform distribution
  random_uniform_variable <- runif(1, min=0, max=1)
  
  # in epsilon % of cases, pick a random arm (exploration)
  if (random_uniform_variable < eps){
    # Sample uniform random arm between 1:n_arms
    chosen_arm <- sample(1:n_arms, 1)
  } else {
     df_reward_overview <- df %>%
      drop_na() %>%
      group_by(item_id) %>%
      summarise(reward_size = sum(click, na.rm=TRUE),
                sample_size = n(),
                success_rate = reward_size / sample_size)
    
    # Pick the arm with the highest success_rate from df_reward_overview
    chosen_arm <- df_reward_overview$item_id[which.max(df_reward_overview$success_rate)]
  }
  
  # Return the chosen arm
  return(chosen_arm)
}
# Get the chosen arm from the function
chosen_arm_practice <- policy_greedy(df_practice, eps)
print(paste0('The chosen arm is: ', chosen_arm_practice))
```


##sim greedy 
```{r}
sim_greedy <- function(df, n_before_sim, n_sim, epsilon, interval=1){
  
  # define the number of observations of all data available
  n_obs <- nrow(df)

   # Give user a warning: the size of the intended experiment is bigger than the data provided
  if(n_sim > (n_obs-n_before_sim)){
    stop("The indicated size of the experiment is bigger than the data provided - shrink the size ")
  }

  # find n_before_sim random observations to be used before start policy
  index_before_sim <- sample(1:n_obs, n_before_sim)
  
  # using indexing, create dataframe with data before start policy
  df_before_policy <- df[index_before_sim,]
  
  # save dataframe with all the results at t - to begin with those before the policy
  df_results_at_t <- df_before_policy %>%
    select(item_id, click)
  
  # create dataframe with data that we can sample from during policy
  df_during_policy <- df[-index_before_sim,]
  
  # dataframe where the results of storing the policy are stored
  df_results_policy <- data.frame(matrix(NA, nrow = n_sim, ncol = 2))
  colnames(df_results_policy) <- c('item_id', 'click')
  
  ## part 2: apply epsilon-greedy algorithm, updating at interval
  for (i in 1:n_sim){
    # update at interval
    if((i==1) || ((i %% interval)==0)){
      # Select the arm with your policy_greedy function and define the current arm
      chosen_arm <- policy_greedy(df_results_at_t, epsilon)
      current_arm <- chosen_arm
    } else {
      # In the case that we do not update, take the current arm
      chosen_arm <- current_arm
    }

    # select from the data for experiment the arm chosen
    df_during_policy_arm <- df_during_policy %>%
      filter(item_id == chosen_arm)
    
    # randomly sample from this arm and observe the reward
    if (nrow(df_during_policy_arm) == 0) {
      warning("You have run out of observations from a chosen arm")
      sampled_arm <- sample(1:n_obs, 1, replace = TRUE)
      reward <- df$click[sampled_arm]
    } else {
      sampled_arm <- sample(1:nrow(df_during_policy_arm), 1)
      reward <- df_during_policy_arm$click[sampled_arm]
    }
    
    # get a vector of results from chosen arm (item_id, reward)
    result_policy_i <- c(chosen_arm, reward)
    
    # add to dataframe to save the result
    df_results_policy[i,] <- result_policy_i
    # update the data.frame 
    df_results_at_t <- rbind(df_results_at_t, result_policy_i)
  }

  # save results in list
  results <- list(df_results_of_policy = df_results_policy,
                  df_sample_of_policy = df[-index_before_sim,])
  
  return(results)
}

```

##parameters
```{r}
n_before_sim <- 500
n_sim <- 4500

# Set the seed for reproducibility
set.seed(0)
```

```{r}
result_sim_eps_04 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim = n_before_sim,
                                n_sim = n_sim,
                                epsilon = 0.4,
                                interval = 1)


result_sim_eps_025 <- sim_greedy(ZOZO_for_sim,
                                 n_before_sim = n_before_sim,
                                 n_sim = n_sim,
                                 epsilon = 0.25,
                                 interval = 1)

result_sim_eps_02 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim = n_before_sim,
                                n_sim = n_sim,
                                epsilon = 0.2,
                                interval = 1)




```


```{r}
# Create a dataframe with the results per epsilon and calculate the cumulative sum over time
df_result_sims <- data.frame(t = 1:n_sim,
                             eps_04 = cumsum(result_sim_eps_04$df_results_of_policy$click),
                             eps_025 = cumsum(result_sim_eps_025$df_results_of_policy$click))

# Melt the dataframe for ggplot visualization
df_result_sims_melted <- melt(df_result_sims, id = 't')

# Create a dataframe with the results per epsilon and calculate the cumulative sum over time
df_result_sims <- data.frame(t = 1:n_sim,
                             eps_025 = cumsum(result_sim_eps_025$df_results_of_policy$click),
                             eps_02 = cumsum(result_sim_eps_02$df_results_of_policy$click))

# Melt the dataframe for ggplot visualization
df_result_sims_melted <- melt(df_result_sims, id = 't')
```

##PLOT 
```{r}
epsilon_comparison <- ggplot(data = df_result_sims_melted, aes(x = t, y = value, col = variable)) +
  geom_line() +
  theme_bw() +
  xlab('Time') +
  ylab('Total Reward') +
  scale_color_manual(name = expression(epsilon),
                     values = c("darkblue", "red"),
                     labels = c('0.4', '0.25'))

print(epsilon_comparison)
```


```{r}
epsilon_comparison <- ggplot(data = df_result_sims_melted, aes(x = t, y = value, col = variable)) +
  geom_line() +
  theme_bw() +
  xlab('Time') +
  ylab('Total Reward') +
  scale_color_manual(name = expression(epsilon),
                     values = c("red", "blue"),
                     labels = c('0.25', '0.2'))

print(epsilon_comparison)
```

###Calculate regret
```{r}
set.seed(0)

# Total reward for epsilon = 0.25
total_reward_eps_025 <- tail(df_result_sims$eps_025, n = 1)

df_result_per_arm_sample_025 <- result_sim_eps_025$df_sample_of_policy %>%
  group_by(item_id) %>%
  summarise(avg_reward = mean(click))

avg_reward_best_arm_025 <- max(df_result_per_arm_sample_025$avg_reward)

total_reward_best_arm_025 <- round(n_sim * avg_reward_best_arm_025, 0)

regret_025 <- total_reward_best_arm_025 - total_reward_eps_025

# Total reward for epsilon = 0.2
total_reward_eps_02 <- tail(df_result_sims$eps_02, n = 1)

df_result_per_arm_sample_02 <- result_sim_eps_02$df_sample_of_policy %>%
  group_by(item_id) %>%
  summarise(avg_reward = mean(click))

avg_reward_best_arm_02 <- max(df_result_per_arm_sample_02$avg_reward)

total_reward_best_arm_02 <- round(n_sim * avg_reward_best_arm_02, 0)

regret_02 <- total_reward_best_arm_02 - total_reward_eps_02


# Total reward for epsilon = 0.4
total_reward_eps_04 <- tail(df_result_sims$eps_04, n = 1)

# Obtain the average reward per arm for epsilon = 0.4
df_result_per_arm_sample_04 <- result_sim_eps_04$df_sample_of_policy %>%
  group_by(item_id) %>%
  summarise(avg_reward = mean(click))

avg_reward_best_arm_04 <- max(df_result_per_arm_sample_04$avg_reward)

total_reward_best_arm_04 <- round(n_sim * avg_reward_best_arm_04, 0)

# Regret for epsilon = 0.4
regret_04 <- total_reward_best_arm_04 - total_reward_eps_04
```

