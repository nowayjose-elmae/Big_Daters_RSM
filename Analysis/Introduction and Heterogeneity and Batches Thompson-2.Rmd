---
title: "Bernoulli Thompson Sampling 1 Final"
authors: Catarina Casaca
output: html_document
date: "2024-10-02"
---

# Introduction

Decision-making problems, where an agent must choose the best action from several alternatives at each point in time, are widely encountered in practical applications, such as when companies decide how to structure their websites to encourage shopping behavior. These problems involve a trade-off between exploration and exploitation (Lai & Robbins, 1985; Auer et al., 2002). This trade-off is typically formulated as the multi-armed bandit problem, where each possible action, or arm, has an unknown reward probability distribution, and the agent aims to maximize cumulative rewards over time by selecting the optimal arm (Bouneffouf & Rish, 2019).

In this assignment, we apply the MAB framework to a real-world dataset from Zozo, a high-traffic fashion website. Here, the arms correspond to different fashion items, and the rewards are based on user clicks on the advertisement banner. The goal is to determine which items are best to recommend and identify the most effective positions on the recommendation interface (left, center, or right) to maximize click-through rates (CTR).

To achieve this, we will first describe and analyze the dataset. Then, we will test several MAB models, including Thompson Sampling, ε-greedy, and Upper Confidence Bound (UCB), to identify the best-performing items and banner positions for maximizing clicks. Additionally, we will perform a heterogeneity analysis to examine how different user segments impact the models’ performance, a batch analysis to assess how varying batch sizes affect outcomes, and a parameter analysis to improve the models.

The dataset captures user interactions with recommended fashion items and includes variables such as timestamps of impressions, item IDs, positions on the interface, and click indicator. It also contains propensity scores, which represent the probability of an item being recommended in each position, along with several user-related features such as age and gender.

# Download Packages

```{r setup, include=FALSE}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

# Download Dataset

```{r}
# this version contains 80 arms (ZOZO)
set.seed(0)
library(readr)
dfZozo_80 <- read_csv("~/Downloads/zozo_Context_80items.csv.zip")%>%
  select(item_id, click, position, user_feature_0, user_feature_1,user_feature_2, user_feature_3)
  


```

# Thompson Sampling: notation

```{=tex}
\begin{enumerate}
  \item The reward $Q_t(a)$ for each arm follows a beta distribution. At the start, at $t = 0$, every arm is assumed to follow the same beta distribution with parameters $\alpha_0 = 1$ and $\beta_0 = 1$. This means we initially assume each arm has an equal probability of giving a reward. 
  \item At each time $t$, the algorithm samples $n_{\mathrm{sample}}$ observations from each of the distributions. The arm which has the highest average reward according to the sample is the chosen arm. 
  \item We then observe the reward $r_t$, and update the parameters of the distribution accordingly. 
  \begin{itemize}
    \item $\alpha_t = \alpha_{t-1} + r_t$
    \item $\beta_t = \beta_{t-1} + 1 - r_t$
  \end{itemize}
\end{enumerate}
```
# Heterogeneity and Aggregation Analysis

To perform a meaningful heterogeneity and aggregation analysis, our first step is to identify potential clusters within the dataset. This involves grouping the data based on various features and selecting the one that provides the most useful segmentation. Ideally, with a detailed description of the features, we could create more targeted segments, such as isolating middle-aged women . However, since we lack detailed information, we will focus on choosing the feature that best divides the population into meaningful subgroups.

What we are looking for is a feature that on one hand does not create too many subgroups, which could dilute our analysis, and on the other hand ensures that each subgroup has a substantial portion of the sample. This will help us avoid insignificant results and ensure that our analysis captures meaningful differences across segments.

```{r}
dfZozo_80 %>%
  group_by(user_feature_0) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_1) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_2) %>%
  summarise(n())


dfZozo_80 %>%
  group_by(user_feature_3) %>%
  summarise(n())
```

Since some user features, such as features 1, 2, and 3, contain very small sample sizes (e.g., values as low as 16), including these in our analysis would lead to unreliable estimates and reduce the statistical power of the results. Therefore, we are excluding these user features from the analysis to ensure the focus is on groups with larger, more balanced sample sizes, such as feature 0. Now, we divide feature 0 into the four previously identified segments, by creating 4 datasets.

```{r}
dfZozo_seg0 <- dfZozo_80%>%
  filter(user_feature_0 == "4ae385d792f81dde128124a925a830de")

dfZozo_seg1 <- dfZozo_80%>%
  filter(user_feature_0 == "574464659df0fc5bac579eff2b1fff99")

dfZozo_seg2 <- dfZozo_80%>%
  filter(user_feature_0 == "81ce123cbb5bd8ce818f60fb3586bba5")

dfZozo_seg3 <- dfZozo_80%>%
  filter(user_feature_0 == "cef3390ed299c09874189c387777674a")
```

Now we run Thompson Sampling simulations on each of the four segmented datasets based on feature 0 to evaluate how the model performs across different segments of users. We define the datasets in a list and set simulation parameters, such as a horizon size of 6641 impressions and 10 simulation runs per dataset. We set the horizon size to 6641, which matches the number of observations in the smallest segment of the dataset based on user_feature_0. By doing this, we ensure that each segment is treated equally during the simulations, preventing over-representation of larger segments. After running the simulations, we calculate the maximum number of observations (t) in each simulation by grouping the results by the simulation and summarizing the largest t value. Identifying the maximum t for each simulation is necessary to determine the length of each simulation run.

```{r}
# Define the feature datasets in a list
feature_0_datasets <- list(dfZozo_seg0, dfZozo_seg1, dfZozo_seg2, dfZozo_seg3)

# Define a vector with dataset names for better result tracking
dataset_names <- c("dfZozo_seg0", "dfZozo_seg1", "dfZozo_seg2", "dfZozo_seg3")

# Parameters for the simulations
size_sim <- 6641
n_sim <- 10

# Store results for each dataset
results_list <- list()

# Loop through each feature dataset and run the TS simulation
for (i in seq_along(feature_0_datasets)) {
  # Create the bandit object for the current dataset
  bandit <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                             data = feature_0_datasets[[i]],
                                             randomize = FALSE)
  
  # Define the Thompson Sampling policy
  TS <- ThompsonSamplingPolicy$new()
  
  # Create the agent
  agent <- Agent$new(TS, bandit)
  
  # Create the simulator
  simulator <- Simulator$new(agent,
                             horizon = size_sim,
                             do_parallel = TRUE,
                             simulations = n_sim)
  
  # Run the simulator
  history <- simulator$run()
  
  # Gather the results and store them in the list
  df_TS <- history$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add results to the list with the corresponding dataset name
  results_list[[dataset_names[i]]] <- df_TS
  
  # Calculate the maximum number of observations per simulation
  df_TS_max_t <- df_TS %>%
    group_by(sim) %>%
    summarize(max_t = max(t))
  
  # Print the maximum observations per simulation for this dataset
  print(paste("Max observations for", dataset_names[i]))
  print(df_TS_max_t)
}


```

Once we have the maximum t for all simulations and datasets, we use the smallest maximum t across them to ensure uniformity when building our comparison.  To identify which segment is more likely to click, we calculate the cumulative rewards for each dataset at each time step, average these rewards across simulations, and generate confidence intervals to account for uncertainty. This approach allows us to assess the performance of each segment in terms of click behavior.

```{r}

max_t <- 60
# Prepare an empty dataframe to store the aggregated results
df_history_all <- data.frame()

# Loop through each dataset, calculate cumulative rewards, and append to the dataframe
for (name in names(results_list)) {
  df_history_agg <- results_list[[name]] %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>% # calculate cumulative reward per sim
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward),
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
    filter(t <= max_t) %>%
    mutate(dataset = name ) # Add dataset name to differentiate

  # Combine with other datasets
  df_history_all <- rbind(df_history_all, df_history_agg)
}

# Plotting the cumulative reward behavior across all datasets
ggplot(df_history_all, aes(x = t, y = avg_cumulative_reward, color = dataset)) +
  geom_line(size = 1.5) + # Add line for average cumulative reward
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0), ymax = cumulative_reward_upper_CI, fill = dataset), alpha = 0.1) + # Add confidence interval ribbons
  labs(x = 'Time', y = 'Average Cumulative Reward', color = 'Dataset', fill = 'Dataset') + # Labels and legends
  theme_bw() + # Set theme
  theme(legend.position = "bottom") # Position the legend at the bottom

```
This graph shows the average cumulative reward over time for four user segments based on feature 0. The green line (dfZozo_feat1) appears to perform the best, accumulating the highest number of rewards (clicks), while the purple line (dfZozo_feat3) shows no reward, indicating that this group does not interact with the banner. However, the overlapping confidence intervals, represented by the shaded areas, introduce uncertainty into these results. Due to this overlap, we cannot definitively determine which segment is more likely to click, as the true performance differences between the segments are not significant.

# Batch Sensitivity Analysis
# Batch Sensitivity Analysis with the Contextual Package

Here we assess how different batch sizes affect a multi-armed bandit model's performance when applying the TS algorithm. Instead of updating the model after every single user interaction (which would be a batch size of 1), we can group several interactions together into "batches" and update the model after processing each batch. For each batch size, we simulate the relationship between item recommendations and user clicks, collect the results, and store them for comparison. 

```{r}
library(contextual)
library(dplyr)

# Set the seed for reproducibility
set.seed(0)

# Number of simulations
n_sim <- 10
size_sim <- 135667

# Define different batch sizes you want to test
batch_sizes <- c(1, 5, 10, 20, 50, 100)# Adjust as needed

# Create an empty list to store results for each batch size
results_list <- list()

# Define the Thompson Sampling policy object
TS <- ThompsonSamplingPolicy$new()

# Loop over each batch size
for (batch_size in batch_sizes) {
  
  # Create the bandit for the data with 80 arms, formula = click ~ item_id, no randomization
  bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                     data = dfZozo_80,
                                                     randomize = FALSE)
  
  # Create the agent object with the TS policy and the bandit
  agent_TS_zozo_80 <- Agent$new(TS, # add policy
                                bandit_Zozo_80) # add bandit
  
  # Create the simulator with the current batch size
  simulator <- Simulator$new(
    agent_TS_zozo_80,          # Set the agent
    horizon = size_sim,        # Set the size of each simulation
    do_parallel = TRUE,        # Run in parallel for speed
    simulations = n_sim,       # Simulate n_sim times
    save_interval = batch_size # Set save_interval to update after each batch
  )
  
  # Run the simulator and store the result for the current batch size
  history_TS_zozo_80 <- simulator$run()
  
  # Gather results for the current batch size
  df_TS_zozo_80 <- history_TS_zozo_80$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add the results to the list, using the batch size as the key
  results_list[[paste0("batch_size_", batch_size)]] <- df_TS_zozo_80
}
```


Once we have the maximum t for all simulations and depending on the batch size, we use the smallest maximum t across them to ensure uniformity when building our comparison.  Then we identify which batch has higher average cumulative rewards at each time step and generate confidence intervals to account for uncertainty. 

```{r}

# Create an empty list to store max_t for each batch size
max_t_list <- list()

# Loop over each batch size
for (batch_size in batch_sizes) {
  
  # Assuming you have df_TS_zozo_80 available for each batch size:
  # df_TS_zozo_80 should be filtered or selected before the loop
  # for each batch_size accordingly.
  
  # Example: df_TS_zozo_80 is a placeholder for the current batch size dataframe
  df_TS_zozo_80 <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Now calculate max_t for each simulation
df_TS_zozo_80_max_t <- df_TS_zozo_80 %>%
  group_by(sim) %>%
  summarise(max_t = max(t))
  
  # Store the max_t result in the max_t_list
  max_t_list[[paste0("batch_size_", batch_size)]] <- df_TS_zozo_80_max_t
}
```

```{r}
# Create an empty dataframe to store cumulative reward data for all batch sizes
df_cumulative_rewards <- data.frame()

max_t <- 1600

# Loop over each batch size to calculate cumulative rewards and store them
for (batch_size in batch_sizes) {
  
  df_TS_zozo_80 <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Calculate cumulative rewards for each simulation
  df_TS_zozo_80_agg <- df_TS_zozo_80 %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>%
    ungroup() %>%
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward), 
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward,
           batch_size = batch_size) %>%
    filter(t <= max_t)  # Filter by max_t
  
  # Add results to the cumulative rewards dataframe
  df_cumulative_rewards <- bind_rows(df_cumulative_rewards, df_TS_zozo_80_agg)
}

# Plot average cumulative rewards for each batch size
ggplot(df_cumulative_rewards, aes(x = t, y = avg_cumulative_reward, color = as.factor(batch_size))) +
  geom_line(size = 1.2) +
  labs(x = "Time", y = "Average Cumulative Reward", color = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards for Different Batch Sizes")

# Optional: Add 95% Confidence Intervals to the plot
ggplot(df_cumulative_rewards, aes(x = t, y = avg_cumulative_reward, color = as.factor(batch_size))) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = cumulative_reward_lower_CI, ymax = cumulative_reward_upper_CI, fill = as.factor(batch_size)), alpha = 0.2) +
  labs(x = "Time", y = "Average Cumulative Reward", color = "Batch Size", fill = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards with 95% CI for Different Batch Sizes")

```

This graph displays the average cumulative rewards over time for different batch sizes (1, 5, 10, 20, 50, 100). The batch size of 1, represented by the red line, achieves the highest cumulative rewards but with wider confidence intervals, indicating greater variability in performance. As batch sizes increase, the cumulative rewards decrease, with batch sizes like 50 and 100 showing more stable but slower reward accumulation, reflected in narrower confidence intervals. This suggests that smaller batch sizes, which update the model more frequently, lead to faster learning and higher rewards, but with more uncertainty, while larger batch sizes result in more consistent, though less optimal, performance over time.

# Batch Sensitivity Analysis without the Contextual Package

Now we will perform Thompson Sampling by hand without the contextual package for batching but due to our limited computing power we will only test for 5 simulations with 3 batch sizes. 

```{r}
set.seed(0)

# Load necessary libraries
library(dplyr)
library(ggplot2)

# Create a list of clicks per item_id (arm)
arm_rewards <- split(dfZozo_80$click, dfZozo_80$item_id)

# Number of simulations
n_simulations <- 5

# Horizon
n_rounds <- 135667

# Batch sizes
batch_sizes <- c(1, 50, 100)

# Initialize list to store results
all_results <- list()

for (sim in 1:n_simulations) {
  cat("Simulation", sim, "\n")
  
  for (batch_size in batch_sizes) {
    cat("  Batch size", batch_size, "\n")
    
    # Initialize variables
    arms <- length(unique(dfZozo_80$item_id))  
    successes <- rep(0, arms)
    failures <- rep(0, arms)
    pull_counts <- rep(0, arms)
    results <- data.frame(Round = 1:n_rounds, ChosenArm = integer(n_rounds), Reward = integer(n_rounds))
    
    # Initialize batch variables
    batch_rewards <- rep(0, batch_size)
    batch_chosen_arms <- rep(0, batch_size)
    batch_index <- 1
    
    for (round in 1:n_rounds) {
      # PREDICT: Sample from the arms Beta distributions for each arm
      beta_samples <- rbeta(arms, successes + 1, failures + 1)
      
      # ACT: Choose the arm with the highest sampled value
      chosen_arm <- which.max(beta_samples)
      
      # ENVIRONMENT: Obtain reward from dfZozo_80 for the chosen arm
      reward <- sample(arm_rewards[[as.character(chosen_arm)]], 1)
      
      # Store the chosen arm and reward
      results$ChosenArm[round] <- chosen_arm
      results$Reward[round] <- reward
      
      # Track number of pulls for the chosen arm
      pull_counts[chosen_arm] <- pull_counts[chosen_arm] + 1
      
      # Store in batch
      batch_rewards[batch_index] <- reward
      batch_chosen_arms[batch_index] <- chosen_arm
      batch_index <- batch_index + 1
      
      # When batch is full, update successes and failures
      if (batch_index > batch_size) {
        # Update successes and failures for the batch
        for (i in 1:batch_size) {
          arm <- batch_chosen_arms[i]
          rew <- batch_rewards[i]
          if (rew == 1) {
            successes[arm] <- successes[arm] + 1
          } else {
            failures[arm] <- failures[arm] + 1
          }
        }
        # Reset batch index
        batch_index <- 1
      }
    } # end of rounds
    
    # Handle any remaining batch updates
    if (batch_index > 1) {
      for (i in 1:(batch_index - 1)) {
        arm <- batch_chosen_arms[i]
        rew <- batch_rewards[i]
        if (rew == 1) {
          successes[arm] <- successes[arm] + 1
        } else {
          failures[arm] <- failures[arm] + 1
        }
      }
    }
    
    # Store results with additional columns for Simulation and BatchSize
    results$Simulation <- sim
    results$BatchSize <- batch_size
    
    # Append to all_results
    all_results[[length(all_results) + 1]] <- results
  } # end of batch sizes
} # end of simulations

# Combine all results into one data frame
df_all_results <- bind_rows(all_results)

# Calculate cumulative rewards
df_cumulative_rewards <- df_all_results %>%
  group_by(BatchSize, Simulation) %>%
  arrange(Round) %>%
  mutate(CumulativeReward = cumsum(Reward)) %>%
  ungroup()

# Number of simulations
n_simulations <- length(unique(df_cumulative_rewards$Simulation))

# Set maximum time (rounds) for plotting
max_t <- 1600  # Adjust as needed

# Calculate average cumulative rewards and standard errors
df_avg_cumulative_rewards <- df_cumulative_rewards %>%
  filter(Round <= max_t) %>%  # Filter to max_t rounds
  group_by(BatchSize, Round) %>%
  summarise(
    AvgCumulativeReward = mean(CumulativeReward),
    SE_CumulativeReward = sd(CumulativeReward) / sqrt(n_simulations)
  ) %>%
  ungroup() %>%
  mutate(
    LowerCI = AvgCumulativeReward - 1.96 * SE_CumulativeReward,
    UpperCI = AvgCumulativeReward + 1.96 * SE_CumulativeReward
  )

# Plot average cumulative rewards for each batch size
ggplot(df_avg_cumulative_rewards, aes(x = Round, y = AvgCumulativeReward, color = as.factor(BatchSize))) +
  geom_line(size = 1.2) +
  labs(x = "Round", y = "Average Cumulative Reward", color = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards for Different Batch Sizes")

# Optional: Add 95% Confidence Intervals to the plot
ggplot(df_avg_cumulative_rewards, aes(x = Round, y = AvgCumulativeReward, color = as.factor(BatchSize))) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI, fill = as.factor(BatchSize)), alpha = 0.2, color = NA) +
  labs(x = "Round", y = "Average Cumulative Reward", color = "Batch Size", fill = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards with 95% CI for Different Batch Sizes")

```
The plot shows that a batch size of 50 achieves higher cumulative rewards compared to batch sizes of 1 and 100 in the Thompson Sampling simulation. This is likely because batch size 50 strikes an effective balance between exploration and exploitation, allowing the algorithm to gather enough observations in each batch to make more informed decisions without delaying updates too much. In contrast, a batch size of 1 may be too exploratory, causing slower cumulative reward growth as it frequently tests different actions, while batch size 100 delays feedback and updates, resulting in slower adaptation and underperformance.

