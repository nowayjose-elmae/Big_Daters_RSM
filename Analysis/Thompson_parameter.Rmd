---
title: "Bernoulli Thompson Sampling 1"
output: html_document
date: "2024-10-02"
---

```{r setup, include=FALSE}
rm(list=ls())
# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

#Dataset

```{r, load data}
# Set seed for reproducibility
set.seed(0)

# Load the dataset (assuming dfZozo_80 contains 'item_id' and 'click' columns)
dfZozo_80 <- read.csv('zozo_Context_80items.csv')

# Get a summary of click means per item
dfZozo_80[1:10000,] %>%
  group_by(item_id) %>%
  summarise(mean_click = mean(click)) #there are a lot more failures than success
```
Based on the summary of mean_click per arms, we have concluded that there is much more failures and success since the mean_click values are close to zero. Therefore, we set the betas(number of failures) larger than the alphas(number of success). 

```{r, load data}
# Define alphas and betas for sensitivity analysis
alphas <- c(1,1,1)
betas <- c(4,5,6)

# Set simulation parameters
size_sim <- 10000  # Number of impressions per simulation
n_sim <- 10        # Number of simulations

# Create bandit for 80 arms
bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                   data = dfZozo_80,
                                                   randomize = FALSE)

# Initialize a list to store results
results_list <- list()

# Loop over alphas and betas to create Thompson Sampling policies and agents
for (i in seq_along(alphas)) {
  
  # Create Thompson Sampling policy for current alpha and beta
  TS <- ThompsonSamplingPolicy$new(alpha = alphas[i], beta = betas[i])

  # Create the agent for 80 arms
  agent_TS_zozo_80 <- Agent$new(TS, bandit_Zozo_80)

  # Run the simulator for Thompson Sampling
  simulator_80 <- Simulator$new(agent_TS_zozo_80, 
                              horizon = size_sim, 
                              simulations = n_sim)

  # Run the simulator
  history_TS_zozo_80 <- simulator_80$run()

  # Store the results and tag with current alpha and beta values
  results <- history_TS_zozo_80$data %>%
    mutate(alpha = alphas[i], beta = betas[i])
  
  results_list[[i]] <- results  # Add results to the list
}

# Combine all the results into a single dataframe
df_TS_zozo_80 <- bind_rows(results_list)

# View the combined results
head(df_TS_zozo_80)
```

```{r, class note}

dfZozo_80[1:10000,]
#second method is to make a list of alpha and beta

#param_settings <- list(
  #list(alpha = 100, beta = 100),  # original
  #list(alpha = 50, beta = 50),    # lower initial values
  #list(alpha = 10, beta = 90),    # imbalanced
  #list(alpha = 1, beta = 1)       # uniform prior) 

dfZozo_80%>%
  group_by(user_feature_0) %>% #we decided to conduct an analysis based on the feature 0. 
  summarise(n())

df_zozo_feat_zero_1 <- dfZozo_80 %>%
  filter(user_feature_0 == '4ae385d792f81dde128124a925a830de')
#make a histogram and assumptions 
#choose one feature

#compare the observation per user_feature
```

#batching note
```{r}
nrow(dfZozo_80)

#create different batch size
#batch numer one = 10% dataset
nrows_one <- nrow(dfZozo_80)*0.1
dfZozo_80_batch_one <- dfZozo_80[1: nrows_one,]

#batch number two = 20% of the dataset
nrows_two <- nrow(dfZozo_80) *0.2 + nrows_one
dfZozo_80_batch_two <- dfZozo_80[(nrow(dfZozo_80_batch_one)+1): nrows_two]
```

# Thompson Sampling: notation
```{r, parameter tuning alternative}
library(contextual)

# set the seed
set.seed(0)

## OfflineReplayEvaluatorBandit: simulates a bandit based on provided data
#
# Arguments:
#
#   data: dataframe that contains variables with (1) the reward and (2) the arms
#
#   formula: should consist of variable names in the dataframe. General structure is: 
#       reward variable name ~ arm variable name
#
#   randomize: whether or not the bandit should receive the data in random order,
#              or as ordered in the dataframe.

bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                   data = dfZozo_80,
                                                   randomize = FALSE) # randomization

# lets generate 10 simulations, each of size 10000
size_sim <- 10000
n_sim<- 10

# Parameter settings for sensitivity analysis
param_settings <- list(
  list(alpha = 100, beta = 100),  # original
  list(alpha = 50, beta = 50),    # lower initial values
  list(alpha = 10, beta = 90),    # imbalanced
  list(alpha = 1, beta = 1)       # uniform prior
)

# Initialize an empty list to store results
results_list <- list()

# Loop over the different parameter settings
for (i in seq_along(param_settings)) {
  
   # Define the Thompson Sampling policy with current alpha and beta
  TS <- ThompsonSamplingPolicy$new(alpha = param_settings[[i]]$alpha, 
                                   beta = param_settings[[i]]$beta)
  
  # the contextual package works with 'agent' objects - which consist of a policy and a bandit
  agent_TS_zozo_80 <-  Agent$new(TS, bandit_Zozo_80) 
  
  
  ## simulator: simulates a bandit + policy based on the provided data and parameters
  simulator          <- Simulator$new(agent_TS_zozo_80, # set our agent
                                        horizon= size_sim, # set the size of each simulation
                                        do_parallel = TRUE, # run in parallel for speed
                                        simulations = n_sim, # simulate it n_sim times
                                    )
    
    
  # run the simulator object  
  history_TS_zozo_80            <- simulator$run()
  
  # Store the results in the results_list
  results_list[[i]] <- history_TS_zozo_80
}

# Accessing results for each parameter setting
for (i in seq_along(results_list)) {
  
  # Extract the results for each parameter setting
  history <- results_list[[i]]$data
  
  # You can now explore the `history` object for each simulation
  print(head(history))
}
```

```{r}
# Create a combined alpha_beta column
df_TS_zozo_80 <- df_TS_zozo_80 %>%
  mutate(alpha_beta = paste0("alpha=", alpha, "_beta=", beta))

# Now group by alpha_beta and sim to calculate max_t
df_TS_zozo_80_max_t <- df_TS_zozo_80 %>%
  group_by(alpha_beta, sim) %>%
  summarize(max_t = max(t)) # Get the maximum time step (t) for each alpha-beta pair and sim

# View the results
print(df_TS_zozo_80_max_t)

```

```{r, results='hide', message=FALSE, warning=FALSE}
df_TS_zozo_80_max_t <- df_TS_zozo_80%>%
  group_by(alpha_beta, sim) %>% # group by per agent
  summarize(max_t = max(t)) # get max t
  ungroup()
  
df_TS_zozo_80_max_t
```

```{r, max obs alternative}
# Loop through each set of results in the sensitivity analysis
for (i in seq_along(results_list)) {
  
  # Extract the results for the current parameter setting
  df_TS_zozo_80 <- results_list[[i]]$data
  
  # Calculate the maximum number of observations per simulation (t_max for each sim)
  df_TS_zozo_80_max_t <- df_TS_zozo_80 %>%
    group_by(sim) %>%  # Group by simulation
    summarise(max_t = max(t))  # Get the maximum time step for each simulation
  
  # Print the results for this parameter setting
  print(paste("Parameter setting", i, "maximum time per simulation:"))
  print(df_TS_zozo_80_max_t)
}

```

The results show that the maximum number of rounds across 10 simulations varied between () and () for and 490, with simulation 8 reaching the highest number of rounds (490) and simulation 5 the lowest (435). While most simulations fall within a narrow range, this variation suggests differences in exploration patterns, where some simulations explored more arms or took more actions before reaching the end of the dataset.

In the dataframe *df_TS_zozo* we have gathered the results of the TS policy. This dataframe contains the following columns:

```{=tex}
\begin{itemize}
\item \textbf{$t$}: the step at which a choice was made
\item \textbf{sim}: the simulation for which we observe results
\item \textbf{choice}: the choice made by the algorithm
\item \textbf{reward}: the reward observed by the algorithm
\item \textbf{agent}: column containing the name of the agent 
\end{itemize}
```

\textbf{Task 3: using this dataframe, make a plot of the average cumulative rewards for all simulations, together with the 95\% confidence interval}.

```{r}
# Max of observations. Adjusted to the number of observations per simulation (min 831) and the size of the dataset used
max_obs <- 102

# dataframe transformation
df_history_agg <- df_TS_zozo_80 %>%
  group_by(alpha_beta, sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  # Calculate cumulative reward over time for each sim
  group_by(alpha_beta, t) %>%
  summarise(avg_cumulative_reward = mean(cumulative_reward),  # Average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n()),  # Standard error
            .groups = 'drop') %>%
  mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
         cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
  filter(t <= max_obs)

# TODO: Make the following two plots:
# 1: A plot that shows only the average cumulative rewards over time using the df_history_agg dataframe
# 2: The plot as defined in (1) together with the 95\% confidence interval. 

legend <- c("Avg." = "orange", "95% CI" = "gray") # set legend


ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward)) +
  geom_line(size=1.5,aes(color="Avg.")) + # add line
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0,cumulative_reward_lower_CI), # add confidence interval
                  ymax=cumulative_reward_upper_CI,
                  color = "95% CI"), 
              alpha=0.1) +
  labs(x = 'Time', y='Average Cumulative Reward', color='Metric') + # add titles
  scale_color_manual(values=legend) + # add legend
  theme_bw() # set the theme

# Plot for Thompson Sampling Arm Choices Over Time
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  geom_ribbon(aes(ymin = cumulative_reward_lower_CI, ymax = cumulative_reward_upper_CI, fill = alpha_beta), 
              alpha = 0.2) +  # Confidence interval ribbon
  labs(title = "Average Cumulative Rewards with 95% CI Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair', 
       fill = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 


```
```{r}
# Max of observations. Adjusted to the number of observations per simulation
max_obs <- 90

# Dataframe transformation
df_history_agg <- df_TS_zozo_80 %>%
  group_by(alpha_beta, sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  # Calculate cumulative reward over time for each sim
  group_by(alpha_beta, t) %>%
  summarise(avg_cumulative_reward = mean(cumulative_reward),  # Average cumulative reward
            .groups = 'drop') %>%
  filter(t <= max_obs)

# Plot the average cumulative rewards over time for each alpha-beta pair
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  labs(title = "Average Cumulative Rewards Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

```

We started by generating batches with sizes that increase by 1262, beginning from 1262, and continuing until the maximum batch size reaches 54266. The number 1262 was chosen because it is a divisor of 54266, ensuring that we can increment the batch sizes evenly and consistently, maintaining equal-sized increments throughout the process.


#Supposedly this is the right version of batching

```{r}
# Function to split the dataset into batches based on batch size
split_into_batches <- function(dfZozo_80, batch_size) {
  n <- nrow(dfZozo_80)
  num_batches <- ceiling(n / batch_size)  # Calculate the number of batches
  batches <- list()
  
  for (i in 1:num_batches) {
    start_row <- ((i - 1) * batch_size) + 1
    end_row <- min(i * batch_size, n)
    batches[[i]] <- dfZozo_80[start_row:end_row, ]
  }
  
  return(batches)
}

# Define different batch sizes you want to test
batch_sizes <- seq(1262, 54266, by = 8834) # Example of batch sizes to test
results_list <- list()

# Loop over each batch size and run the model
for (batch_size in batch_sizes) {
  
  # Split the dataset into batches for the current batch size
  batches <- split_into_batches(dfZozo_80, batch_size)
  
  # Empty list to store the history for each batch
  history_batches <- list()
  
  for (i in 1:length(batches)) {
    # Thompson Sampling policy
    TS <- ThompsonSamplingPolicy$new()
    
    # Create bandit for the current batch
    bandit_batch <- OfflineReplayEvaluatorBandit$new(
      formula = click ~ item_id,
      data = batches[[i]],
      randomize = FALSE
    )
    
    # Create the agent with the Thompson Sampling policy and batch-specific bandit
    agent_batch <- Agent$new(TS, bandit_batch)
    
    # Set up simulator for the batch
    simulator <- Simulator$new(
      agent_batch,
      horizon = nrow(batches[[i]]),  # Use the batch size for each simulation
      do_parallel = TRUE,
      simulations = n_sim
    )
    
    #simulator <- Simulator$new(agent_TS_zozo_20, horizon = size_sim, do_parallel = TRUE, simulations = n_sim)
    
    # Run the simulation for the current batch
    history_batch <- simulator$run()
    
    # Get the results and add the batch number
    history_batch_data <- history_batch$data
    history_batch_data$batch_num <- i
    
    # Store the result for this batch
    history_batches[[i]] <- history_batch_data
    
    # Print batch progress
    print(paste("Completed batch", i, "for batch size", batch_size))
  }
  
  # Combine all batch histories into a single dataframe for the current batch size
  df_TS_combined <- do.call(rbind, history_batches) %>%
    select(t, sim, choice, reward, agent, batch_num)
  
  # Store results in a list with batch size as the key
  results_list[[paste0("batch_size_", batch_size)]] <- df_TS_combined
}

# You can now access the results for each batch size by calling:
# results_list[["batch_size_1262"]]
# results_list[["batch_size_2524"]]
# etc.

# Track the maximum t (time) for each simulation for batch size 1262 as an example
df_TS_zozo_combined_max_t <- results_list[["batch_size_1262"]] %>%
  group_by(sim) %>%
  summarize(max_t = max(t))

# View the max time per simulation
df_TS_zozo_combined_max_t

```


```{r}
# Function to calculate regret
calculate_regret <- function(df_TS_combined, dfZozo_80) {
  
  # Calculate total reward obtained by Thompson Sampling in each batch
  total_reward_TS <- df_TS_combined %>%
    group_by(batch_num) %>%
    summarize(total_reward_TS = sum(reward)) %>%
    ungroup()
  
  # Calculate average reward per arm from the entire dataset
  avg_reward_per_arm <- dfZozo_80 %>%
    group_by(item_id) %>%
    summarize(avg_reward = mean(click)) %>%
    ungroup()
  
  # Identify the arm with the highest average reward
  best_arm_avg_reward <- max(avg_reward_per_arm$avg_reward)
  
  # Calculate the total reward for the best arm if it had been chosen every time
  total_reward_best_arm <- df_TS_combined %>%
    group_by(batch_num) %>%
    summarize(total_reward_best = n() * best_arm_avg_reward) %>%
    ungroup()
  
  # Merge both total_reward_TS and total_reward_best
  regret_df <- total_reward_TS %>%
    left_join(total_reward_best_arm, by = "batch_num") %>%
    mutate(regret = total_reward_best - total_reward_TS)
  
  return(regret_df)
}

# Apply the regret calculation function for each batch size and store total regret
regret_summary <- data.frame(batch_size = integer(), total_regret_TS = numeric())

for (batch_size in batch_sizes) {
  # Access the combined results for the current batch size
  df_TS_combined <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Calculate regret
  regret_df <- calculate_regret(df_TS_combined, dfZozo_80)
  
  # Calculate the total regret for this batch size
  total_regret_TS <- sum(regret_df$regret)
  
  # Append the results to the regret summary table
  regret_summary <- rbind(regret_summary, data.frame(batch_size = batch_size, total_regret_TS = total_regret_TS))
}

# Print the regret summary in a format similar to the TS row
print("Table: Regret for TS at different batch sizes")
print(regret_summary)

```


Parameter tuning





