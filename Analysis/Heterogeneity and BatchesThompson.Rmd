---
title: "Bernoulli Thompson Sampling 1 Final"
output: html_document
date: "2024-10-02"
---

```{r setup, include=FALSE}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

#Dataset

```{r}
# this version contains 80 arms (ZOZO)
set.seed(0)
library(readr)
dfZozo_80 <- read_csv("~/Downloads/zozo_Context_80items.csv.zip")%>%
  select(item_id, click, position, user_feature_0, user_feature_1,user_feature_2, user_feature_3)
  


```
# Thompson Sampling: notation

```{=tex}
\begin{enumerate}
  \item The reward $Q_t(a)$ for each arm follows a beta distribution. At the start, at $t = 0$, every arm is assumed to follow the same beta distribution with parameters $\alpha_0 = 1$ and $\beta_0 = 1$. This means we initially assume each arm has an equal probability of giving a reward. 
  \item At each time $t$, the algorithm samples $n_{\mathrm{sample}}$ observations from each of the distributions. The arm which has the highest average reward according to the sample is the chosen arm. 
  \item We then observe the reward $r_t$, and update the parameters of the distribution accordingly. 
  \begin{itemize}
    \item $\alpha_t = \alpha_{t-1} + r_t$
    \item $\beta_t = \beta_{t-1} + 1 - r_t$
  \end{itemize}
\end{enumerate}
```

#Heterogeneity and Aggregation Analysis
```{r}
dfZozo_80 %>%
  group_by(user_feature_0) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_1) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_2) %>%
  summarise(n())


dfZozo_80 %>%
  group_by(user_feature_3) %>%
  summarise(n())
```
Since some user features, such as features 1, 2, and 3, contain very small sample sizes (e.g., values as low as 16), including these in our analysis would lead to unreliable estimates and reduce the statistical power of the results. Small sample sizes can introduce significant variability, making it difficult to draw robust and meaningful conclusions. Therefore, we are excluding these user features from the analysis to ensure the focus is on groups with larger, more balanced sample sizes, which will provide more reliable insights.

```{r}
dfZozo_feat0 <- dfZozo_80%>%
  filter(user_feature_0 == "4ae385d792f81dde128124a925a830de")

dfZozo_feat1 <- dfZozo_80%>%
  filter(user_feature_0 == "574464659df0fc5bac579eff2b1fff99")

dfZozo_feat2 <- dfZozo_80%>%
  filter(user_feature_0 == "81ce123cbb5bd8ce818f60fb3586bba5")

dfZozo_feat3 <- dfZozo_80%>%
  filter(user_feature_0 == "cef3390ed299c09874189c387777674a")
```


```{r}
# Define the feature datasets in a list
feature_datasets <- list(dfZozo_feat0, dfZozo_feat1, dfZozo_feat2, dfZozo_feat3)

# Define a vector with dataset names for better result tracking
dataset_names <- c("dfZozo_feat0", "dfZozo_feat1", "dfZozo_feat2", "dfZozo_feat3")

# Parameters for the simulations
size_sim <- 6641
n_sim <- 10

# Store results for each dataset
results_list <- list()

# Loop through each feature dataset and run the TS simulation
for (i in seq_along(feature_datasets)) {
  # Create the bandit object for the current dataset
  bandit <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                             data = feature_datasets[[i]],
                                             randomize = FALSE)
  
  # Define the Thompson Sampling policy
  TS <- ThompsonSamplingPolicy$new()
  
  # Create the agent
  agent <- Agent$new(TS, bandit)
  
  # Create the simulator
  simulator <- Simulator$new(agent,
                             horizon = size_sim,
                             do_parallel = TRUE,
                             simulations = n_sim)
  
  # Run the simulator
  history <- simulator$run()
  
  # Gather the results and store them in the list
  df_TS <- history$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add results to the list with the corresponding dataset name
  results_list[[dataset_names[i]]] <- df_TS
  
  # Calculate the maximum number of observations per simulation
  df_TS_max_t <- df_TS %>%
    group_by(sim) %>%
    summarize(max_t = max(t))
  
  # Print the maximum observations per simulation for this dataset
  print(paste("Max observations for", dataset_names[i]))
  print(df_TS_max_t)
}


```

```{r}

max_t <- 60
# Prepare an empty dataframe to store the aggregated results
df_history_all <- data.frame()

# Loop through each dataset, calculate cumulative rewards, and append to the dataframe
for (name in names(results_list)) {
  df_history_agg <- results_list[[name]] %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>% # calculate cumulative reward per sim
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward),
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
    filter(t <= max_t) %>%
    mutate(dataset = name ) # Add dataset name to differentiate

  # Combine with other datasets
  df_history_all <- rbind(df_history_all, df_history_agg)
}

# Plotting the cumulative reward behavior across all datasets
ggplot(df_history_all, aes(x = t, y = avg_cumulative_reward, color = dataset)) +
  geom_line(size = 1.5) + # Add line for average cumulative reward
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0), ymax = cumulative_reward_upper_CI, fill = dataset), alpha = 0.1) + # Add confidence interval ribbons
  labs(x = 'Time', y = 'Average Cumulative Reward', color = 'Dataset', fill = 'Dataset') + # Labels and legends
  theme_bw() + # Set theme
  theme(legend.position = "bottom") # Position the legend at the bottom

```


```{r}
library(contextual)
library(dplyr)

# Set the seed for reproducibility
set.seed(0)

# Number of simulations
n_sim <- 10
size_sim <- 135667

# Define different batch sizes you want to test
batch_sizes <- c(1, 5, 10, 20, 50, 100) # Adjust as needed

# Create an empty list to store results for each batch size
results_list <- list()

# Define the Thompson Sampling policy object
TS <- ThompsonSamplingPolicy$new()

# Loop over each batch size
for (batch_size in batch_sizes) {
  
  # Create the bandit for the data with 80 arms, formula = click ~ item_id, no randomization
  bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                     data = dfZozo_80,
                                                     randomize = FALSE)
  
  # Create the agent object with the TS policy and the bandit
  agent_TS_zozo_80 <- Agent$new(TS, # add policy
                                bandit_Zozo_80) # add bandit
  
  # Create the simulator with the current batch size
  simulator <- Simulator$new(
    agent_TS_zozo_80,          # Set the agent
    horizon = size_sim,        # Set the size of each simulation
    do_parallel = TRUE,        # Run in parallel for speed
    simulations = n_sim,       # Simulate n_sim times
    save_interval = batch_size # Set save_interval to update after each batch
  )
  
  # Run the simulator and store the result for the current batch size
  history_TS_zozo_80 <- simulator$run()
  
  # Gather results for the current batch size
  df_TS_zozo_80 <- history_TS_zozo_80$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add the results to the list, using the batch size as the key
  results_list[[paste0("batch_size_", batch_size)]] <- df_TS_zozo_80
}
```


```{r}

# Assume df_TS_zozo_80 is already generated for each batch size in previous steps

# Create an empty list to store max_t for each batch size
max_t_list <- list()

# Loop over each batch size
for (batch_size in batch_sizes) {
  
  # Assuming you have df_TS_zozo_80 available for each batch size:
  # df_TS_zozo_80 should be filtered or selected before the loop
  # for each batch_size accordingly.
  
  # Example: df_TS_zozo_80 is a placeholder for the current batch size dataframe
  df_TS_zozo_80 <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Now calculate max_t for each simulation
df_TS_zozo_80_max_t <- df_TS_zozo_80 %>%
  group_by(sim) %>%
  summarise(max_t = max(t))
  
  # Store the max_t result in the max_t_list
  max_t_list[[paste0("batch_size_", batch_size)]] <- df_TS_zozo_80_max_t
}
```


```{r}
# Create an empty dataframe to store cumulative reward data for all batch sizes
df_cumulative_rewards <- data.frame()

max_t <- 1600

# Loop over each batch size to calculate cumulative rewards and store them
for (batch_size in batch_sizes) {
  
  df_TS_zozo_80 <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Calculate cumulative rewards for each simulation
  df_TS_zozo_80_agg <- df_TS_zozo_80 %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>%
    ungroup() %>%
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward), 
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward,
           batch_size = batch_size) %>%
    filter(t <= max_t)  # Filter by max_t
  
  # Add results to the cumulative rewards dataframe
  df_cumulative_rewards <- bind_rows(df_cumulative_rewards, df_TS_zozo_80_agg)
}

# Plot average cumulative rewards for each batch size
ggplot(df_cumulative_rewards, aes(x = t, y = avg_cumulative_reward, color = as.factor(batch_size))) +
  geom_line(size = 1.2) +
  labs(x = "Time", y = "Average Cumulative Reward", color = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards for Different Batch Sizes")

# Optional: Add 95% Confidence Intervals to the plot
ggplot(df_cumulative_rewards, aes(x = t, y = avg_cumulative_reward, color = as.factor(batch_size))) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = cumulative_reward_lower_CI, ymax = cumulative_reward_upper_CI, fill = as.factor(batch_size)), alpha = 0.2) +
  labs(x = "Time", y = "Average Cumulative Reward", color = "Batch Size", fill = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards with 95% CI for Different Batch Sizes")

```












