---
title: "Bernoulli Thompson Sampling 1 Final"
authors: Catarina Casaca
output: html_document
date: "2024-10-02"
---

# Introduction

Decision-making problems, where an agent must choose the best action from several alternatives at each point in time, are widely encountered in practical applications, such as when companies decide how to structure their websites to encourage shopping behavior. These problems involve a trade-off between exploration and exploitation (Lai & Robbins, 1985; Auer et al., 2002). This trade-off is typically formulated as the multi-armed bandit problem, where each possible action, or arm, has an unknown reward probability distribution, and the agent aims to maximize cumulative rewards over time by selecting the optimal arm (Bouneffouf & Rish, 2019).

In this assignment, we apply the MAB framework to a real-world dataset from Zozo, a high-traffic fashion website. Here, the arms correspond to different fashion items, and the rewards are based on user clicks on the advertisement banner. The goal is to determine which items are best to recommend and identify the most effective positions on the recommendation interface (left, center, or right) to maximize click-through rates (CTR).

To achieve this, we will first describe and analyze the dataset. Then, we will test several MAB models, including Thompson Sampling, ε-greedy, and Upper Confidence Bound (UCB), to identify the best-performing items and banner positions for maximizing clicks. Additionally, we will perform a heterogeneity analysis to examine how different user segments impact the models’ performance, a batch analysis to assess how varying batch sizes affect outcomes, and a parameter analysis to improve the models.

The dataset captures user interactions with recommended fashion items and includes variables such as timestamps of impressions, item IDs, positions on the interface, and click indicator. It also contains propensity scores, which represent the probability of an item being recommended in each position, along with several user-related features such as age and gender.

# Download Packages

```{r setup, include=FALSE}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

# Download Dataset

```{r}
# this version contains 80 arms (ZOZO)
set.seed(0)
library(readr)
dfZozo_80 <- read_csv("~/Downloads/zozo_Context_80items.csv.zip")%>%
  select(item_id, click, position, user_feature_0, user_feature_1,user_feature_2, user_feature_3)
  


```

# Thompson Sampling: notation

```{=tex}
\begin{enumerate}
  \item The reward $Q_t(a)$ for each arm follows a beta distribution. At the start, at $t = 0$, every arm is assumed to follow the same beta distribution with parameters $\alpha_0 = 1$ and $\beta_0 = 1$. This means we initially assume each arm has an equal probability of giving a reward. 
  \item At each time $t$, the algorithm samples $n_{\mathrm{sample}}$ observations from each of the distributions. The arm which has the highest average reward according to the sample is the chosen arm. 
  \item We then observe the reward $r_t$, and update the parameters of the distribution accordingly. 
  \begin{itemize}
    \item $\alpha_t = \alpha_{t-1} + r_t$
    \item $\beta_t = \beta_{t-1} + 1 - r_t$
  \end{itemize}
\end{enumerate}
```
# Heterogeneity and Aggregation Analysis

To perform a meaningful heterogeneity and aggregation analysis, our first step is to identify potential clusters within the dataset. This involves grouping the data based on various features and selecting the one that provides the most useful segmentation. Ideally, with a detailed description of the features, we could create more targeted segments, such as isolating middle-aged women . However, since we lack detailed information, we will focus on choosing the feature that best divides the population into meaningful subgroups.

What we are looking for is a feature that on one hand does not create too many subgroups, which could dilute our analysis, and on the other hand ensures that each subgroup has a substantial portion of the sample. This will help us avoid insignificant results and ensure that our analysis captures meaningful differences across segments.

```{r}
dfZozo_80 %>%
  group_by(user_feature_0) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_1) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_2) %>%
  summarise(n())


dfZozo_80 %>%
  group_by(user_feature_3) %>%
  summarise(n())
```

Since some user features, such as features 1, 2, and 3, contain very small sample sizes (e.g., values as low as 16), including these in our analysis would lead to unreliable estimates and reduce the statistical power of the results. Therefore, we are excluding these user features from the analysis to ensure the focus is on groups with larger, more balanced sample sizes, such as feature 0. Now, we divide feature 0 into the four previously identified segments, by creating 4 ew datasets.

```{r}
dfZozo_seg0 <- dfZozo_80%>%
  filter(user_feature_0 == "4ae385d792f81dde128124a925a830de")

dfZozo_seg1 <- dfZozo_80%>%
  filter(user_feature_0 == "574464659df0fc5bac579eff2b1fff99")

dfZozo_seg2 <- dfZozo_80%>%
  filter(user_feature_0 == "81ce123cbb5bd8ce818f60fb3586bba5")

dfZozo_seg3 <- dfZozo_80%>%
  filter(user_feature_0 == "cef3390ed299c09874189c387777674a")
```

Now we run Thompson Sampling simulations on each of the four segmented datasets based on feature 0 to evaluate how the model performs across different segments of users. We define the datasets in a list and set simulation parameters, such as a horizon size of 6641 impressions and 10 simulation runs per dataset. We set the horizon size to 6641, which matches the number of observations in the smallest segment of the dataset based on user_feature_0. By doing this, we ensure that each segment is treated equally during the simulations, preventing over-representation of larger segments. After running the simulations, we calculate the maximum number of observations (t) in each simulation by grouping the results by the simulation and summarizing the largest t value. Identifying the maximum t for each simulation is necessary to determine the length of each simulation run.

```{r}
# Define the feature datasets in a list
feature_0_datasets <- list(dfZozo_seg0, dfZozo_seg1, dfZozo_seg2, dfZozo_seg3)

# Define a vector with dataset names for better result tracking
dataset_names <- c("dfZozo_seg0", "dfZozo_seg1", "dfZozo_seg2", "dfZozo_seg3")

# Parameters for the simulations
size_sim <- 6641
n_sim <- 10

# Store results for each dataset
results_list <- list()

# Loop through each feature dataset and run the TS simulation
for (i in seq_along(feature_datasets)) {
  # Create the bandit object for the current dataset
  bandit <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                             data = feature_0_datasets[[i]],
                                             randomize = FALSE)
  
  # Define the Thompson Sampling policy
  TS <- ThompsonSamplingPolicy$new()
  
  # Create the agent
  agent <- Agent$new(TS, bandit)
  
  # Create the simulator
  simulator <- Simulator$new(agent,
                             horizon = size_sim,
                             do_parallel = TRUE,
                             simulations = n_sim)
  
  # Run the simulator
  history <- simulator$run()
  
  # Gather the results and store them in the list
  df_TS <- history$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add results to the list with the corresponding dataset name
  results_list[[dataset_names[i]]] <- df_TS
  
  # Calculate the maximum number of observations per simulation
  df_TS_max_t <- df_TS %>%
    group_by(sim) %>%
    summarize(max_t = max(t))
  
  # Print the maximum observations per simulation for this dataset
  print(paste("Max observations for", dataset_names[i]))
  print(df_TS_max_t)
}


```

Once we have the maximum t for all simulations and datasets, we use the smallest maximum t across them to ensure uniformity when building our comparison.  To identify which segment is more likely to click, we calculate the cumulative rewards for each dataset at each time step, average these rewards across simulations, and generate confidence intervals to account for uncertainty. This approach allows us to assess the performance of each segment in terms of click behavior.

```{r}

max_t <- 60
# Prepare an empty dataframe to store the aggregated results
df_history_all <- data.frame()

# Loop through each dataset, calculate cumulative rewards, and append to the dataframe
for (name in names(results_list)) {
  df_history_agg <- results_list[[name]] %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>% # calculate cumulative reward per sim
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward),
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
    filter(t <= max_t) %>%
    mutate(dataset = name ) # Add dataset name to differentiate

  # Combine with other datasets
  df_history_all <- rbind(df_history_all, df_history_agg)
}

# Plotting the cumulative reward behavior across all datasets
ggplot(df_history_all, aes(x = t, y = avg_cumulative_reward, color = dataset)) +
  geom_line(size = 1.5) + # Add line for average cumulative reward
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0), ymax = cumulative_reward_upper_CI, fill = dataset), alpha = 0.1) + # Add confidence interval ribbons
  labs(x = 'Time', y = 'Average Cumulative Reward', color = 'Dataset', fill = 'Dataset') + # Labels and legends
  theme_bw() + # Set theme
  theme(legend.position = "bottom") # Position the legend at the bottom

```
This graph shows the average cumulative reward over time for four user segments based on feature 0. The green line (dfZozo_feat1) appears to perform the best, accumulating the highest number of rewards (clicks), while the purple line (dfZozo_feat3) shows no reward, indicating that this group does not interact with the banner. However, the overlapping confidence intervals, represented by the shaded areas, introduce uncertainty into these results. Due to this overlap, we cannot definitively determine which segment is more likely to click, as the true performance differences between the segments are not significant.

# Batch Sensitivity Analysis

Here we assess how different batch sizes affect a multi-armed bandit model's performance when applying the TS algorithm. Instead of updating the model after every single user interaction (which would be a batch size of 1), we can group several interactions together into "batches" and update the model after processing each batch. For each batch size, we simulate the relationship between item recommendations and user clicks, collect the results, and store them for comparison. 

```{r}
library(contextual)
library(dplyr)

# Set the seed for reproducibility
set.seed(0)

# Number of simulations
n_sim <- 10
size_sim <- 135667

# Define different batch sizes you want to test
batch_sizes <- c(1, 5, 10, 20, 50, 100) # Adjust as needed

# Create an empty list to store results for each batch size
results_list <- list()

# Define the Thompson Sampling policy object
TS <- ThompsonSamplingPolicy$new()

# Loop over each batch size
for (batch_size in batch_sizes) {
  
  # Create the bandit for the data with 80 arms, formula = click ~ item_id, no randomization
  bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                     data = dfZozo_80,
                                                     randomize = FALSE)
  
  # Create the agent object with the TS policy and the bandit
  agent_TS_zozo_80 <- Agent$new(TS, # add policy
                                bandit_Zozo_80) # add bandit
  
  # Create the simulator with the current batch size
  simulator <- Simulator$new(
    agent_TS_zozo_80,          # Set the agent
    horizon = size_sim,        # Set the size of each simulation
    do_parallel = TRUE,        # Run in parallel for speed
    simulations = n_sim,       # Simulate n_sim times
    save_interval = batch_size # Set save_interval to update after each batch
  )
  
  # Run the simulator and store the result for the current batch size
  history_TS_zozo_80 <- simulator$run()
  
  # Gather results for the current batch size
  df_TS_zozo_80 <- history_TS_zozo_80$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add the results to the list, using the batch size as the key
  results_list[[paste0("batch_size_", batch_size)]] <- df_TS_zozo_80
}
```


Once we have the maximum t for all simulations and depending on the batch size, we use the smallest maximum t across them to ensure uniformity when building our comparison.  Then we identify which batch has higher average cumulative rewards at each time step and generate confidence intervals to account for uncertainty. 

```{r}

# Create an empty list to store max_t for each batch size
max_t_list <- list()

# Loop over each batch size
for (batch_size in batch_sizes) {
  
  # Assuming you have df_TS_zozo_80 available for each batch size:
  # df_TS_zozo_80 should be filtered or selected before the loop
  # for each batch_size accordingly.
  
  # Example: df_TS_zozo_80 is a placeholder for the current batch size dataframe
  df_TS_zozo_80 <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Now calculate max_t for each simulation
df_TS_zozo_80_max_t <- df_TS_zozo_80 %>%
  group_by(sim) %>%
  summarise(max_t = max(t))
  
  # Store the max_t result in the max_t_list
  max_t_list[[paste0("batch_size_", batch_size)]] <- df_TS_zozo_80_max_t
}
```

```{r}
# Create an empty dataframe to store cumulative reward data for all batch sizes
df_cumulative_rewards <- data.frame()

max_t <- 1600

# Loop over each batch size to calculate cumulative rewards and store them
for (batch_size in batch_sizes) {
  
  df_TS_zozo_80 <- results_list[[paste0("batch_size_", batch_size)]]
  
  # Calculate cumulative rewards for each simulation
  df_TS_zozo_80_agg <- df_TS_zozo_80 %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>%
    ungroup() %>%
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward), 
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward,
           batch_size = batch_size) %>%
    filter(t <= max_t)  # Filter by max_t
  
  # Add results to the cumulative rewards dataframe
  df_cumulative_rewards <- bind_rows(df_cumulative_rewards, df_TS_zozo_80_agg)
}

# Plot average cumulative rewards for each batch size
ggplot(df_cumulative_rewards, aes(x = t, y = avg_cumulative_reward, color = as.factor(batch_size))) +
  geom_line(size = 1.2) +
  labs(x = "Time", y = "Average Cumulative Reward", color = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards for Different Batch Sizes")

# Optional: Add 95% Confidence Intervals to the plot
ggplot(df_cumulative_rewards, aes(x = t, y = avg_cumulative_reward, color = as.factor(batch_size))) +
  geom_line(size = 1.2) +
  geom_ribbon(aes(ymin = cumulative_reward_lower_CI, ymax = cumulative_reward_upper_CI, fill = as.factor(batch_size)), alpha = 0.2) +
  labs(x = "Time", y = "Average Cumulative Reward", color = "Batch Size", fill = "Batch Size") +
  theme_minimal() +
  theme(legend.position = "right") +
  ggtitle("Average Cumulative Rewards with 95% CI for Different Batch Sizes")

```

This graph displays the average cumulative rewards over time for different batch sizes (1, 5, 10, 20, 50, 100). The batch size of 1, represented by the red line, achieves the highest cumulative rewards but with wider confidence intervals, indicating greater variability in performance. As batch sizes increase, the cumulative rewards decrease, with batch sizes like 50 and 100 showing more stable but slower reward accumulation, reflected in narrower confidence intervals. This suggests that smaller batch sizes, which update the model more frequently, lead to faster learning and higher rewards, but with more uncertainty, while larger batch sizes result in more consistent, though less optimal, performance over time.