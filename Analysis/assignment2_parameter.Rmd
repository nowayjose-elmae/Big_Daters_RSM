---
title: "Assignment 2"
author: "Shinyoung"
date: "2024-10-03"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Dataset
```{r}
# this version contains 20 arms (ZOZO)
dfZozo_20 <- read.csv('zozo_noContext_20items_tiny.csv') %>%
  select(item_id, click)
```


#model
```{r}
# set the seed
set.seed(0)

# List of multiple parameter settings for alpha and beta
param_settings <- list(
  list(alpha = 1000, beta = 1000),  # original
  list(alpha = 50, beta = 50),    # lower initial values
  list(alpha = 10, beta = 90),    # imbalanced
  list(alpha = 1, beta = 1)       # uniform prior
)

# Initialize a list to store results from each sensitivity setting
results_list <- list()

length(param_settings[1])

param_settings[1][[1]][[2]]


for (i in 1:length(param_settings)) {
  alpha <- param_settings[[i]]$alpha
  beta <- param_settings[[i]]$beta
  
  # Extract current alpha and beta settings
  #alpha <- param_settings[i][[1]][[1]]
  #beta <- param_settings[i][[1]][[2]]
  
  # Create a dataframe with different alpha and beta per arm (if required)
  df_alpha_beta <- data.frame(arm = 1:20, alpha = alpha, beta = beta)
  
  n_sample <- 100
  
  # Sample n_sample observations from each arm's beta distribution
  df_sampled <- mapply(rbeta, n_sample, df_alpha_beta$alpha, df_alpha_beta$beta)
  
  # Select an arm based on Thompson Sampling (arm with the highest average reward)
  avg_from_sampled <- apply(df_sampled, 2, mean)
  chosen_arm <- which.max(avg_from_sampled)
  
  # OfflineReplayEvaluatorBandit with current settings
  bandit_Zozo_20 <- OfflineReplayEvaluatorBandit$new(
    formula = click ~ item_id,
    data = dfZozo_20,
    randomize = FALSE
  )
  
  size_sim <- 10000
  n_sim <- 10
  
  # Thompson Sampling policy object
  TS <- ThompsonSamplingPolicy$new()
  
  # Create agent with the TS policy and bandit
  agent_TS_zozo_20 <- Agent$new(TS, bandit_Zozo_20)
  
  # Simulator object for running the simulations
  simulator <- Simulator$new(agent_TS_zozo_20, horizon = size_sim, do_parallel = TRUE, simulations = n_sim)
  
  # Run the simulation
  history_TS_zozo_20 <- simulator$run()
  
  # Gather results for the current parameter setting
  df_TS_zozo_20 <- history_TS_zozo_20$data %>%
    select(t, sim, choice, reward, agent)
  
  # Store results with alpha and beta identifiers
  df_TS_zozo_20$alpha <- alpha
  df_TS_zozo_20$beta <- beta
  
  # Append the results to the list
  results_list[[i]] <- df_TS_zozo_20
}

# Combine all results into one dataframe
df_combined_results <- bind_rows(results_list)

```


#graphs
```{r}
# Max of observations. 
# You may want to adjust this depending on the number of observations per simulation and the size of the dataset used
max_obs <- 900

# dataframe transformation
df_history_agg <- df_combined_results %>%
  group_by(sim, alpha, beta) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%
  group_by(t, alpha, beta) %>%
  summarise(
    avg_cumulative_reward = mean(cumulative_reward),
    se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)
  ) %>%
  mutate(
    cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
    cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward
  ) %>%
  filter(t <= max_obs)

# TODO: Make the following two plots:
# 1: A plot that compares the average cumulative rewards over time for 10 and 20 arms using the df_history_agg dataframe
# 2: The plot as defined in (1) together with the 95\% confidence interval.
legend <- c("Avg." = "orange", "95% CI" = "gray") # Set legend

ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward)) +
  geom_line(size=1.5, aes(color="Avg.")) +  # Add line
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0, cumulative_reward_lower_CI), # Confidence interval
                  ymax=cumulative_reward_upper_CI, color = "95% CI"), 
              alpha=0.1) +
  labs(x = 'Time', y='Cumulative Reward', color='Metric') +  # Add titles
  scale_color_manual(values=legend) +  # Add legend
  facet_wrap(~alpha+beta) +  # Facet by alpha and beta
  theme_bw()
```