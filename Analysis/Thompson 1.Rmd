---
title: "Bernoulli Thompson Sampling 1"
output: html_document
date: "2024-10-02"
---

```{r setup, include=FALSE}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

#Dataset

To manage the large dataset more efficiently, we grouped the data by arm and sampled 4% from each group. This approach reduces the computational load while preserving the representativeness of the original dataset. Additionally, we shuffled the sampled data to eliminate any potential ordering bias, because in the original dataset before any transformation the dataset is not ordered by item_id but when we group it, it becomes ordered.

```{r}
# this version contains 80 arms (ZOZO)
set.seed(0)
library(readr)
dfZozo_80 <- read_csv("~/Downloads/zozo_Context_80items.csv.zip")%>%
  select(item_id, click) %>%
  group_by(item_id) %>%
  sample_frac(0.04) %>%
  ungroup()%>%          # Ungroup to remove the group structure
  arrange(sample(n()))   # Randomly shuffle the rows


```
# Thompson Sampling: notation

```{=tex}
\begin{enumerate}
  \item The reward $Q_t(a)$ for each arm follows a beta distribution. At the start, at $t = 0$, every arm is assumed to follow the same beta distribution with parameters $\alpha_0 = 1$ and $\beta_0 = 1$. This means we initially assume each arm has an equal probability of giving a reward. 
  \item At each time $t$, the algorithm samples $n_{\mathrm{sample}}$ observations from each of the distributions. The arm which has the highest average reward according to the sample is the chosen arm. 
  \item We then observe the reward $r_t$, and update the parameters of the distribution accordingly. 
  \begin{itemize}
    \item $\alpha_t = \alpha_{t-1} + r_t$
    \item $\beta_t = \beta_{t-1} + 1 - r_t$
  \end{itemize}
\end{enumerate}
```

\textbf{Task 1: select from the dataframe below an arm according to the Thompson Sampling algorithm}. Per arm, the alpha and beta parameters have been given. Use $n_{\mathrm{sample}}=100$. 

```{r}

# set the seed
set.seed(0)

# arm index
arm <- 1:80

# alpha per arm
alpha <- rep(1, length(arm))

# beta per arm
beta <- rep(1, length(arm))

# dataframe with alpha, beta per arm
df_alpha_beta <- data.frame(arm=arm,alpha = alpha, beta = beta)

# number of samples to draw
n_sample <- 100

# create dataframe with sampled values
df_sampled <- mapply(rbeta, n_sample, df_alpha_beta$alpha, df_alpha_beta$beta)

# average per sample
avg_from_sampled <- apply(df_sampled, 2, mean)


# get the arm with the highest average
chosen_arm <- which.max(avg_from_sampled)

chosen_arm
```




```{r, results='hide', message=FALSE, warning=FALSE}

library(contextual)

# set the seed
set.seed(0)

## OfflineReplayEvaluatorBandit: simulates a bandit based on provided data
#
# Arguments:
#
#   data: dataframe that contains variables with (1) the reward and (2) the arms
#
#   formula: should consist of variable names in the dataframe. General structure is: 
#       reward variable name ~ arm variable name
#
#   randomize: whether or not the bandit should receive the data in random order,
#              or as ordered in the dataframe.
#

# in our case, create a bandit for the data with 10 arms, 
#   formula = click ~ item_id, 
#   no randomization 

bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                   data = dfZozo_80,
                                                   randomize = FALSE)

# lets generate 10 simulations, each of size 54266
size_sim <- 54266
n_sim<- 10
# here we define the Thompson Sampling policy object
TS          <- ThompsonSamplingPolicy$new()

# the contextual package works with 'agent' objects - which consist of a policy and a bandit
agent_TS_zozo_80 <-  Agent$new(TS, # add policy
                               bandit_Zozo_80) # add bandit


## simulator: simulates a bandit + policy based on the provided data and parameters
#
# Arguments:
#
#   agent: the agent object, previously defined
#
#   horizon: how many observations from dataset used in the simulation
#
#   do_parallel: if TRUE, runs simulation in parallel
#
#   simulations: how many simulations?
#

simulator          <- Simulator$new(agent_TS_zozo_80, # set our agent
                                      horizon= size_sim, # set the size of each simulation
                                      do_parallel = TRUE, # run in parallel for speed
                                      simulations = n_sim, # simulate it n_sim times
                                  )
  
  
# run the simulator object  
history_TS_zozo_80            <- simulator$run()

# gather results
df_TS_zozo_80 <- history_TS_zozo_80$data %>%
  select(t, sim, choice, reward, agent) 



```

Now that we have gathered the results, let's dive a little bit deeper into each component. First, the bandit object, *OfflineReplayEvaluatorBandit*. The idea behind this function is that it 'replays' the results of an algorithm/policy based on random data. Once an algorithm picks an arm, for instance arm 1, it takes the first observation of arm 1 in the dataset. If the algorithm again picks arm 1, it again samples the next observation from arm 1, etc.

Let us calculate per simulation (1-10), the maximum number of observations.

```{r}
df_TS_zozo_80_max_t <- df_TS_zozo_80%>%
  group_by(sim) %>% # group by per agent
  summarize(max_t = max(t)) # get max t

df_TS_zozo_80_max_t
```
The results show that the maximum number of rounds across 10 simulations varied between 435 and 490, with simulation 8 reaching the highest number of rounds (490) and simulation 5 the lowest (435). While most simulations fall within a narrow range, this variation suggests differences in exploration patterns, where some simulations explored more arms or took more actions before reaching the end of the dataset.

In the dataframe *df_TS_zozo* we have gathered the results of the TS policy. This dataframe contains the following columns:

```{=tex}
\begin{itemize}
\item \textbf{$t$}: the step at which a choice was made
\item \textbf{sim}: the simulation for which we observe results
\item \textbf{choice}: the choice made by the algorithm
\item \textbf{reward}: the reward observed by the algorithm
\item \textbf{agent}: column containing the name of the agent 
\end{itemize}
```

\textbf{Task 3: using this dataframe, make a plot of the average cumulative rewards for all simulations, together with the 95\% confidence interval}.

```{r}

# Max of observations. Adjusted to the number of observations per simulation (min 831) and the size of the dataset used
max_obs <- 650
# dataframe transformation
df_history_agg <- df_TS_zozo_80 %>%
  group_by( sim) %>% # group by simulation
  mutate(cumulative_reward = cumsum(reward))%>% # calculate, per sim, cumulative reward over time
  group_by( t) %>% # group by time
  summarise(avg_cumulative_reward = mean(cumulative_reward), # average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>% # SE + Confidence interval
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)


# TODO: Make the following two plots:
# 1: A plot that shows only the average cumulative rewards over time using the df_history_agg dataframe
# 2: The plot as defined in (1) together with the 95\% confidence interval. 

legend <- c("Avg." = "orange", "95% CI" = "gray") # set legend


ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward)) +
  geom_line(size=1.5,aes(color="Avg.")) + # add line
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0,cumulative_reward_lower_CI), # add confidence interval
                  ymax=cumulative_reward_upper_CI,
                  color = "95% CI"), 
              alpha=0.1) +
  labs(x = 'Time', y='Average Cumulative Reward', color='Metric') + # add titles
  scale_color_manual(values=legend) + # add legend
  theme_bw() # set the theme

results <- data.frame(Round = numeric(), ChosenArm = numeric(), Reward = numeric()) # initialize results dataframe

for (i in 1:n_distinct(df_TS_zozo_80$sim)) { # ensure sim count is correct
  current_sim <- df_TS_zozo_80 %>% filter(sim == i)
  
  for (t in 1:nrow(current_sim)) {
    item_id <- current_sim$choice[t]
    click <- current_sim$reward[t]
    
    results <- rbind(results, data.frame(Round = t, ChosenArm = item_id, Reward = click))
  }
}

# Plot for Thompson Sampling Arm Choices Over Time
ggplot(results, aes(x = Round, y = as.factor(ChosenArm))) +
  geom_point(aes(color = as.factor(Reward))) +
  labs(x = "Round", y = "Chosen Arm", title = "Thompson Sampling Arm Choices Over Time") +
  scale_color_manual(values = c("0" = "red", "1" = "green"), name = "Reward") +
  scale_y_discrete(breaks = function(x) x[seq(1, length(x), by = 5)]) + # Show every 5th arm
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 45, size = 6)) # Adjust y-axis label size and orientation

```

We started by generating batches with sizes that increase by 1262, beginning from 1262, and continuing until the maximum batch size reaches 54266. The number 1262 was chosen because it is a divisor of 54266, ensuring that we can increment the batch sizes evenly and consistently, maintaining equal-sized increments throughout the process.
```{r}
batch_size <- 1262
n <- nrow(dfZozo_80) 

# Create an empty list to store batches
batches <- list()

# Determine how many complete batches we can make
div <- seq(batch_size, n, batch_size)

# Loop to divide the dataset into batches
for(i in 1:length(div)) {
    start_row <- 1  # Always start at the beginning for cumulative batching
    end_row <- div[i]  # End of the batch is increasing in each loop
    batches[[i]] <- dfZozo_80[start_row:end_row, ]  # Subset each batch cumulatively
}

```

```{r}
set.seed(0)
# Thompson Sampling policy
TS <- ThompsonSamplingPolicy$new()

# Create an empty list to store the history of each batch
history_batches <- list()

# Loop through each batch and update the model only after processing the entire batch
for (i in 1:length(batches)) {
  
  # Create bandit for each batch without any internal updates
  bandit_batch <- OfflineReplayEvaluatorBandit$new(
    formula = click ~ item_id,
    data = batches[[i]],
    randomize = FALSE
  )
  
  # Create the agent with the Thompson Sampling policy and batch-specific bandit
  agent_batch <- Agent$new(TS, bandit_batch)
  
  # Set up simulator for the batch without updating the policy inside the batch
  simulator <- Simulator$new(
    agent_batch,
    horizon = nrow(batches[[i]]), # Use the batch size for each simulation
    do_parallel = TRUE,
    simulations = n_sim
  )
  
  # Run the simulation for the batch
  history_batch <- simulator$run()
  
  # Get the results and add the batch number
  history_batch_data <- history_batch$data
  history_batch_data$batch_num <- i
  
  # Store the result
  history_batches[[i]] <- history_batch_data

  # Print batch progress
  print(paste("Completed batch", i, "of", length(batches)))
}


```



```{r}

# Max of observations. Adjusted to the number of observations per simulation (min 831) and the size of the dataset used
max_obs <- 650
# dataframe transformation
df_history_agg <- df_TS_combined %>%
  group_by( sim) %>% # group by simulation
  mutate(cumulative_reward = cumsum(reward))%>% # calculate, per sim, cumulative reward over time
  group_by( t) %>% # group by time
  summarise(avg_cumulative_reward = mean(cumulative_reward), # average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>% # SE + Confidence interval
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)


# TODO: Make the following two plots:
# 1: A plot that shows only the average cumulative rewards over time using the df_history_agg dataframe
# 2: The plot as defined in (1) together with the 95\% confidence interval. 

legend <- c("Avg." = "orange", "95% CI" = "gray") # set legend


ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward)) +
  geom_line(size=1.5,aes(color="Avg.")) + # add line
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0,cumulative_reward_lower_CI), # add confidence interval
                  ymax=cumulative_reward_upper_CI,
                  color = "95% CI"), 
              alpha=0.1) +
  labs(x = 'Time', y='Average Cumulative Reward', color='Metric') + # add titles
  scale_color_manual(values=legend) + # add legend
  theme_bw() # set the theme

results <- data.frame(Round = numeric(), ChosenArm = numeric(), Reward = numeric()) # initialize results dataframe

for (i in 1:n_distinct(df_TS_zozo_80$sim)) { # ensure sim count is correct
  current_sim <- df_TS_zozo_80 %>% filter(sim == i)
  
  for (t in 1:nrow(current_sim)) {
    item_id <- current_sim$choice[t]
    click <- current_sim$reward[t]
    
    results <- rbind(results, data.frame(Round = t, ChosenArm = item_id, Reward = click))
  }
}

# Plot for Thompson Sampling Arm Choices Over Time
ggplot(results, aes(x = Round, y = as.factor(ChosenArm))) +
  geom_point(aes(color = as.factor(Reward))) +
  labs(x = "Round", y = "Chosen Arm", title = "Thompson Sampling Arm Choices Over Time") +
  scale_color_manual(values = c("0" = "red", "1" = "green"), name = "Reward") +
  scale_y_discrete(breaks = function(x) x[seq(1, length(x), by = 5)]) + # Show every 5th arm
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 45, size = 6)) # Adjust y-axis label size and orientation

```


#Supposedly this is the right version of batching

```{r}
# Function to split the dataset into batches based on batch size
split_into_batches <- function(dfZozo_80, batch_size) {
  n <- nrow(dfZozo_80)
  num_batches <- ceiling(n / batch_size)  # Calculate the number of batches
  batches <- list()
  
  for (i in 1:num_batches) {
    start_row <- ((i - 1) * batch_size) + 1
    end_row <- min(i * batch_size, n)
    batches[[i]] <- dfZozo_80[start_row:end_row, ]
  }
  
  return(batches)
}

# Define different batch sizes you want to test
batch_sizes <- c (1262, 27133, 54266)  # Example of batch sizes to test
results_list <- list()

# Loop over each batch size and run the model
for (batch_size in batch_sizes) {
  
  # Split the dataset into batches for the current batch size
  batches <- split_into_batches(dfZozo_80, batch_size)
  
  # Empty list to store the history for each batch
  history_batches <- list()
  
  for (i in 1:length(batches)) {
    # Thompson Sampling policy
    TS <- ThompsonSamplingPolicy$new()
    
    # Create bandit for the current batch
    bandit_batch <- OfflineReplayEvaluatorBandit$new(
      formula = click ~ item_id,
      data = batches[[i]],
      randomize = FALSE
    )
    
    # Create the agent with the Thompson Sampling policy and batch-specific bandit
    agent_batch <- Agent$new(TS, bandit_batch)
    
    # Set up simulator for the batch
    simulator <- Simulator$new(
      agent_batch,
      horizon = nrow(batches[[i]]),  # Use the batch size for each simulation
      do_parallel = TRUE,
      simulations = n_sim
    )
    
    # Run the simulation for the current batch
    history_batch <- simulator$run()
    
    # Get the results and add the batch number
    history_batch_data <- history_batch$data
    history_batch_data$batch_num <- i
    
    # Store the result for this batch
    history_batches[[i]] <- history_batch_data
    
    # Print batch progress
    print(paste("Completed batch", i, "for batch size", batch_size))
  }
  
  # Combine all batch histories into a single dataframe for the current batch size
  df_TS_combined <- do.call(rbind, history_batches) %>%
    select(t, sim, choice, reward, agent, batch_num)
  
  # Store results in a list with batch size as the key
  results_list[[paste0("batch_size_", batch_size)]] <- df_TS_combined
}

# You can now access the results for each batch size by calling:
# results_list[["batch_size_1262"]]
# results_list[["batch_size_2524"]]
# etc.

# Track the maximum t (time) for each simulation for batch size 1262 as an example
df_TS_zozo_combined_max_t <- results_list[["batch_size_1262"]] %>%
  group_by(sim) %>%
  summarize(max_t = max(t))

# View the max time per simulation
df_TS_zozo_combined_max_t

```









