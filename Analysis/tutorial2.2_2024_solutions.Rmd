---
title: "Tutorial 2.2 - Thompson Sampling"
author: "Tayyip Altan"
date: "October 2024"
output:
  pdf_document:
    fig_caption: yes
header-includes: 
  \usepackage{float} 
  \usepackage{booktabs} % To thicken table lines
urlcolor: blue
---

```{r setup, include=FALSE}
rm(list=ls())
# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

In this tutorial, we will be covering the Thompson Sampling (TS) method to solve multi-armed bandit problems.

# Dataset
```{r}
set.seed(0)
# Set the seed for reproducibility

# Load the dataset
dfZozo_80 <- read.csv('zozo_Context_80items.csv') %>%
  select(item_id, click, position, user_feature_0, user_feature_1, user_feature_2, user_feature_3)

# Set simulation parameters
size_sim <- 10000  # Number of impressions per simulation
n_sim <- 10        # Number of simulations
```

# Thompson Sampling: notation
#Thompson Sampling
```{r, results='hide', message=FALSE, warning=FALSE}
# Create bandit for 80 arms
bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                   data = dfZozo_80,
                                                   randomize = FALSE)

# Define the Thompson Sampling policy
TS <- ThompsonSamplingPolicy$new()

# Create the agent for 80 arms
agent_TS_zozo_80 <- Agent$new(TS, bandit_Zozo_80)

# Run the simulator for Thompson Sampling
simulator_80 <- Simulator$new(agent_TS_zozo_80, 
                              horizon = size_sim, 
                              simulations = n_sim)

history_TS_zozo_80 <- simulator_80$run()

# Gather results for Thompson Sampling
df_TS_zozo_80 <- history_TS_zozo_80$data %>%
  select(t, sim, choice, reward, agent)  # Gather the simulation results

# Map the 'choice' column back to the original item_id and user features
df_mapping <- dfZozo_80 %>%
  distinct(item_id, user_feature_0, user_feature_1, user_feature_2, user_feature_3) %>%
  mutate(choice = 1:n())  # Create a choice mapping

# Join with the mapping to include item_id and user features in the results
df_TS_zozo_80 <- df_TS_zozo_80 %>%
  left_join(df_mapping, by = "choice")

```

#random policy
```{r}
# Create a Random Policy agent
RandomPolicy <- RandomPolicy$new()

# Create the agent for Random Policy
agent_Random <- Agent$new(RandomPolicy, bandit_Zozo_80)

# Run the simulator for Random Policy
simulator_random <- Simulator$new(agent_Random, 
                                  horizon = size_sim, 
                                  simulations = n_sim)

history_random <- simulator_random$run()

# Gather results for Random Policy
df_random <- history_random$data %>%
  select(t, sim, choice, reward, agent) %>%
  left_join(df_mapping, by = "choice")

```

#comparison of CTRs
```{r}
# Function to calculate CTR
calculate_ctr <- function(data) {
  data %>%
    group_by(sim) %>%
    summarise(CTR = sum(reward) / n())
}

# Calculate CTR for Thompson Sampling
ctr_TS <- calculate_ctr(df_TS_zozo_80)

# Calculate CTR for Random Policy
ctr_random <- calculate_ctr(df_random)

# Combine results into one data frame for plotting
ctr_results <- bind_rows(
  mutate(ctr_TS, method = "Thompson Sampling"),
  mutate(ctr_random, method = "Random Policy")
)

# Plot the CTRs for comparison
ggplot(ctr_results, aes(x = method, y = CTR, fill = method)) +
  geom_boxplot() +
  labs(title = "Comparison of Click-Through Rates", y = "CTR", x = "Method") +
  theme_minimal()

```

#sensitivity analysis
```{r}
# Function to run simulations with different batch sizes
sensitivity_analysis <- function(batch_size) {
  # Recreate bandit
  bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                     data = dfZozo_80,
                                                     randomize = FALSE)
  
  agent_TS <- Agent$new(TS, bandit_Zozo_80)
  
  # Run simulator
  simulator <- Simulator$new(agent_TS, horizon = batch_size, simulations = n_sim)
  history <- simulator$run()
  
  # Gather simulation results and map back to original data
  df_results <- history$data %>%
    select(t, sim, choice, reward, agent) %>%
    left_join(df_mapping, by = "choice")
  
  return(calculate_ctr(df_results))
}

# Test different batch sizes
batch_sizes <- c(5000, 7000, 9000, 10000)
sensitivity_results <- lapply(batch_sizes, sensitivity_analysis)

# Combine sensitivity results for plotting
sensitivity_df <- bind_rows(
  lapply(seq_along(sensitivity_results), function(i) {
    data.frame(batch_size = batch_sizes[i], CTR = sensitivity_results[[i]]$CTR)
  })
)

# Plot sensitivity analysis results
ggplot(sensitivity_df, aes(x = factor(batch_size), y = CTR, fill = factor(batch_size))) +
  geom_boxplot() +
  labs(title = "Sensitivity Analysis of CTR by Batch Size", y = "CTR", x = "Batch Size") +
  theme_minimal()

```

#aggregation/heterogeneity analysis
```{r}
# After running the simulation
df_TS_zozo_80 <- history_TS_zozo_80$data %>%
  select(t, sim, choice, reward, agent)  # Gather simulation results

# Join back with the original dataset to get user features (user_feature_0)
df_TS_zozo_80 <- df_TS_zozo_80 %>%
  left_join(dfZozo_80, by = choice)  # Join with original data based on 'choice'

# Now you can group by user_feature_0 and calculate CTR
user_feature_results <- df_TS_zozo_80 %>%
  group_by(user_feature_0) %>%
  summarise(CTR = sum(reward) / n())

# Plot CTR by User Feature
ggplot(user_feature_results, aes(x = user_feature_0, y = CTR)) +
  geom_bar(stat = "identity") +
  labs(title = "CTR by User Feature 0", y = "CTR", x = "User Feature 0") +
  theme_minimal()

```

#parameter tuning
```{r}
# Function to run simulations with varying alpha and beta
parameter_tuning <- function(alpha_init, beta_init) {
  bandit_Zozo_80 <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                     data = dfZozo_80,
                                                     randomize = FALSE)
  
  # Adjust alpha and beta for Thompson Sampling
  TS <- ThompsonSamplingPolicy$new(alpha = alpha_init, beta = beta_init)
  agent <- Agent$new(TS, bandit_Zozo_80)
  
  simulator <- Simulator$new(agent, horizon = size_sim, simulations = n_sim)
  history <- simulator$run()
  
  df_results <- history$data %>%
    select(t, sim, choice, reward, agent)
  
  return(calculate_ctr(df_results))
}

# Test different initial parameters
alpha_values <- c(1, 5, 10)
beta_values <- c(1, 5, 10)
results <- expand.grid(alpha = alpha_values, beta = beta_values)

# Store CTR for each combination
results$CTR <- apply(results, 1, function(x) {
  param_result <- parameter_tuning(x[1], x[2])
  mean(param_result$CTR)
})

# Plot parameter tuning results
ggplot(results, aes(x = factor(alpha), y = CTR, fill = factor(beta))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "CTR by Alpha and Beta Parameters", y = "CTR", x = "Alpha") +
  theme_minimal() +
  scale_fill_discrete(name = "Beta")
```

Now that we have gathered the results, let's dive a little bit deeper into each component. First, the bandit object, *OfflineReplayEvaluatorBandit*. The idea behind this function is that it 'replays' the results of an algorithm/policy based on random data. Once an algorithm picks an arm, for instance arm 1, it takes the first observation of arm 1 in the dataset. If the algorithm again picks arm 1, it again samples the next observation from arm 1, etc.

Let us calculate per simulation (1-10), the maximum number of observations.

```{r}
df_TS_zozo_80_max_t <- df_TS_zozo_80 %>%
  group_by(sim) %>% # group by simulation
  summarize(max_t = max(t)) # get the maximum time step per simulation

df_TS_zozo_80_max_t
```

\textbf{Task 2: answer below: why is the maximum number of observations different for each simulation?}

\textbf{Your answer to task 2 here}: when we use offline replay evaluation, there is only a limited number of observations per arm (if we do not sample with replacement). So if one simulation picks a certain arm more often, at some point there are no more observations to sample. Different simulations reach this point at different moments.

In the dataframe *df_TS_zozo* we have gathered the results of the TS policy. This dataframe contains the following columns:

```{=tex}
\begin{itemize}
\item \textbf{$t$}: the step at which a choice was made
\item \textbf{sim}: the simulation for which we observe results
\item \textbf{choice}: the choice made by the algorithm
\item \textbf{reward}: the reward observed by the algorithm
\item \textbf{agent}: column containing the name of the agent 
\end{itemize}
```

\textbf{Task 3: using this dataframe, make a plot of the average cumulative rewards for all simulations, together with the 95\% confidence interval}.

```{r}
# Max observations to consider (you can adjust this as needed)
max_obs <- 125

# Calculate cumulative rewards and confidence intervals
df_history_agg_80 <- df_TS_zozo_80 %>%
  group_by(sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>% # Calculate cumulative reward per simulation
  group_by(t) %>%
  summarise(avg_cumulative_reward = mean(cumulative_reward), # Calculate average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>% # Calculate SE
  mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
         cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
  filter(t <= max_obs) # Limit the number of time steps

# Plot the results with confidence intervals
legend <- c("Avg." = "orange", "95% CI" = "gray")

ggplot(data = df_history_agg_80, aes(x = t, y = avg_cumulative_reward)) +
  geom_line(size = 1.5, aes(color = "Avg.")) + # Line for average cumulative reward
  geom_ribbon(aes(ymin = ifelse(cumulative_reward_lower_CI < 0, 0, cumulative_reward_lower_CI), # Confidence interval lower bound
                  ymax = cumulative_reward_upper_CI, # Confidence interval upper bound
                  color = "95% CI"), alpha = 0.1) + # Confidence interval ribbon
  labs(x = 'Time', y = 'Cumulative Reward', color = 'Metric') + # Labels
  scale_color_manual(values = legend) + # Add legend
  theme_bw() # Use a clean theme

```
