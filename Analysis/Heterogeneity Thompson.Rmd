---
title: "Bernoulli Thompson Sampling 1 Final"
output: html_document
date: "2024-10-02"
---

```{r setup, include=FALSE}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

```

#Dataset

```{r}
# this version contains 80 arms (ZOZO)
set.seed(0)
library(readr)
dfZozo_80 <- read_csv("~/Downloads/zozo_Context_80items.csv.zip")%>%
  select(item_id, click, position, user_feature_0, user_feature_1,user_feature_2, user_feature_3)
  


```
# Thompson Sampling: notation

```{=tex}
\begin{enumerate}
  \item The reward $Q_t(a)$ for each arm follows a beta distribution. At the start, at $t = 0$, every arm is assumed to follow the same beta distribution with parameters $\alpha_0 = 1$ and $\beta_0 = 1$. This means we initially assume each arm has an equal probability of giving a reward. 
  \item At each time $t$, the algorithm samples $n_{\mathrm{sample}}$ observations from each of the distributions. The arm which has the highest average reward according to the sample is the chosen arm. 
  \item We then observe the reward $r_t$, and update the parameters of the distribution accordingly. 
  \begin{itemize}
    \item $\alpha_t = \alpha_{t-1} + r_t$
    \item $\beta_t = \beta_{t-1} + 1 - r_t$
  \end{itemize}
\end{enumerate}
```

#Heterogeneity and Aggregation Analysis
```{r}
dfZozo_80 %>%
  group_by(user_feature_0) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_1) %>%
  summarise(n())

dfZozo_80 %>%
  group_by(user_feature_2) %>%
  summarise(n())


dfZozo_80 %>%
  group_by(user_feature_3) %>%
  summarise(n())
```
Since some user features, such as features 1, 2, and 3, contain very small sample sizes (e.g., values as low as 16), including these in our analysis would lead to unreliable estimates and reduce the statistical power of the results. Small sample sizes can introduce significant variability, making it difficult to draw robust and meaningful conclusions. Therefore, we are excluding these user features from the analysis to ensure the focus is on groups with larger, more balanced sample sizes, which will provide more reliable insights.

```{r}
dfZozo_feat0 <- dfZozo_80%>%
  filter(user_feature_0 == "4ae385d792f81dde128124a925a830de")

dfZozo_feat1 <- dfZozo_80%>%
  filter(user_feature_0 == "574464659df0fc5bac579eff2b1fff99")

dfZozo_feat2 <- dfZozo_80%>%
  filter(user_feature_0 == "81ce123cbb5bd8ce818f60fb3586bba5")

dfZozo_feat3 <- dfZozo_80%>%
  filter(user_feature_0 == "cef3390ed299c09874189c387777674a")
```


```{r}
# Define the feature datasets in a list
feature_datasets <- list(dfZozo_feat0, dfZozo_feat1, dfZozo_feat2, dfZozo_feat3)

# Define a vector with dataset names for better result tracking
dataset_names <- c("dfZozo_feat0", "dfZozo_feat1", "dfZozo_feat2", "dfZozo_feat3")

# Parameters for the simulations
size_sim <- 6641
n_sim <- 10

# Store results for each dataset
results_list <- list()

# Loop through each feature dataset and run the TS simulation
for (i in seq_along(feature_datasets)) {
  # Create the bandit object for the current dataset
  bandit <- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                             data = feature_datasets[[i]],
                                             randomize = FALSE)
  
  # Define the Thompson Sampling policy
  TS <- ThompsonSamplingPolicy$new()
  
  # Create the agent
  agent <- Agent$new(TS, bandit)
  
  # Create the simulator
  simulator <- Simulator$new(agent,
                             horizon = size_sim,
                             do_parallel = TRUE,
                             simulations = n_sim)
  
  # Run the simulator
  history <- simulator$run()
  
  # Gather the results and store them in the list
  df_TS <- history$data %>%
    select(t, sim, choice, reward, agent)
  
  # Add results to the list with the corresponding dataset name
  results_list[[dataset_names[i]]] <- df_TS
  
  # Calculate the maximum number of observations per simulation
  df_TS_max_t <- df_TS %>%
    group_by(sim) %>%
    summarize(max_t = max(t))
  
  # Print the maximum observations per simulation for this dataset
  print(paste("Max observations for", dataset_names[i]))
  print(df_TS_max_t)
}


```
```{r}

max_t <- 60
# Prepare an empty dataframe to store the aggregated results
df_history_all <- data.frame()

# Loop through each dataset, calculate cumulative rewards, and append to the dataframe
for (name in names(results_list)) {
  df_history_agg <- results_list[[name]] %>%
    group_by(sim) %>%
    mutate(cumulative_reward = cumsum(reward)) %>% # calculate cumulative reward per sim
    group_by(t) %>%
    summarise(avg_cumulative_reward = mean(cumulative_reward),
              se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim)) %>%
    mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
           cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
    filter(t <= max_t) %>%
    mutate(dataset = name) # Add dataset name to differentiate

  # Combine with other datasets
  df_history_all <- rbind(df_history_all, df_history_agg)
}

# Plotting the cumulative reward behavior across all datasets
ggplot(df_history_all, aes(x = t, y = avg_cumulative_reward, color = dataset)) +
  geom_line(size = 1.5) + # Add line for average cumulative reward
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0), ymax = cumulative_reward_upper_CI, fill = dataset), alpha = 0.1) + # Add confidence interval ribbons
  labs(x = 'Time', y = 'Average Cumulative Reward', color = 'Dataset', fill = 'Dataset') + # Labels and legends
  theme_bw() + # Set theme
  theme(legend.position = "bottom") # Position the legend at the bottom

```
The graph shows the average cumulative reward (user clicks) over time for four datasets: dfZozo_feat0, dfZozo_feat1, dfZozo_feat2, and dfZozo_feat3. The green line representing dfZozo_feat1 demonstrates the highest cumulative reward, peaking at around 0.75, indicating it is the most successful strategy for driving clicks. The blue line (dfZozo_feat2) also performs well but at a slightly lower level, while the red line (dfZozo_feat0) shows moderate success, reaching about 0.25. In contrast, the purple line (dfZozo_feat3) remains flat, indicating this strategy was ineffective in generating clicks for clients with this characteristic. The step-like increases in the green and blue lines suggest bursts of success at specific points, followed by plateaus, while the lack of growth in the purple line suggests little to no user engagement for that feature set




