---
title: "FINAL ASSIGNMENT 2"
author: "Tobi"
date: "2024-10-20"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Introduction

Decision-making problems, where an agent must choose the best action
from several alternatives at each point in time, are widely encountered
in practical applications, such as when companies decide how to
structure their websites to encourage shopping behavior. These problems
involve a trade-off between exploration and exploitation (Lai & Robbins,
1985; Auer et al., 2002). This trade-off is typically formulated as the
multi-armed bandit problem, where each possible action, or arm, has an
unknown reward probability distribution and the agent aims to maximize
cumulative rewards over time by selecting the optimal arm (Bouneffouf &
Rish, 2019).

In this assignment, we apply the MAB framework to a real-world dataset
from Zozo, a high-traffic fashion website. Here, the arms correspond to
different fashion items, and the rewards are based on user clicks on the
advertisement banner. The goal is to determine which items are best to
recommend and identify the most effective positions on the
recommendation interface (left, center, or right) to maximize
click-through rates (CTR).

To achieve this, we will first describe and analyze the dataset. Then,
we will test several MAB models, including Thompson Sampling, ε-greedy,
and Upper Confidence Bound (UCB), to identify the best-performing items
and banner positions for maximizing clicks. Additionally, we will
perform a heterogeneity analysis to examine how different user segments
impact the models’ performance, a batch analysis to assess how varying
batch sizes affect outcomes, and a parameter analysis to improve the
models.

The dataset captures user interactions with recommended fashion items
and includes variables such as timestamps of impressions, item IDs,
positions on the interface, and click indicator. It also contains
propensity scores, which represent the probability of an item being
recommended in each position, along with several user-related features
such as age and gender.

# II. Download Libraries

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               dplyr,
               contextual, 
               knitr
               )
```

# III. Download Dataset

Given that there were three options for datasets, the analysis was
performed using the 20items_tiny. This was a decision powered by the
fact that it provides more arms than the smallest dataset, and the same
amount of variables as the biggest dataset. These variables include the
unique agent features which are a fundamental part in the data
aggregation.

```{r}
df <- read.csv("~/Downloads/zozo_noContext_20items_tiny.csv")

#Selecting the two relevant columns from ZOZO dataset; arm shown to user and reward observed
df_for_sim <- df %>% select(item_id, click)
df_for_sim$index <- 1:nrow(df_for_sim)

```

# IV Descriptive Statistics

## D1.1 Click Distribution - Raw data

The first distribution is by number of clicks (graph below) and it aims
to show how many interactions result in click or not. Visibly, the
non-click is 99.8% (9980 obs.) of all observations while the clicked
items are only 0.2% (20 obs.).

```{r, distribution clicks }

set.seed(123)

click_distribution <- df %>%
  group_by(click) %>%
  summarise(count = n()) %>%
  mutate(percentage = (count / sum(count)) * 100)

print(click_distribution)

ggplot(click_distribution, aes(x = factor(click), y = count, fill = factor(click))) +
  geom_bar(stat = "identity", width = 0.6) +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("0"=  "blue", "1"=  "red")) +
  scale_x_discrete(labels = c("0" = "0 = No Click", "1" = "1 = Click")) +
  labs(x = "Click Status", y = "Count", title = "Distribution of Clicks", fill = "Click Status") +
  theme_minimal(base_size = 10) +
  theme(legend.position = "top", plot.title = element_text(hjust = 0.5, face = "bold")) +
  ylim(0, max(click_distribution$count) * 1.1)


```

## D1.2 Click Rate - Raw Data

The second distribution helps us understand which is the most and least
clicked items based on historical data. The distribution presented below
shows that not all items were being clicked. The most clicked item was
number 11, while item 1,3 to 6, 12 and 17 to 20 have no visible bars
meaning that they received no clicks.

```{r, click rate per item}
set.seed(123)

item_click <- df %>%
  group_by(item_id) %>%
  summarise(click_rate = mean(click))

ggplot(item_click, aes(x = factor(item_id), y = click_rate)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(x = "Item ID (arm)", y = "Click Rate", title = "Click Rate by Item ID (arms)") +
  theme_minimal()

```

## D1.3 Clicks by Position - Raw Data

The third histogram shows which position is the best one based on
received clicks. This is position 2 with 8 clicks, followed by position
1 and 3 with 6 clicks.

```{r, clicks per position}
set.seed(123)

clicks_per_position <- df %>%
  group_by(position) %>%
  summarise(click_count = sum(click), .groups = "drop")

ggplot(clicks_per_position, aes(x = factor(position), y = click_count)) +
  geom_bar(stat = "identity", fill = "red") +
  labs(x = "Position", y = "Number of Clicks", title = "Distribution of Clicks per Position") +
  theme_minimal()
```

## D1.4 Clicks by Item ID (arms) and Position - Raw Data

The final histogram/map shows the combination of Arms and Positions. The best combination turns out to be arm 11 and position 3. 

```{r, combo map}
set.seed(123)

click_heatmap_data <- df%>%
  group_by(item_id, position) %>%
  summarise(click_count = sum(click))

#Heatmap
ggplot(click_heatmap_data, aes(x = factor(item_id), y = factor(position), fill = click_count)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "red", name = "Click Count") +
  labs(x = "Item ID (arm)", y = "Position", title = "Clicks by Item ID and Position") +
  theme_minimal()

```

# V Key Assumptions

For the context of this analysis, each operation was computed with the
Set.Seed(123). This is given that the parameters were adapted instead of
the seed. This ensures that the data can be replicated and is consistent
across "random" simulations.

It is assumed that each combination of four features is a unique agent.
Given that the nature of these features is unknown, it is possible that
multiple agents share the same unique features. For example, a tagged
Agent 106 has four unique features. In reality, two people can have the
same unique features depending on their nature. In the analysis, both
would fall under Agent 106. Given the uncertainty
of these features, this decision was made, and hence understanding that more specific features
lead to a more realistic unique agent identification.

# VI Data Aggregation and Heterogeneity

## DH1.1 Heterogeneity and Aggregation

As per how it was addressed in the Overview of Methodology, the analysis
will follow to stages. One in which it is assumed that each data point is
an individual user (Unaggregated) and another in which the interaction
with the website is grouped by the activity of unique users. In the
dataset, each user (agent) has a combination of four unique tags which
will be used to identify unique agents. The identification of the unique
values is given by the code below:

```{r}
set.seed(123)

# Identify the number of unique values and list them for each user feature column
unique_user_features <- list(
  user_feature_0 = unique(df$user_feature_0),
  user_feature_1 = unique(df$user_feature_1),
  user_feature_2 = unique(df$user_feature_2),
  user_feature_3 = unique(df$user_feature_3)
)

# Display the number of unique values and their respective lists
for (i in names(unique_user_features)) {
  cat("\n", i, "- Number of unique values:", length(unique_user_features[[i]]), "\n")
  print(unique_user_features[[i]])
}


```

The results are represented by the table below:

| Feature        | Unique Values |
|----------------|---------------|
| User_feature_0 | 4             |
| User_feaure_1  | 6             |
| User_feature_2 | 9             |
| User_feature_3 | 8             |

: The dataset includes a matrix combination of all of these features,
providing a multiplicity amount of unique agents. Each agent has a unique
combination of these four unique features. Hence, the reproduction of
all possible agents is given by the following code:

```{r}
set.seed(123)

# Extract unique values for each user feature column
user_feature_0 <- unique(df$user_feature_0)
user_feature_1 <- unique(df$user_feature_1)
user_feature_2 <- unique(df$user_feature_2)
user_feature_3 <- unique(df$user_feature_3)

# Create all possible combinations of the user features
all_combinations <- expand.grid(
  user_feature_0 = user_feature_0,
  user_feature_1 = user_feature_1,
  user_feature_2 = user_feature_2,
  user_feature_3 = user_feature_3
)

all_combinations <- all_combinations %>%
  mutate(Agent = paste0("Agent_", row_number()))

print(all_combinations)

write.csv(all_combinations, "agent_combinations.csv", row.names = FALSE)


```

Hence, there are 1728 possible different unique agents accessing the
website. This means that the total amount of samples in the dataset,
**10,000**, are produced by the behavior of these 1728 unique agents.

However, not all of the agents interact the same. Some agents interact
multiple times with the ad, yet they never click. Other agents click
multiple times. For the context and simplicity of this analysis, there
will be a clear distinction between the "Active" agents (Those who have
clicked at least once) and the "Passive" agents (Those who have not clicked).
The analysis will be focused on the "Active Agents". It will
suggest an item and position to users who the algorithm has feedback to
learn from. This is identified with the code below, where the code
filters out those agents who only access, yet never click the ads, the
"Passive Agents" and keeps the "Active Agents" who click on the add at
least one time.

```{r}
set.seed(123)

# Step 1: Extract unique values for each user feature column
user_feature_0 <- unique(df$user_feature_0)
user_feature_1 <- unique(df$user_feature_1)
user_feature_2 <- unique(df$user_feature_2)
user_feature_3 <- unique(df$user_feature_3)

# Step 2: Create all possible combinations of the user features (Agents)
all_combinations <- expand.grid(
  user_feature_0 = user_feature_0,
  user_feature_1 = user_feature_1,
  user_feature_2 = user_feature_2,
  user_feature_3 = user_feature_3
)

all_combinations <- all_combinations %>%
  mutate(Agent = paste0(row_number()))

# Step 3: Merge agents with the original dataset based on the user features
df_with_agents <- df %>%
  left_join(all_combinations, by = c("user_feature_0", "user_feature_1", "user_feature_2", "user_feature_3"))

# Step 4: Group by Agent and filter agents that have at least one click (reward)
agents_with_clicks <- df_with_agents %>%
  group_by(Agent) %>%
  summarize(total_clicks = sum(click, na.rm = TRUE)) %>%
  filter(total_clicks > 0)  # Keep only agents with at least one click

# Step 5: Eliminate agents with no clicks 
df_filtered_agents <- df_with_agents %>%
  filter(Agent %in% agents_with_clicks$Agent)

# Step 6: Sort the filtered dataset by Agent
df_sorted_by_agent <- df_filtered_agents %>%
  arrange(Agent)

# Step 7: View the sorted dataset
print(df_sorted_by_agent)

write.csv(df_sorted_by_agent, "sorted_agents_with_clicks.csv", row.names = FALSE)

#Step 8: Identify how many unique Agents are there
unique_agent_count <- n_distinct(df_filtered_agents$Agent)

```

The aforementioned results exemplify two things. The initial dataset is
reduced to **4442** samples which represent the interactions of
meaningful costumers. Additionally, it identifies that from the **1728**
agents who accessed the website, only **8** agents actually interacted
with the page in a meaningful way. These are the potential customers that
the MAB should target in a way that suggests the right ad and
position for them.

The behavior of each agent can be modelled based on their *exploration*
and *exploitation.* The amount of rows each agent has is a
representation of the length of their "game" or search. The amount of
clicks within those rows represent the degree to which the agent was
able to *exploit* their choice and obtain a reward (click).

The behavior of each of the **8 agents** was summarized by the
relationship between the length of their game (rows) and the amount of
clicks (Rewards)*.* The total result of their behavior within the data
set is summarised in the table below:

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Compute the sum of clicks and the row count per agent
agent_stats <- agents_data %>%
  group_by(Agent) %>%
  summarise(
    total_clicks = sum(click),  
    row_count = n()             
  ) %>%
  arrange(desc(row_count), desc(total_clicks))  

# Step 3: Ranked Table
print(agent_stats)
kable(agent_stats, caption = "Agent Ranking by Rows (Exploration) and Total Clicks")

```

From the aforementioned table it can be seen how **Agent 486** has the
largest exploration and exploitation value. However, the other 7 agents
also are potential customers of ZOZO as they have shown their interest
by clicking. The three methods of MAB's for the model will suggest the
best item, position, and combination for each of these potential
customers.

# VII Epsilon and Epsilon Greedy

## E1 - Epsilon Greedy Analysis - Raw Data

### E1.1 Epsilon-Greedy Algorithm: Notation

In this part, when implementing the epsilon-greedy algorithm choose
the best-performing arm(item_id) from our dataset. The arm that is
chosen is 11, proving that histogram 2 ("Click rate per item") aligns
with the Epsilon Greedy algorithm.

```{r, epsilon greedy notation}
set.seed(123)

df_practice <- df
eps <- 0.1

policy_greedy <- function(df, eps, n_arms=20){
  random_uniform_variable <- runif(1, min=0, max=1)
  
  if (random_uniform_variable < eps){
    
    chosen_arm <- sample(1:n_arms, 1)
  } else {
     df_reward_overview <- df %>%
      drop_na() %>%
      group_by(item_id) %>%
      summarise(reward_size = sum(click, na.rm=TRUE),
                sample_size = n(),
                success_rate = reward_size / sample_size)
    
    chosen_arm <- df_reward_overview$item_id[which.max(df_reward_overview$success_rate)]
  }
  

  return(chosen_arm)
}
chosen_arm_practice <- policy_greedy(df_practice, eps)
print(paste0('The chosen item_id/arm is: ', chosen_arm_practice))

```

### E1.2 Simulation Greedy: Simulates performance of an epsilon-greedy policy

#### Part 1: create two dataframes, one with data before the start of the policy, and one with data after

```{r,epsilon-greedy policy }
set.seed(123)

sim_greedy <- function(df, n_before_sim, n_sim, epsilon, interval=1){
  
  #Number of observations for Raw Data
  n_obs <- nrow(df)

  if(n_sim > (n_obs-n_before_sim)){
    stop("The indicated size of the experiment is bigger than the data provided - shrink the size ")
  }

  #N_before_sim random observations to be used before start policy
  index_before_sim <- sample(1:n_obs, n_before_sim)
  
  df_before_policy <- df[index_before_sim,]
  
  #Save dataframe with all the results at t - to begin with those before the policy
  df_results_at_t <- df_before_policy %>%
    select(item_id, click)
  
  #Create dataframe with data that we can sample from during policy
  df_during_policy <- df[-index_before_sim,]
  
  #Dataframe where the results of storing the policy are stored
  df_results_policy <- data.frame(matrix(NA, nrow = n_sim, ncol = 2))
  colnames(df_results_policy) <- c('item_id', 'click')
}


```

#### Part 2: apply epsilon-greedy algorithm, updating at interval

```{r}
set.seed(123)

  for (i in 1:n_sim){
    if((i==1) || ((i %% interval)==0)){
      chosen_arm <- policy_greedy(df_results_at_t, epsilon)
      current_arm <- chosen_arm
    } else {
      chosen_arm <- current_arm
    }
    
    df_during_policy_arm <- df_during_policy %>%
      filter(item_id == chosen_arm)
    
    if (nrow(df_during_policy_arm) == 0) {
      warning("You have run out of observations from a chosen arm")
      sampled_arm <- sample(1:n_obs, 1, replace = TRUE)
      reward <- df$click[sampled_arm]
    } else {
      sampled_arm <- sample(1:nrow(df_during_policy_arm), 1)
      reward <- df_during_policy_arm$click[sampled_arm]
    }
    
    result_policy_i <- c(chosen_arm, reward)
    
    df_results_policy[i,] <- result_policy_i
    
    df_results_at_t <- rbind(df_results_at_t, result_policy_i)
  }

  results <- list(df_results_of_policy = df_results_policy,
                  df_sample_of_policy = df[-index_before_sim,])
  
  return(results)

```

### E1.3 Parameters - Raw Data

The parameters for Epsilon-Greedy analysis are: first, n_before_sim
indicates the number of observations before the start of
epsilon-greedy policy, we choose 300, while n_sim is timed to simulate,
we choose 4000.

```{r, parameters epsilon greedy}
set.seed(123)

n_before_sim <- 300
n_sim <- 4000

set.seed(0)
```

### E1.4 Epsilon Algorithm - Raw Data

The code below provides the computation of the algorithm with the
epsilon exploration values of 0.05 until 0.45 by 0.05. When comparing
the different results, it can be concluded which Epsilon value has the
best performance and leads to the highest CTR (reward).

```{r, results for diffrent epsilons on full data}
set.seed(123)

ZOZO_for_sim <- df 

result_sim_eps_5 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.05,
                                interval=1)

result_sim_eps_10 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.1,
                                interval=1)

result_sim_eps_15 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.15,
                                interval=1)

result_sim_eps_20 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.2,
                                interval=1)

result_sim_eps_25 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.25,
                                interval = 1)
result_sim_eps_30 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.3,
                                interval = 1)
result_sim_eps_35 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.35,
                                interval = 1)
result_sim_eps_40 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.40,
                                interval = 1)
result_sim_eps_45 <- sim_greedy(ZOZO_for_sim,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.45,
                                interval = 1)


```

Then, the cumulative average rewards were computed for the Epsilon
values chosen. This part of the code is a crucial step in evaluating the
Epsilon value performance.

```{r}
set.seed(123)

#Cumulative average reward for epsilon = 0.05
cumulative_reward_5 <- cumsum(result_sim_eps_5$df_results_of_policy$click)
average_reward_5 <- cumulative_reward_5 / (n_sim)
df_eps_5 <- data.frame(t = 1:n_sim, epsilon = 0.05, avg_reward = average_reward_5)

#Cumulative average reward for epsilon = 0.10
cumulative_reward_10 <- cumsum(result_sim_eps_10$df_results_of_policy$click)
average_reward_10 <- cumulative_reward_10 / (n_sim)
df_eps_10 <- data.frame(t = 1:n_sim, epsilon = 0.10, avg_reward = average_reward_10)

#Cumulative average reward for epsilon = 0.15
cumulative_reward_15 <- cumsum(result_sim_eps_15$df_results_of_policy$click)
average_reward_15 <- cumulative_reward_15 / (n_sim)
df_eps_15 <- data.frame(t = 1:n_sim, epsilon = 0.15, avg_reward = average_reward_15)

#Cumulative average reward for epsilon = 0.20
cumulative_reward_20 <- cumsum(result_sim_eps_20$df_results_of_policy$click)
average_reward_20 <- cumulative_reward_20 / (n_sim)
df_eps_20 <- data.frame(t = 1:n_sim, epsilon = 0.20, avg_reward = average_reward_20)

#Cumulative average reward for epsilon = 0.25
cumulative_reward_25 <- cumsum(result_sim_eps_25$df_results_of_policy$click)
average_reward_25 <- cumulative_reward_25 / (n_sim)
df_eps_25 <- data.frame(t = 1:n_sim, epsilon = 0.25, avg_reward = average_reward_25)

#Cumulative average reward for epsilon = 0.30
cumulative_reward_30 <- cumsum(result_sim_eps_30$df_results_of_policy$click)
average_reward_30 <- cumulative_reward_30 / (n_sim)
df_eps_30 <- data.frame(t = 1:n_sim, epsilon = 0.3, avg_reward = average_reward_30)

#Cumulative average reward for epsilon = 0.35
cumulative_reward_35 <- cumsum(result_sim_eps_35$df_results_of_policy$click)
average_reward_35 <- cumulative_reward_35 / (n_sim)
df_eps_35 <- data.frame(t = 1:n_sim, epsilon = 0.35, avg_reward = average_reward_35)

#Cumulative average reward for epsilon = 0.40
cumulative_reward_40 <- cumsum(result_sim_eps_40$df_results_of_policy$click)
average_reward_40 <- cumulative_reward_40 / (n_sim)
df_eps_40 <- data.frame(t = 1:n_sim, epsilon = 0.4, avg_reward = average_reward_40)

#Cumulative average reward for epsilon = 0.45
cumulative_reward_45 <- cumsum(result_sim_eps_45$df_results_of_policy$click)
average_reward_45 <- cumulative_reward_45 / (n_sim)
df_eps_45 <- data.frame(t = 1:n_sim, epsilon = 0.45, avg_reward = average_reward_45)

```

Finally, it is important to analyze the performance of the epsilons and
compare them. The average reward over time was chosen given that it is
an effective way to evaluate which epsilon has the better performance
over time.

### E1.5 Epsilon Performance - Raw Data

Based on the graph presented below, at time step 4000, epsilon value 0.1
reaches the highest average reward followed by 0.15, 0.25 and 0.4.

```{r, epsalons on full data}
set.seed(123)

df_all_eps <- rbind(df_eps_5,df_eps_10,df_eps_15,df_eps_20, df_eps_25, df_eps_30,df_eps_35, df_eps_40, df_eps_45)

ggplot(df_all_eps, aes(x = t, y = avg_reward, color = factor(epsilon), group = factor(epsilon))) +
  geom_line(size = 1) +
  scale_color_manual(values = c("black","purple","#2ca02c", "red", "pink","orange", "blue","yellow","brown")) +
  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
  labs(title = "Average Reward Over Time for Different Epsilon-Greedy Policies on Full Data",
       x = "Time Steps",
       y = "Average Reward",
       color = "Epsilon Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

The following code presents a separate histogram with its confidence
interval for the best 4 performed Epsilons.

```{r, confidence interval epsilons full data}
set.seed(123)

df_all_eps <- rbind( df_eps_10, df_eps_15, df_eps_25, df_eps_40)

ggplot(df_all_eps, aes(x = t, y = avg_reward, color = factor(epsilon), group = factor(epsilon))) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = avg_reward - 1.96 * sd(avg_reward), ymax = avg_reward + 1.96 * sd(avg_reward), fill = factor(epsilon)), alpha = 0.2, color = NA) +
  scale_color_manual(values = c( "purple", "green", "pink", "yellow")) +
  scale_fill_manual(values = c( "purple", "green", "pink","yellow")) +
  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
  labs(title = "Average Reward Over Time for Different Epsilon-Greedy Policies on Full data",
       x = "Time Steps",
       y = "Average Reward",
       color = "Epsilon Value",
       fill = "Epsilon Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

In order to check if the results are statistically significant, an ANOVA
test was computed. This shows if there are any significant differences
between the average rewards of four best-performed epsilons values.

Based on the results, we can conclude that at 5 % significance level,
there are statistically significant differences in average rewards.

```{r, ANOVA statistically significant}
set.seed(123)

df_all_rewards <- rbind(df_eps_10, df_eps_15, df_eps_25, df_eps_40) 
df_all_rewards$epsilon <- as.factor(df_all_rewards$epsilon)

anova_result <- aov(avg_reward ~ epsilon, data = df_all_rewards)
summary(anova_result)

```

## E2 EPSILON GREEDY FOR AGGREGATED DATA

### E2.1 Epsilon-Greedy Algorithm: Notation on Aggregated Data

```{r}
set.seed(123)

ZOZOdf_Aggregate <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

ZOZO_for_sim_aggregate <- ZOZOdf_Aggregate %>%
  select(item_id, click)

ZOZO_for_sim_aggregate$index <- 1:nrow(ZOZO_for_sim_aggregate)

df_practice_aggregate <- ZOZO_for_sim_aggregate
eps <- 0 

policy_greedy <- function(df, eps, n_arms=20){
  random_uniform_variable <- runif(1, min=0, max=1)
  
  
  if (random_uniform_variable < eps){
    
    chosen_arm <- sample(1:n_arms, 1)
  } else {
     df_reward_overview <- df %>%
      drop_na() %>%
      group_by(item_id) %>%
      summarise(reward_size = sum(click, na.rm=TRUE),
                sample_size = n(),
                success_rate = reward_size / sample_size)
    
    # Pick the arm with the highest success_rate from df_reward_overview
    chosen_arm <- df_reward_overview$item_id[which.max(df_reward_overview$success_rate)]
  }
  
  # Return the chosen arm
  return(chosen_arm)
}
# Get the chosen arm from the function
chosen_arm_practice <- policy_greedy(df_practice_aggregate, eps)
print(paste0('The chosen item_id/arm is: ', chosen_arm_practice))

```

### E2.2 Simulation Greedy: Simulates performance of an epsilon-greedy policy on aggregated data

#### Part 1: create two dataframes, one with data before the start of the policy, and one with data after

```{r, aggregated data epsilon}
set.seed(123)

sim_greedy <- function(df, n_before_sim, n_sim, epsilon, interval=1){
  
  n_obs <- nrow(df)

  if(n_sim > (n_obs-n_before_sim)){
    stop("The indicated size of the experiment is bigger than the data provided - shrink the size ")
  }

  index_before_sim <- sample(1:n_obs, n_before_sim)
  
  df_before_policy <- df[index_before_sim,]
  
  df_results_at_t <- df_before_policy %>%
    select(item_id, click)
  
  df_during_policy <- df[-index_before_sim,]
  
  # dataframe where the results of the policy are stored
  df_results_policy <- data.frame(matrix(NA, nrow = n_sim, ncol = 2))
  colnames(df_results_policy) <- c('item_id', 'click')
}
  
 

```

#### Part 2: apply epsilon-greedy algorithm, updating at interval

```{r}
set.seed(123)

  for (i in 1:n_sim){
    
    if((i==1) || ((i %% interval)==0)){
      
      chosen_arm <- policy_greedy(df_results_at_t, epsilon)
      current_arm <- chosen_arm
    } else {
      #In the case that we do not update, take the current arm
      chosen_arm <- current_arm
    }

    #Select from the data for the experiment the arm chosen
    df_during_policy_arm <- df_during_policy %>%
      filter(item_id == chosen_arm)
    
    #Randomly sample from this arm and observe the reward
    if (nrow(df_during_policy_arm) == 0) {
      warning("You have run out of observations from a chosen arm")
      sampled_arm <- sample(1:n_obs, 1, replace = TRUE)
      reward <- df$click[sampled_arm]
    } else {
      sampled_arm <- sample(1:nrow(df_during_policy_arm), 1)
      reward <- df_during_policy_arm$click[sampled_arm]
    }
    
    
    result_policy_i <- c(chosen_arm, reward)
    
    
    df_results_policy[i,] <- result_policy_i
    
    df_results_at_t <- rbind(df_results_at_t, result_policy_i)
  }

  results <- list(df_results_of_policy = df_results_policy,
                  df_sample_of_policy = df[-index_before_sim,])
  
  return(results)
```

### E2.3 Parameters - Aggregated Data

The parameters from E1.3 were held constant.

```{r, parameters}
set.seed(123)

n_before_sim <- 300
n_sim <- 4000

set.seed(0)
```

### E2.4 Epsilon Greedy Algorithm - Aggregated Data

The Epsilon Analysis was performed with exploration values of 0.05 until
0.45 by 0.05 and on the aggregated data.

```{r, results for different epsilons with aggregated data}
set.seed(123)

result_sim_agg_eps_5 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.05,
                                interval=1)

result_sim_agg_eps_10 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.1,
                                interval=1)

result_sim_agg_eps_15 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.15,
                                interval=1)

result_sim_agg_eps_20 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.2,
                                interval=1)

result_sim_agg_eps_25 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.25,
                                interval = 1)
result_sim_agg_eps_30 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.3,
                                interval = 1)
result_sim_agg_eps_35 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.35,
                                interval = 1)
result_sim_agg_eps_40 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.40,
                                interval = 1)
result_sim_agg_eps_45 <- sim_greedy(ZOZO_for_sim_aggregate,
                                n_before_sim=n_before_sim,
                                n_sim=n_sim,
                                epsilon=0.45,
                                interval = 1)


```

Then, the cumulative average rewards were computed for the Epsilon
values chosen. This part of the code is a crucial step in evaluating the
Epsilon value performance.

```{R, cumulative average reward calculation on aggregated data}
set.seed(123)

# Calculate cumulative average reward for epsilon = 0.05
cumulative_reward_5 <- cumsum(result_sim_agg_eps_5$df_results_of_policy$click)
average_reward_5 <- cumulative_reward_5 / (n_sim)
df_eps_5_agg <- data.frame(t = 1:n_sim, epsilon = 0.05, avg_reward = average_reward_5)

# Calculate cumulative average reward for epsilon = 0.10
cumulative_reward_10 <- cumsum(result_sim_agg_eps_10$df_results_of_policy$click)
average_reward_10 <- cumulative_reward_10 / (n_sim)
df_eps_10_agg <- data.frame(t = 1:n_sim, epsilon = 0.10, avg_reward = average_reward_10)

# Calculate cumulative average reward for epsilon = 0.15
cumulative_reward_15 <- cumsum(result_sim_agg_eps_15$df_results_of_policy$click)
average_reward_15 <- cumulative_reward_15 / (n_sim)
df_eps_15_agg <- data.frame(t = 1:n_sim, epsilon = 0.15, avg_reward = average_reward_15)

# Calculate cumulative average reward for epsilon = 0.20
cumulative_reward_20 <- cumsum(result_sim_agg_eps_20$df_results_of_policy$click)
average_reward_20 <- cumulative_reward_20 / (n_sim)
df_eps_20_agg <- data.frame(t = 1:n_sim, epsilon = 0.20, avg_reward = average_reward_20)

# Calculate cumulative average reward for epsilon = 0.25
cumulative_reward_25 <- cumsum(result_sim_agg_eps_25$df_results_of_policy$click)
average_reward_25 <- cumulative_reward_25 / (n_sim)
df_eps_25_agg <- data.frame(t = 1:n_sim, epsilon = 0.25, avg_reward = average_reward_25)

# Calculate cumulative average reward for epsilon = 0.30
cumulative_reward_30 <- cumsum(result_sim_agg_eps_30$df_results_of_policy$click)
average_reward_30 <- cumulative_reward_30 / (n_sim)
df_eps_30_agg <- data.frame(t = 1:n_sim, epsilon = 0.3, avg_reward = average_reward_30)

# Calculate cumulative average reward for epsilon = 0.35
cumulative_reward_35 <- cumsum(result_sim_agg_eps_35$df_results_of_policy$click)
average_reward_35 <- cumulative_reward_35 / (n_sim)
df_eps_35_agg <- data.frame(t = 1:n_sim, epsilon = 0.35, avg_reward = average_reward_35)

# Calculate cumulative average reward for epsilon = 0.40
cumulative_reward_40 <- cumsum(result_sim_agg_eps_40$df_results_of_policy$click)
average_reward_40 <- cumulative_reward_40 / (n_sim)
df_eps_40_agg <- data.frame(t = 1:n_sim, epsilon = 0.4, avg_reward = average_reward_40)

# Calculate cumulative average reward for epsilon = 0.45
cumulative_reward_45 <- cumsum(result_sim_agg_eps_45$df_results_of_policy$click)
average_reward_45 <- cumulative_reward_45 / (n_sim)
df_eps_45_agg <- data.frame(t = 1:n_sim, epsilon = 0.45, avg_reward = average_reward_45)

```

### E2.5 Epsilon Performance - Aggregated Data 

Finally, the graph provides the analysis of the performance of the
epsilons and compare them based on aggregated data. The average reward
over time was chosen as the metric to compare the performance across
different values of the Epsilon. The results are found below:

```{r, visualization on aggregated data}
set.seed(123)

# Combine all data frames into one
df_all_eps <- rbind(df_eps_5_agg,df_eps_10_agg,df_eps_15_agg,df_eps_20_agg, df_eps_25_agg, df_eps_30_agg,df_eps_35_agg, df_eps_40_agg, df_eps_45_agg)

ggplot(df_all_eps, aes(x = t, y = avg_reward, color = factor(epsilon), group = factor(epsilon))) +
  geom_line(size = 1) +
  scale_color_manual(values = c("black","purple","#2ca02c", "red", "pink","orange", "blue","yellow","brown")) +
  scale_x_continuous(breaks = seq(0, 5000, by = 500)) +
  labs(title = "Average Reward Over Time for Different Epsilon-Greedy Policies with Aggregated Data",
       x = "Time Steps",
       y = "Average Reward",
       color = "Epsilon Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

Based on the graph presented above, at time step 4000, the epsilon value 0.4
reaches the highest average reward followed by 0.2, 0.25 and 0.35.

### E2.6 Optimal Item 

Following up on the chosen parameters in the previous section, the code
aims to identify the best item.

```{r, Best Item Recommendation with Epsilon-Greedy}
set.seed(123)

agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))

# Initialize matrices to store rewards (clicks) and trials (attempts)
rewards <- matrix(0, nrow = n_users, ncol = n_items)
trials <- matrix(0, nrow = n_users, ncol = n_items)

# Create a mapping from user and item to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))
item_map <- as.numeric(factor(agents_data$item_id))

# Implement the epsilon-greedy function
epsilon_greedy <- function(epsilon, rewards, trials, n_items) {
  if (runif(1) < epsilon) {
    # Explore: Choose a random item
    return(sample(1:n_items, 1))
  } else {
    # Exploit: Choose the item with the highest average reward
    mean_rewards <- rewards / pmax(trials, 1)  # Avoid division by zero
    return(which.max(mean_rewards))
  }
}

#Simulate epsilon-greedy for each user-item interaction
n_rounds <- 1000
epsilon <- 0.4 #this is the best-performed epsilon) 

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Choose an item using epsilon-greedy
    chosen_item <- epsilon_greedy(epsilon, rewards[user, ], trials[user, ], n_items)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    click_data <- user_data[user_data$item_id == unique(agents_data$item_id)[chosen_item], "click"]
    
    if (length(click_data) > 0) {
      # Update rewards and trials based on clicks
      reward <- sum(click_data)
      rewards[user, chosen_item] <- rewards[user, chosen_item] + reward
      trials[user, chosen_item] <- trials[user, chosen_item] + 1
    }
  }
}

best_item_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Display the results
results <- data.frame(
  Agent = unique(agents_data$Agent), 
  Best_Item = unique(agents_data$item_id)[best_item_for_user]
)
print(results)
```

The interpretation of the table above suggests that different users have
different preferences for best item based on Epsilon Greedy analysis.

### **E2.7 Optimal Position** 

This part suggests the best position for each agent based on historical
data

```{r, Best Position Recommendation with Epsilon-Greedy}
set.seed(123)

agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

n_users <- length(unique(agents_data$Agent))
n_positions <- length(unique(agents_data$position))

# Initialize matrices to store rewards (clicks) and trials (attempts)
rewards <- matrix(0, nrow = n_users, ncol = n_positions)
trials <- matrix(0, nrow = n_users, ncol = n_positions)

# Create a mapping from user and position to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))
position_map <- as.numeric(factor(agents_data$position))

#Simulate epsilon-greedy for each user-position interaction
n_rounds <- 1000
epsilon <- 0.4

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Choose a position using epsilon-greedy
    chosen_position <- epsilon_greedy(epsilon, rewards[user, ], trials[user, ], n_positions)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    click_data <- user_data[user_data$position == unique(agents_data$position)[chosen_position], "click"]
    
    if (length(click_data) > 0) {
      reward <- sum(click_data)
      rewards[user, chosen_position] <- rewards[user, chosen_position] + reward
      trials[user, chosen_position] <- trials[user, chosen_position] + 1
    }
  }
}

#  Identify the best position for each user
best_position_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

#  Display the results
results <- data.frame(
  Agent = unique(agents_data$Agent), 
  Best_Position = unique(agents_data$position)[best_position_for_user]
)
print(results)

```

The table above presents the best position for each agent. Overall, we
can see that the position 2 is the most common one (similarly to the
distribution per position histogram).

### **E2.8 Optimal Combination** 

```{r,  Best Item-Position Combination with Epsilon-Greedy}
set.seed(123)

agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

#Initialize variables
n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))
n_positions <- length(unique(agents_data$position))

# Create all possible combinations of item_id and position
item_position_combinations <- expand.grid(item_id = unique(agents_data$item_id), position = unique(agents_data$position))
n_combinations <- nrow(item_position_combinations)

# Initialize matrices to store rewards and trials
rewards <- matrix(0, nrow = n_users, ncol = n_combinations)
trials <- matrix(0, nrow = n_users, ncol = n_combinations)

# Function to map a specific item_id and position to its combination index
get_combination_index <- function(item_id, position) {
  which(item_position_combinations$item_id == item_id & item_position_combinations$position == position)
}

#Simulate epsilon-greedy for each user-item_position combination interaction
n_rounds <- 1000
epsilon <- 0.1

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    chosen_combination <- epsilon_greedy(epsilon, rewards[user, ], trials[user, ], n_combinations)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    chosen_item <- item_position_combinations$item_id[chosen_combination]
    chosen_position <- item_position_combinations$position[chosen_combination]
    
    click_data <- user_data[user_data$item_id == chosen_item & user_data$position == chosen_position, "click"]
    
    if (length(click_data) > 0) {
      reward <- sum(click_data)
      rewards[user, chosen_combination] <- rewards[user, chosen_combination] + reward
      trials[user, chosen_combination] <- trials[user, chosen_combination] + 1
    }
  }
}

# Identify the best item-position combination for each user
best_combination_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Display the results
best_item_position <- item_position_combinations[best_combination_for_user, ]
results <- data.frame(Agent = unique(agents_data$Agent), Best_Item = best_item_position$item_id, Best_Position = best_item_position$position)

print(results)

```

# VIII Thompson Sampling

## TS1 Identifying Optimal Initial Parameters

Based on the summary of mean_click per arm, it can be concluded that
there are many more failures than successes since the mean_click values are
close to zero. These parameters represent the prior belief about the
success rate, where $\alpha$ represents the number of observed successes
and $\beta$ represents the number of observed failures. A higher initial
$\beta$ reflects a bias towards failure. With a slight bias towards
failure, 20 different pairs of alphas and betas were chosen to be
explored, from (1,10) for alpha and (1,100) for beta.

After comparing the result with various different pairs of alphas and
betas, the best pair of alphas and betas were chosen to be (2,70),
(1,90) and (3,90) for this dataset. We will use this for further
analysis. The results of the other pairs can be found in the appendix.

```{r, TS parameter tuning}
set.seed(123)

# Define alphas and betas for sensitivity analysis
alphas <- c(2,1,3)
betas <-  c(70,90,90)

# Set simulation parameters
size_sim <- 4442 # Number of impressions per simulation
n_sim <- 10       # Number of simulations

# Create bandit for 20 arms
bandit_Zozo_20 <- OfflineReplayEvaluatorBandit$new(formula = click ~ combination_index,
                                                   data = df_filtered_agents,
                                                   randomize = FALSE)

# Initialize a list to store results
results_list <- list()

# Loop over alphas and betas to create Thompson Sampling policies and agents
for (i in seq_along(alphas)) {
  
  # Create Thompson Sampling policy for current alpha and beta
  TS <- ThompsonSamplingPolicy$new(alpha = alphas[i], beta = betas[i])

  # Create the agent for 20 arms
  agent_TS_zozo_20 <- Agent$new(TS, bandit_Zozo_20)

  # Run the simulator for Thompson Sampling
  simulator_20 <- Simulator$new(agent_TS_zozo_20, 
                              horizon = size_sim, 
                              simulations = n_sim)

  # Run the simulator
  history_TS_zozo_20 <- simulator_20$run()

  # Store the results and tag with current alpha and beta values
  results <- history_TS_zozo_20$data %>%
    mutate(alpha = alphas[i], beta = betas[i])
  
  results_list[[i]] <- results  # Add results to the list
}

# Combine all the results into a single dataframe
df_TS_zozo_20 <- bind_rows(results_list)

```

```{r, TS max obs}
set.seed(123)

# Create a combined alpha_beta column
df_TS_zozo_20 <- df_TS_zozo_20 %>%
  mutate(alpha_beta = paste0("alpha=", alpha, "_beta=", beta))

# Now group by alpha_beta and sim to calculate max_t
df_TS_zozo_20_max_t <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  summarize(max_t = max(t)) # Get the maximum time step (t) for each alpha-beta pair and sim

# View the results
print(df_TS_zozo_20_max_t)

x <- min(df_TS_zozo_20_max_t$max_t)
y <- max(df_TS_zozo_20_max_t$max_t)
print(x)
print(y)
```

The results show that the maximum number of rounds across the 10
simulations for each pair of alpha and betas varied between 58 and 95.
While most simulations fall within a narrow range, this variation
suggests differences in exploration patterns, where some simulations
explored more arms or took more actions before reaching the end of the
dataset.

```{r, TS ggplot}
set.seed(123)

#Max of observations, adjusted to the number of observations per simulation and the size of the dataset used
max_obs <- x

#Dataframe transformation
df_history_agg <- df_TS_zozo_20 %>%
  group_by(sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  # Calculate cumulative reward over time for each sim
  group_by(t) %>%
  summarise(avg_cumulative_reward = mean(cumulative_reward),  # Average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim),  # Standard error
            .groups = 'drop') %>%
  mutate(cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
         cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward) %>%
  filter(t <= max_obs)

#Dataframe transformation: cumulative reward calculations and confidence intervals
df_history_agg <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  # Calculate cumulative reward over time for each sim
  group_by(alpha_beta, t) %>%
  summarise(
    avg_cumulative_reward = mean(cumulative_reward),  # Average cumulative reward
    se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n()),  # Standard error
    .groups = 'drop'
  ) %>%
  mutate(
    cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
    cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward
  ) %>%
  filter(t <= max_obs)

# Plot the average cumulative rewards over time for each alpha-beta pair
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  labs(title = "Average Cumulative Rewards Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

# Plot Average Cumulative Reward with 95% CI
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0),  # Ensure lower CI does not go below 0
                  ymax = cumulative_reward_upper_CI, 
                  fill = alpha_beta), alpha = 0.1) +  # Confidence interval ribbon
  labs(title = "Average Cumulative Rewards with 95% CI Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair', 
       fill = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

This graph indicates that the alpha-beta pair of alpha=2 and beta=70
achieves the highest average cumulative reward, outperforming other
configurations over time. This suggests that it is the most effective
combination for maximizing average cumulative rewards. However, the wide
confidence intervals associated with this pair also suggest a higher
degree of uncertainty in the data, preventing a definitive conclusion
about its superiority.

## TS2 True CTR's 

In Thompson Sampling, the true click-through rate (true CTR) represents
the actual probability that a user will click on a given option (or arm)
when it is shown. It is calculated as the total number of clicks divided
by the total number of times that option was displayed. In our case, we
can know this because we have offline data available, which allows us to
directly observe and calculate the true CTR for each arm.

```{r, true ctr}
set.seed(123)

# Calculate the true CTR for each arm
true_CTRs <- df_filtered_agents %>%
  group_by(combination_index) %>%
  summarize(
    total_clicks = sum(click),    
    total_shown = n(),            
    true_CTR = total_clicks / total_shown 
  )

print(true_CTRs)

```

## TS3 Results

Following up on the chosen parameters of the Optimal Parameters (Section
VIII, TS1). The algorithm chooses the optimal item, position and
combination with the parameters presented as Alpha = 2 Beta = 70. The
results are computer below:

### TS3.1 Optimal Item 

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize variables
n_agents <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))

# Create all possible combinations of item_id and agent
item_agent_combinations <- expand.grid(item_id = unique(agents_data$item_id), 
                                       Agent = unique(agents_data$Agent))

# Initialize matrices to store rewards and trials for Thompson Sampling
rewards_thompson <- matrix(0, nrow = n_agents, ncol = n_items)
trials_thompson <- matrix(0, nrow = n_agents, ncol = n_items)

# Map agent and item_id to matrix indices
agent_indices <- match(agents_data$Agent, unique(agents_data$Agent))
item_indices <- match(agents_data$item_id, unique(agents_data$item_id))

# Adjustable parameters for Thompson Sampling
alpha_thompson <- 2  # Alpha parameter for Thompson Sampling
beta_thompson <- 70   # Beta parameter for Thompson Sampling

# Thompson Sampling function with adjustable alpha and beta
thompson_sampling <- function(successes, trials, alpha = alpha_thompson, beta = beta_thompson) {
  sapply(1:length(successes), function(i) {
    rbeta(1, successes[i] + alpha, trials[i] - successes[i] + beta)
  })
}

# Step 3: Run Thompson Sampling for each agent
for (i in 1:nrow(agents_data)) {
  agent <- agent_indices[i]
  item <- item_indices[i]
  
  # Get reward (click)
  reward <- agents_data$click[i]
  
  # Update reward and trial counts
  rewards_thompson[agent, item] <- rewards_thompson[agent, item] + reward
  trials_thompson[agent, item] <- trials_thompson[agent, item] + 1
}

# Step 4: For each agent, identify the best item (arm) using Thompson Sampling
best_items <- sapply(1:n_agents, function(agent) {
  thompson_probs <- thompson_sampling(rewards_thompson[agent, ], 
                                      trials_thompson[agent, ], 
                                      alpha = alpha_thompson, 
                                      beta = beta_thompson)
  # Identify the item with the highest probability
  best_item_index <- which.max(thompson_probs)
  return(unique(agents_data$item_id)[best_item_index])
})

result_table <- data.frame(Agent = unique(agents_data$Agent), Best_Item = best_items)
print(result_table)

```

### TS3.2 Optimal Position 

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize variables
n_agents <- length(unique(agents_data$Agent))        
n_positions <- length(unique(agents_data$position))  

# Initialize matrices to store rewards and trials for Thompson Sampling
rewards_thompson <- matrix(0, nrow = n_agents, ncol = n_positions)
trials_thompson <- matrix(0, nrow = n_agents, ncol = n_positions)

# Map agent and position to matrix indices
agent_indices <- match(agents_data$Agent, unique(agents_data$Agent))
position_indices <- match(agents_data$position, unique(agents_data$position))

# Adjustable parameters for Thompson Sampling
alpha_thompson <- 2  
beta_thompson <- 70   

# Thompson Sampling function with adjustable alpha and beta
thompson_sampling <- function(successes, trials, alpha = alpha_thompson, beta = beta_thompson) {
  sapply(1:length(successes), function(i) {
    rbeta(1, successes[i] + alpha, trials[i] - successes[i] + beta)
  })
}

# Step 3: Run Thompson Sampling for each agent and position (treated as arms)
for (i in 1:nrow(agents_data)) {
  agent <- agent_indices[i]
  position <- position_indices[i]
  
  # Get reward (click)
  reward <- agents_data$click[i]
  
  # Update reward and trial counts for the agent-position pair
  rewards_thompson[agent, position] <- rewards_thompson[agent, position] + reward
  trials_thompson[agent, position] <- trials_thompson[agent, position] + 1
}

# Step 4: For each agent, identify the best position (arm) using Thompson Sampling
best_positions <- sapply(1:n_agents, function(agent) {
  thompson_probs <- thompson_sampling(rewards_thompson[agent, ], 
                                      trials_thompson[agent, ], 
                                      alpha = alpha_thompson, 
                                      beta = beta_thompson)
  # Identify the position with the highest probability
  best_position_index <- which.max(thompson_probs)
  return(unique(agents_data$position)[best_position_index])
})

result_table <- data.frame(Agent = unique(agents_data$Agent), Best_Position = best_positions)
print(result_table)


```

### TS3.3 Optimal Combination 

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize variables
n_agents <- length(unique(agents_data$Agent))             # Number of unique agents
n_items <- length(unique(agents_data$item_id))            # Number of unique items
n_positions <- length(unique(agents_data$position))       # Number of unique positions
n_arms <- n_items * n_positions                           # Total number of arms (item_id-position combinations)

# Create all possible combinations of item_id and position (60 arms)
item_position_combinations <- expand.grid(item_id = unique(agents_data$item_id), 
                                          position = unique(agents_data$position))

# Initialize matrices to store rewards and trials for Thompson Sampling
rewards_thompson <- matrix(0, nrow = n_agents, ncol = n_arms)
trials_thompson <- matrix(0, nrow = n_agents, ncol = n_arms)

# Map agent, item_id, and position to matrix indices
agent_indices <- match(agents_data$Agent, unique(agents_data$Agent))
item_position_indices <- match(paste(agents_data$item_id, agents_data$position), 
                               paste(item_position_combinations$item_id, item_position_combinations$position))

# Adjustable parameters for Thompson Sampling
alpha_thompson <- 2  # Alpha parameter for Thompson Sampling
beta_thompson <- 70   # Beta parameter for Thompson Sampling

# Thompson Sampling function with adjustable alpha and beta
thompson_sampling <- function(successes, trials, alpha = alpha_thompson, beta = beta_thompson) {
  sapply(1:length(successes), function(i) {
    rbeta(1, successes[i] + alpha, trials[i] - successes[i] + beta)
  })
}

# Step 3: Run Thompson Sampling for each agent and item-position combination (arm)
for (i in 1:nrow(agents_data)) {
  agent <- agent_indices[i]
  item_position <- item_position_indices[i]
  
  # Get reward (click)
  reward <- agents_data$click[i]
  
  # Update reward and trial counts for the agent-item-position pair
  rewards_thompson[agent, item_position] <- rewards_thompson[agent, item_position] + reward
  trials_thompson[agent, item_position] <- trials_thompson[agent, item_position] + 1
}

# Step 4: For each agent, identify the best item-position combination (arm) using Thompson Sampling
best_combinations <- lapply(1:n_agents, function(agent) {
  thompson_probs <- thompson_sampling(rewards_thompson[agent, ], 
                                      trials_thompson[agent, ], 
                                      alpha = alpha_thompson, 
                                      beta = beta_thompson)
  # Identify the combination with the highest probability
  best_combination_index <- which.max(thompson_probs)
  best_combination <- item_position_combinations[best_combination_index, ]
  return(best_combination)
})

best_combinations_df <- do.call(rbind, best_combinations)
result_table <- data.frame(Agent = unique(agents_data$Agent), 
                           Best_Item = best_combinations_df$item_id, 
                           Best_Position = best_combinations_df$position)

print(result_table)





```

# IX UCB

The Upper Confidence Bound presents an analysis in which the agent
balances the tradeoff between *exploration* and *exploitation* based on
the degree of curiosity represented by c. This is similar to the value
of Alpha used in previous method representations. However, unlike other
methods, when faced with uncertainty, the agent chooses an arm based on
the optimistic higher bound around the mean estimate.

When an agent goes shopping, they are more interested in the
*exploitation* than the *exploration* value. Within the scope of this
analysis, it is assumed that there is only one agent of C = 0.1.

## U1 One Agent

```{r}
set.seed(123)

df <- read.csv("~/Downloads/zozo_noContext_20items_tiny.csv")

# Simulation parameters
n_arms <- length(unique(df$item_id))  # Number of unique arms (items)
n_simulations <- 1000  # Number of simulations (for a meaningful confidence interval)
sim_size <- 1000  # Number of rounds per simulation

# Create placeholders for the results of each simulation
all_sim_results <- vector("list", n_simulations)

# Get unique item_ids
item_ids <- unique(df$item_id)

# Store cumulative rewards across simulations
ucb_cum_rewards_all <- matrix(0, nrow = sim_size, ncol = n_simulations)

# Start simulations
set.seed(123)  # For reproducibility

for (sim in 1:n_simulations) {
  # Initialize for each simulation
  pull_counts <- rep(0, n_arms)  
  total_rewards <- rep(0, n_arms)  
  cum_rewards <- numeric(sim_size)  
  
  # Randomly sample sim_size rows from the dataset for the simulation
  sampled_data <- df[sample(nrow(df), sim_size, replace = TRUE), ]
  
  # Map item_id in sampled data to arm index
  arm_index <- match(sampled_data$item_id, item_ids)
  
  # Simulate for each round in the simulation
  for (round in 1:sim_size) {
    ucb_values <- ifelse(pull_counts == 0, Inf, 0)
    if (all(pull_counts > 0)) {
      avg_reward <- total_rewards / pull_counts
      exploration_value <- sqrt((2 * log(round)) / pull_counts)
      ucb_values <- avg_reward + 0.1 * exploration_value  
    }
    
    # Select the arm with the highest UCB value
    chosen_arm <- which.max(ucb_values)
    
    # Get the reward from the sampled data
    reward <- sampled_data$click[round]
    
    # Update total rewards and pull counts for the chosen arm
    total_rewards[chosen_arm] <- total_rewards[chosen_arm] + reward
    pull_counts[chosen_arm] <- pull_counts[chosen_arm] + 1
    
    # Calculate cumulative rewards
    if (round == 1) {
      cum_rewards[round] <- reward
    } else {
      cum_rewards[round] <- cum_rewards[round - 1] + reward
    }
  }
  
  # Store the cumulative rewards for this simulation
  ucb_cum_rewards_all[, sim] <- cum_rewards
}

# Calculate the average and standard deviation of cumulative rewards across simulations
ucb_avg_cum_rewards <- rowMeans(ucb_cum_rewards_all)
ucb_std_cum_rewards <- apply(ucb_cum_rewards_all, 1, sd)

# Calculate 95% confidence intervals
lower_ci <- ucb_avg_cum_rewards - 1.96 * (ucb_std_cum_rewards / sqrt(n_simulations))
upper_ci <- ucb_avg_cum_rewards + 1.96 * (ucb_std_cum_rewards / sqrt(n_simulations))

# Prepare data for ggplot
results_df <- data.frame(
  time = 1:sim_size,
  avg_cum_rewards = ucb_avg_cum_rewards,
  lower_ci = lower_ci,
  upper_ci = upper_ci
)

```

```{r}
# Create ggplot with 95% confidence intervals
ggplot(results_df, aes(x = time, y = avg_cum_rewards)) +
  geom_line(color = "blue", size = 1) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2, fill = "blue") +
  labs(title = "UCB Cumulative Rewards with 95% Confidence Interval",
       x = "Time (Rounds)",
       y = "Cumulative Reward") +
  theme_minimal()
```

## U2 Multiple Agents

However, it is unrealistic to think that there is only one agent (C=0.1)
accessing the website. This is why it is important to compute the UCB
The algorithm analyzes different policies and assesses which one performs
better over time.

### U2.1 Code for Agents

```{r}
set.seed(123)

# Initialize simulation parameters
n_arms <- length(unique(df$item_id))  
n_simulations <- 100  
sim_size <- 1000  
n_agents <- 5  

# Create a list to store results for all agents
all_agent_results <- vector("list", n_agents)

# Define exploration parameters (C) for each agent
agent_exploration_params <- c(0.1, 0.25, 0.5, 0.75, 1)  

# Get unique item_ids
item_ids <- unique(df$item_id)

# Start simulations for each agent
set.seed(123)  # For reproducibility

for (agent in 1:n_agents) {
  # Placeholder for storing cumulative rewards for each simulation for this agent
  agent_sim_results <- matrix(0, nrow = sim_size, ncol = n_simulations)

  # Run simulations for this agent
  C <- agent_exploration_params[agent]  # Set exploration parameter for this agent
  
  for (sim in 1:n_simulations) {
    # Initialize for each simulation
    pull_counts <- rep(0, n_arms)  # Track how many times each arm has been pulled
    total_rewards <- rep(0, n_arms)  # Total rewards for each arm
    cumulative_rewards <- numeric(sim_size)  # Track cumulative rewards for each round

    # Randomly sample `sim_size` rows from the dataset for the simulation
    sampled_data <- df[sample(nrow(df), sim_size, replace = TRUE), ]
    
    # Map item_id in sampled data to arm index
    arm_index <- match(sampled_data$item_id, item_ids)
    
    # Simulate for each round in the simulation
    for (round in 1:sim_size) {
      ucb_values <- ifelse(pull_counts == 0, Inf, 0)
      if (all(pull_counts > 0)) {
        avg_reward <- total_rewards / pull_counts
        exploration_value <- sqrt((2 * log(round)) / pull_counts)
        ucb_values <- avg_reward + C * exploration_value  
      }

      # Select the arm with the highest UCB value
      chosen_arm <- which.max(ucb_values)
      
      # Get the reward from the sampled data (click column)
      reward <- sampled_data$click[round]

      # Update total rewards and pull counts for the chosen arm
      total_rewards[chosen_arm] <- total_rewards[chosen_arm] + reward
      pull_counts[chosen_arm] <- pull_counts[chosen_arm] + 1

      # Update cumulative reward
      if (round == 1) {
        cumulative_rewards[round] <- reward
      } else {
        cumulative_rewards[round] <- cumulative_rewards[round - 1] + reward
      }
    }

    # Store cumulative rewards for this simulation
    agent_sim_results[, sim] <- cumulative_rewards
  }

  # Store all simulation results
  all_agent_results[[agent]] <- data.frame(
    time = 1:sim_size,
    mean_cum_reward = rowMeans(agent_sim_results),
    std_cum_reward = apply(agent_sim_results, 1, sd),
    lower_ci = rowMeans(agent_sim_results) - 1.96 * apply(agent_sim_results, 1, sd) / sqrt(n_simulations),
    upper_ci = rowMeans(agent_sim_results) + 1.96 * apply(agent_sim_results, 1, sd) / sqrt(n_simulations),
    agent = paste("Agent", agent)
  )
}

# Combine all agents' results into one dataframe
combined_results <- do.call(rbind, all_agent_results)

```

### U2.2 Comparison between Agents

```{r}
set.seed(123)

ggplot(combined_results, aes(x = time, y = mean_cum_reward, color = agent)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = agent), alpha = 0.3, color = NA) +
  labs(title = "Comparison of Agents' Average Cumulative Rewards with 95% Confidence Interval",
       x = "Time (Rounds)",
       y = "Average Cumulative Reward",
       color = "Agent",
       fill = "Agent") +
  theme_minimal()
```

The results conclude that the Agent (C=0.5) seems to be the most
reactive customer to the costume advertisements they are shown.
Additionally, the UCB classifier varies in effectiveness among the
agents. However, the degree of exploration was assigned to each agent.
It performed the analysis assuming that each row is a different user.
However, there can be a case in which the same user interacts with the
website multiple times.

In reality, each agent (user) has common characteristics or unique user
features which model their behavior. The dataset provided by ZOZO
stores assigns each user a unique set of four different features. This
is particularly important when addressing the fact that some users
access or interact with the website multiple times. The analysis of the
agents performed above is executed under the assumption that every row
is a different user. Which is not the case. Hence, it is important to
group the interaction by the user, meaning aggregating the dataset by
different agents and be able to assume how many times did an agent
react or participated. This addresses the concept of Heterogeneity.

## U3 Results

### U3.1 Optimal Item 

The UCB can be an effective Machine Learning method for training a
Multi-Armed Bandit within the context of the ZOZO website. Firstly, it
is important to identify which item is better for which agent,
disregarding position. This is seen with the code below:

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize the required variables
n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))

# Initialize matrices to store rewards (clicks) and trials (attempts)
rewards <- matrix(0, nrow = n_users, ncol = n_items)  
trials <- matrix(0, nrow = n_users, ncol = n_items)   

# Create a mapping from user and item to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))  
item_map <- as.numeric(factor(agents_data$item_id))  

# Step 3: Implement the UCB function
ucb <- function(rewards, trials, t, c = 2) {
  # UCB score: mean reward + exploration term
  mean_rewards <- rewards / pmax(trials, 1)  
  exploration_term <- sqrt(c * log(t + 1) / pmax(trials, 1))  
  ucb_scores <- mean_rewards + exploration_term
  return(ucb_scores)
}

# Step 4: Simulate UCB for each user-item interaction
n_rounds <- 1000  # Number of UCB rounds

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Calculate UCB scores for all items for the current user
    ucb_scores <- ucb(rewards[user, ], trials[user, ], t)
    
    # Choose the item with the highest UCB score
    chosen_item <- which.max(ucb_scores)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    
    # Find if the chosen item was clicked by the user
    click_data <- user_data[user_data$item_id == unique(agents_data$item_id)[chosen_item], "click"]
    
    if (length(click_data) > 0) {
      reward <- sum(click_data)  
      rewards[user, chosen_item] <- rewards[user, chosen_item] + reward
      trials[user, chosen_item] <- trials[user, chosen_item] + 1
    }
  }
}

# Step 5: Identify the best item for each user
best_item_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Step 6: Display the results
results <- data.frame(
  Agent = unique(agents_data$Agent), 
  Best_Item = unique(agents_data$item_id)[best_item_for_user]
)
print(results)

```

### U3.2 Optimal Position 

Secondly, regardless of the item, it is important to understand which
position is better for active Customers. This is seen with the
table below:

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize the required variables
n_users <- length(unique(agents_data$Agent))
n_positions <- length(unique(agents_data$position))  

# Initialize matrices to store rewards (clicks) and trials (attempts)
rewards <- matrix(0, nrow = n_users, ncol = n_positions)  
trials <- matrix(0, nrow = n_users, ncol = n_positions)   

# Create a mapping from user and position to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))  
position_map <- as.numeric(factor(agents_data$position))  

# Step 3: Implement the UCB function
ucb <- function(rewards, trials, t, c = 2) {
  # UCB score: mean reward + exploration term
  mean_rewards <- rewards / pmax(trials, 1)  
  exploration_term <- sqrt(c * log(t + 1) / pmax(trials, 1))  
  ucb_scores <- mean_rewards + exploration_term
  return(ucb_scores)
}

# Step 4: Simulate UCB for each user-position interaction
n_rounds <- 1000  

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Calculate UCB scores for all positions for the current user
    ucb_scores <- ucb(rewards[user, ], trials[user, ], t)
    
    # Choose the position with the highest UCB score
    chosen_position <- which.max(ucb_scores)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    
    # Find if the chosen position was clicked by the user
    click_data <- user_data[user_data$position == unique(agents_data$position)[chosen_position], "click"]
    
    if (length(click_data) > 0) {
      # If there is a click interaction, update rewards and trials
      reward <- sum(click_data)  
      rewards[user, chosen_position] <- rewards[user, chosen_position] + reward
      trials[user, chosen_position] <- trials[user, chosen_position] + 1
    }
  }
}

# Step 5: Identify the best position for each user
best_position_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Step 6: Display the results
results <- data.frame(
  Agent = unique(agents_data$Agent), 
  Best_Position = unique(agents_data$position)[best_position_for_user]
)
print(results)

```

### U3.3 Optimal Combination 

In general, the analysis has identified which Item and which Position is
better suggested for users. However, it disregards the possibility of a
combination. Now each combination of item_id and position is regarded as
a new arm. Hence, the program is run with a 30-armed bandit (3
[Position] x 20 [Item_ID]). The results are as follows:

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize variables
n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id))
n_positions <- length(unique(agents_data$position))

# Create all possible combinations of item_id and position
item_position_combinations <- expand.grid(item_id = unique(agents_data$item_id), position = unique(agents_data$position))
n_combinations <- nrow(item_position_combinations)

# Initialize matrices to store rewards and trials
rewards <- matrix(0, nrow = n_users, ncol = n_combinations)  
trials <- matrix(0, nrow = n_users, ncol = n_combinations)

# Create a mapping from user, item_id, and position to matrix indices
user_map <- as.numeric(factor(agents_data$Agent))  

# Function to map a specific item_id and position to its combination index
get_combination_index <- function(item_id, position) {
  which(item_position_combinations$item_id == item_id & item_position_combinations$position == position)
}

# Step 3: Implement the UCB function
ucb <- function(rewards, trials, t, c = 2) {
  # UCB score: mean reward + exploration term
  mean_rewards <- rewards / pmax(trials, 1)  
  exploration_term <- sqrt(c * log(t + 1) / pmax(trials, 1))  
  ucb_scores <- mean_rewards + exploration_term
  return(ucb_scores)
}

# Step 4: Simulate UCB for each user-item_position combination interaction
n_rounds <- 1000  # Number of UCB rounds

for (t in 1:n_rounds) {
  for (user in 1:n_users) {
    # Calculate UCB scores for all item-position combinations for the current user
    ucb_scores <- ucb(rewards[user, ], trials[user, ], t)
    
    # Choose the combination with the highest UCB score
    chosen_combination <- which.max(ucb_scores)
    
    # Simulate the click based on the actual data
    user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
    chosen_item <- item_position_combinations$item_id[chosen_combination]
    chosen_position <- item_position_combinations$position[chosen_combination]
    
    # Find if the chosen item-position combination was clicked by the user
    click_data <- user_data[user_data$item_id == chosen_item & user_data$position == chosen_position, "click"]
    
    if (length(click_data) > 0) {
      # If there is a click interaction, update rewards and trials
      reward <- sum(click_data)  # Sum of all clicks for the user-item-position combination
      rewards[user, chosen_combination] <- rewards[user, chosen_combination] + reward
      trials[user, chosen_combination] <- trials[user, chosen_combination] + 1
    }
  }
}

# Step 5: Identify the best item-position combination for each user
best_combination_for_user <- apply(rewards / pmax(trials, 1), 1, which.max)

# Step 6: Display the results
best_item_position <- item_position_combinations[best_combination_for_user, ]
results <- data.frame(Agent = unique(agents_data$Agent), Best_Item = best_item_position$item_id, Best_Position = best_item_position$position)

print(results)

```

# X Analysis

## A1.1 Reported Suggestion per Method 

| Active |   | Position |   |   | Item_ID |   |
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
| **Agent** | **Epsilon** | **Thompson** | **UCB** | **Epsilon** | **Thompson** | **UCB** |
| 11 | [2]{.underline} | 1 | [2]{.underline} | 8 | [20]{.underline} | [15]{.underline} |
| 105 | [2]{.underline} | [2]{.underline} | [2]{.underline} | [15]{.underline} | [20]{.underline} | [15]{.underline} |
| 133 | [2]{.underline} | 1 | [2]{.underline} | [15]{.underline} | 8 | [15]{.underline} |
| 137 | [2]{.underline} | [2]{.underline} | [2]{.underline} | 10 | 10 | [15]{.underline} |
| 217 | 3 | 1 | 1 | [15]{.underline} | 9 | 9 |
| 361 | 1 | [2]{.underline} | 1 | 7 | 7 | 7 |
| 486 | [2]{.underline} | 3 | [2]{.underline} | 14 | 19 | 14 |
| 721 | [2]{.underline} | 3 | [2]{.underline} | [15]{.underline} | 14 | [15]{.underline} |
| **Chosen** | **2** | **2** | **2** | **15** | **20** | **15** |

## A1.2 Batch Sensitivity

```{r}
set.seed(123)

# Step 1: Load the dataset
agents_data <- read.csv("agents_data_encoded.csv", stringsAsFactors = FALSE)

# Step 2: Initialize variables
n_users <- length(unique(agents_data$Agent))
n_items <- length(unique(agents_data$item_id)) 
n_positions <- length(unique(agents_data$position))

# Create all possible combinations of item_id and position
item_position_combinations <- expand.grid(item_id = unique(agents_data$item_id), 
                                          position = unique(agents_data$position))
n_combinations <- nrow(item_position_combinations)

# Initialize matrices to store rewards and trials for UCB, Thompson Sampling, and Epsilon Greedy
rewards_ucb <- matrix(0, nrow = n_users, ncol = n_combinations)
trials_ucb <- matrix(0, nrow = n_users, ncol = n_combinations)
rewards_thompson <- matrix(0, nrow = n_users, ncol = n_combinations)
trials_thompson <- matrix(0, nrow = n_users, ncol = n_combinations)
rewards_epsilon <- matrix(0, nrow = n_users, ncol = n_combinations)
trials_epsilon <- matrix(0, nrow = n_users, ncol = n_combinations)

# Function to map a specific item_id and position to its combination index
get_combination_index <- function(item_id, position) {
  which(item_position_combinations$item_id == item_id & item_position_combinations$position == position)
}

# UCB function with adjustable exploration parameter C
ucb <- function(rewards, trials, t, C = C_ucb) {
  mean_rewards <- rewards / pmax(trials, 1)
  exploration_term <- sqrt(C * log(t + 1) / pmax(trials, 1))
  ucb_scores <- mean_rewards + exploration_term
  return(ucb_scores)
}

# Thompson Sampling function with adjustable alpha and beta
thompson_sampling <- function(successes, trials, alpha = alpha_thompson, beta = beta_thompson) {
  sapply(1:length(successes), function(i) {
    rbeta(1, successes[i] + alpha, trials[i] - successes[i] + beta)
  })
}

# Epsilon Greedy function with adjustable epsilon
epsilon_greedy <- function(rewards, trials, epsilon = epsilon_value) {
  if (runif(1) < epsilon) {
    return(sample(1:length(rewards), 1))  # Exploration: choose random arm
  } else {
    mean_rewards <- rewards / pmax(trials, 1)
    return(which.max(mean_rewards))  # Exploitation: choose best arm
  }
}

# Step 3: Define batch sizes and simulation parameters
batch_sizes <- seq(10, 100, by = 10)  # Batch sizes ranging from 10 to 1000, increment by 50
n_rounds <- 1000  # Total rounds for simulation

# Adjustable parameters for each method
C_ucb <- 2          
alpha_thompson <- 1  
beta_thompson <- 1   
epsilon_value <- 0.1  

# Initialize a list to store rewards for each batch size
rewards_list <- list()

# Step 4: Run simulations for UCB, Thompson Sampling, and Epsilon Greedy
for (batch_size in batch_sizes) {
  ucb_total_rewards <- 0
  thompson_total_rewards <- 0
  epsilon_total_rewards <- 0
  
  for (t in seq(1, n_rounds, by = batch_size)) {
    # Collect batch interactions
    for (b in 1:batch_size) {
      for (user in 1:n_users) {
        user_data <- agents_data[agents_data$Agent == unique(agents_data$Agent)[user], ]
        
        # UCB
        ucb_scores <- ucb(rewards_ucb[user, ], trials_ucb[user, ], t, C_ucb)
        chosen_ucb <- which.max(ucb_scores)
        chosen_item <- item_position_combinations$item_id[chosen_ucb]
        chosen_position <- item_position_combinations$position[chosen_ucb]
        click_data <- user_data[user_data$item_id == chosen_item & user_data$position == chosen_position, "click"]
        
        if (length(click_data) > 0) {
          reward <- sum(click_data)
          rewards_ucb[user, chosen_ucb] <- rewards_ucb[user, chosen_ucb] + reward
          trials_ucb[user, chosen_ucb] <- trials_ucb[user, chosen_ucb] + 1
          ucb_total_rewards <- ucb_total_rewards + reward
        }

        # Thompson Sampling
        thompson_probs <- thompson_sampling(rewards_thompson[user, ], trials_thompson[user, ], alpha_thompson, beta_thompson)
        chosen_thompson <- which.max(thompson_probs)
        chosen_item <- item_position_combinations$item_id[chosen_thompson]
        chosen_position <- item_position_combinations$position[chosen_thompson]
        click_data <- user_data[user_data$item_id == chosen_item & user_data$position == chosen_position, "click"]
        
        if (length(click_data) > 0) {
          reward <- sum(click_data)
          rewards_thompson[user, chosen_thompson] <- rewards_thompson[user, chosen_thompson] + reward
          trials_thompson[user, chosen_thompson] <- trials_thompson[user, chosen_thompson] + 1
          thompson_total_rewards <- thompson_total_rewards + reward
        }

        # Epsilon Greedy
        chosen_epsilon <- epsilon_greedy(rewards_epsilon[user, ], trials_epsilon[user, ], epsilon_value)
        chosen_item <- item_position_combinations$item_id[chosen_epsilon]
        chosen_position <- item_position_combinations$position[chosen_epsilon]
        click_data <- user_data[user_data$item_id == chosen_item & user_data$position == chosen_position, "click"]
        
        if (length(click_data) > 0) {
          reward <- sum(click_data)
          rewards_epsilon[user, chosen_epsilon] <- rewards_epsilon[user, chosen_epsilon] + reward
          trials_epsilon[user, chosen_epsilon] <- trials_epsilon[user, chosen_epsilon] + 1
          epsilon_total_rewards <- epsilon_total_rewards + reward
        }
      }
    }
  }
  
  # Store the average rewards (total rewards divided by number of rounds) for each method for the current batch size
  rewards_list[[as.character(batch_size)]] <- c(UCB = ucb_total_rewards / n_rounds,
                                                Thompson = thompson_total_rewards / n_rounds,
                                                Epsilon_Greedy = epsilon_total_rewards / n_rounds)
}

# Step 5: Convert the list to a data frame with the desired format
results_table <- do.call(cbind, rewards_list)
row.names(results_table) <- c("UCB", "Thompson", "Epsilon Greedy")
results_table <- rbind("Algorithm" = colnames(results_table), results_table)
colnames(results_table) <- paste0("Batch_Size_", batch_sizes)

# Step 6: Final results table
print(results_table)

# Step 7: Calculate the variance of average rewards across batch sizes
reward_variances <- apply(results_table[-1, ], 1, function(x) var(as.numeric(x)))

# Display the variances
reward_variances

```

## A1.3 Comparison between Methods

From the aforementioned table, it can be identified that Thompson
Sampling seems to be the weakest method of the chosen three. This is only
within the context of batching. Batching is a determining element of the
effectiveness of a model, given that it is associated with the
processing costs of the algorithm. There are two other methods which can
be further compared which are the Epsilon-Greedy, and the Upper
Confidence Bound.

Epsilon-Greedy surpasses the UCB algorithm for average total rewards in
every single batching. However, it has a more significant variance
(5.26) than the UCB counterpart (0.10). Hence, in situations where the
precision of the model is important, depending on the chosen batching
size, it is debatable which model is best.

### A1.3.1 UCB and Epsilon

Combining the methodology in Epsilon Greedy (Section VII) and UCB
(Section IX), one can analyse the performance of each method. For each
UCB Agent, there is an "Epsiloner Doppelganger", simulating the same
degree of *curiosity*. For example, UCB Agent (C=0.5) has an Epsiloner
Dopperganger (E=0.5). They choose to model, in the same environment, the
behavior of this doppelganger and observe which machine learning method
yields the highest average cumulative frequency.

```{r}
set.seed(123)

# Load the dataset
df <- read.csv("~/Downloads/zozo_noContext_20items_tiny.csv")

simulate_reward <- function(arm, data) {
  reward <- data$click[data$item_id == arm]
  sample(reward, 1)
}

# Parameters
n_arms <- length(unique(df$item_id))  

# Function to run UCB and Epsilon-Greedy and compare their performances
run_simulation <- function(n_simulations, sim_size, C_values, epsilon_values) {
  
  results <- data.frame()  # Initialize empty data frame to store results for all agents
  
  # Iterate over the agents (each with a different combination of C and epsilon)
  for (agent in 1:length(C_values)) {
    
    C <- C_values[agent]  # Get the UCB exploration parameter for this agent
    epsilon <- epsilon_values[agent]  # Get the epsilon for Epsilon-Greedy for this agent
    
    # Initialize matrices to store cumulative rewards for each policy
    ucb_cum_rewards_all <- matrix(0, nrow = sim_size, ncol = n_simulations)
    eg_cum_rewards_all <- matrix(0, nrow = sim_size, ncol = n_simulations)

    # Run simulations
    for (sim in 1:n_simulations) {

      # Initialize UCB variables
      ucb_pull_counts <- rep(0, n_arms)  
      ucb_total_rewards <- rep(0, n_arms)  
      ucb_cumulative_rewards <- numeric(sim_size)  

      # Initialize Epsilon-Greedy variables
      eg_pull_counts <- rep(0, n_arms)  
      eg_total_rewards <- rep(0, n_arms)  
      eg_cumulative_rewards <- numeric(sim_size)  

      for (round in 1:sim_size) {

        #### UCB Algorithm ####
        ucb_values <- ifelse(ucb_pull_counts == 0, Inf, 0)  # UCB for arms not yet pulled
        if (all(ucb_pull_counts > 0)) {
          ucb_avg_reward <- ucb_total_rewards / ucb_pull_counts
          ucb_exploration_value <- sqrt((2 * log(round)) / ucb_pull_counts)
          ucb_values <- ucb_avg_reward + C * ucb_exploration_value
        }

        chosen_ucb_arm <- which.max(ucb_values)
        reward_ucb <- simulate_reward(chosen_ucb_arm, df)

        ucb_total_rewards[chosen_ucb_arm] <- ucb_total_rewards[chosen_ucb_arm] + reward_ucb
        ucb_pull_counts[chosen_ucb_arm] <- ucb_pull_counts[chosen_ucb_arm] + 1

        if (round == 1) {
          ucb_cumulative_rewards[round] <- reward_ucb
        } else {
          ucb_cumulative_rewards[round] <- ucb_cumulative_rewards[round - 1] + reward_ucb
        }

        #### Epsilon-Greedy Algorithm ####
        if (runif(1) < epsilon) {
          chosen_eg_arm <- sample(1:n_arms, 1)  
        } else {
          eg_avg_reward <- ifelse(eg_pull_counts == 0, 0, eg_total_rewards / eg_pull_counts)
          chosen_eg_arm <- which.max(eg_avg_reward)  
        }

        reward_eg <- simulate_reward(chosen_eg_arm, df)

        eg_total_rewards[chosen_eg_arm] <- eg_total_rewards[chosen_eg_arm] + reward_eg
        eg_pull_counts[chosen_eg_arm] <- eg_pull_counts[chosen_eg_arm] + 1

        if (round == 1) {
          eg_cumulative_rewards[round] <- reward_eg
        } else {
          eg_cumulative_rewards[round] <- eg_cumulative_rewards[round - 1] + reward_eg
        }
      }

      # Store cumulative rewards for this simulation
      ucb_cum_rewards_all[, sim] <- ucb_cumulative_rewards
      eg_cum_rewards_all[, sim] <- eg_cumulative_rewards
    }

    # Compute the average and standard deviation of cumulative rewards across simulations
    ucb_avg_cum_rewards <- rowMeans(ucb_cum_rewards_all)
    eg_avg_cum_rewards <- rowMeans(eg_cum_rewards_all)
    
    ucb_std_cum_rewards <- apply(ucb_cum_rewards_all, 1, sd)
    eg_std_cum_rewards <- apply(eg_cum_rewards_all, 1, sd)
    
    # Calculate 95% confidence intervals
    ucb_lower_ci <- ucb_avg_cum_rewards - 1.96 * (ucb_std_cum_rewards / sqrt(n_simulations))
    ucb_upper_ci <- ucb_avg_cum_rewards + 1.96 * (ucb_std_cum_rewards / sqrt(n_simulations))
    
    eg_lower_ci <- eg_avg_cum_rewards - 1.96 * (eg_std_cum_rewards / sqrt(n_simulations))
    eg_upper_ci <- eg_avg_cum_rewards + 1.96 * (eg_std_cum_rewards / sqrt(n_simulations))
    
    # Prepare the data for ggplot (store UCB and Epsilon-Greedy results)
    results <- rbind(results,
                     data.frame(
                       time = 1:sim_size,
                       Policy = paste("UCB (C =", C, ")"),
                       AverageCumulativeReward = ucb_avg_cum_rewards,
                       LowerCI = ucb_lower_ci,
                       UpperCI = ucb_upper_ci,
                       Agent = paste("Agent", agent)
                     ),
                     data.frame(
                       time = 1:sim_size,
                       Policy = paste("Epsilon-Greedy (ε =", epsilon, ")"),
                       AverageCumulativeReward = eg_avg_cum_rewards,
                       LowerCI = eg_lower_ci,
                       UpperCI = eg_upper_ci,
                       Agent = paste("Agent", agent)
                     )
    )
  }

  # Return the results data frame
  return(results)
}

# Simulation parameters
n_simulations <- 100  
sim_size <- 1000  

# Values of C (UCB) and epsilon (Epsilon-Greedy) for 5 agents
C_values <- c(0.1, 0.25, 0.5, 0.75, 1.0)
epsilon_values <- c(0.1, 0.25, 0.5, 0.75, 1.0)

# Run the simulation
results <- run_simulation(n_simulations, sim_size, C_values, epsilon_values)


```

### A1.3.2 Visualization of Analysis between the Epsiloners and the UCB'ers across agents

```{r}
#ggplot to compare UCB and Epsilon-Greedy policies for all agents
ggplot(results, aes(x = time, y = AverageCumulativeReward, color = Policy, fill = Policy)) +
  geom_line(size = 1) +
  geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI), alpha = 0.2) +
  facet_wrap(~ Agent) +
  labs(title = "Comparison of UCB and Epsilon-Greedy Policies (with 95% CI)",
       x = "Time (Rounds)",
       y = "Average Cumulative Reward",
       color = "Policy") +
  theme_minimal()
```

The aforementioned statistics, performed in the Raw Dataset, exemplify
that for each agent with a particular degree of *exploration.* From the
five chosen values (0.1, 0.25, 0.5, 0.75, 1) for each agent, it can be
seen how for the first two agents the difference is significant. As the
value increases the difference between the two methods becomes less
significant. Given that there isn't a significant difference across
methods, except for the first two agents where UCB outperforms Epsilon,
UCB can be considered an optimal method to train the model.

# XI Overview of Methodology

As discussed in the Introduction (Section I), the purpose of the
the analysis is to explore the differences across three Machine Learning
methods for Multi-Armed Bandit Models, and identify which combination of
position and item is an optimal suggestion to maximize the CTR of Zozo's
website. Additionally, exploring how these results are sensitive to
batch sizes, aggregation and parameters.

Each method individually identified the Optimal Combination for item and
position. The parameters chosen were the best-performing parameters that
were identified in the sub-sections prior to deriving this conclusion.

#### Epsilon-Greedy

Within the context of Epsilon-Greedy, it was identified that the best
performing Epsilon, measured by the average cumulative rewards across
samples was the parameter with an Epsilon of (E=0.1). This Epsilon was
used to compute the combination suggestion for the aggregated relevant
agents in Optimal Combination (E2.8). Where, overall, the optimal
combination was **Position 2** and **Item 15.**

#### Thompson Sampling 

The Thompson Sampling method identified in Identifying Optimal
Parameters (Section TS1) that the optimal algorithm was that with the
initial (Alpha = 2) and (Beta = 70). This is given the low rate of clicks
amongst the dataset. This assumption used as a parameter to compute the
Optimal Combination (TS3.3) where it can be concluded that across the 8
relevant agents, the combination that maximizes the CTR is **Position
2** and **Item 20.**

#### **Upper Confidence Bound (UCB)** 

Within the context of the UCB, it is assumed that the C-value is agent
dependent and overall the exploration value of (C= 0.5) represents the
best-performing agent. This parameter was used to run the Optimal
Combination (U3.3) where it was identified that the best combination is
**Position 2** and **Item 15.** Equivalent to what was computed by the
Epsilon Greedy Algorithm.

# XII Answering the Research Question

The three methods agree that the most common CTR-maximizing position in
2. Two of the methods agree that **Position 2** and **Item 15**
represents the CTR maximizing combination with regard to their best
performing parameters and the given aggregated dataset in which the 8
potential active customers clicked. Hence, this can be assumed to be the
best item combination.

With regards to the implementation of an algorithm for the Multi-Armed
Bandit for the Zozo website. From the chosen three methods, it is
relative which one performs better as it is objective and dependent on
the purpose of the research. If the company is looking to invest in a
quick response that updates every 10 or 20 cases, the UCB is the optimal
choice given its consistent average reward shown by its low variance as
discussed in Batch Sensitivity (A1.2). If the company is not willing to
invest in an algorithm that updates so often, then certainly the Epsilon
Greedy is an attractive option given that its assumption of which arm
performs best is better identified with a larger sample size. Given that
it can observe and identify better which arm is to be exploited.

When choosing between the two methods, if the company expects customers
with low levels of exploration then UCB inherently outperforms the
Epsilon as presented by the graphs in Analysis (Section X, A1.3.2).
However, as the exploration degree increases the difference between
models become insignificant.

The research purpose highlighted the analysis of the results and their
sensitivity to different parameters, batch size, and aggregation. The
analysis has proven the different scenarios of an optimal arm given an
aggregated and raw dataset, with different parameters and batch size. It
has concluded that the best combination to maximize the CTR, agreed by
Epsilon Greedy and UCB, is **Position 2** and **Item 15.** This
combination will yield the highest amount of clicks if reached out by
the agents.

This is consistent with the data presented in the Descriptive Statistics
(Section IV) where **Item 15** is amongst the top three performing items
(D1.2), and **Position 2** is the most common among the three (D1.3).

# XII Future Analysis 

The results presented in Answering the Research Question (Section XI)
have identified the best combination to maximize the CTR. Additionally,
in the Table presented in the Analysis (Section X, A1.1) the optimal
combination for each active agent was computed. It must be noted that
this is for the aggregated dataset and under the assumption of the
Active Agent. For future analysis, the cosine similarity between these
agents across its 4 unique components can be computed to identify the
similarity between them. Additionally, given that each unique agent has
optimal agents, non-active users (Agents who didn't click at least
once) can suggest the optimal combination for the agent with the
closest cosine relation. This is a potential improvement for the model,
that given the scope of the analysis, is only presented as a potential
addition.

# XIV Reflection (Strengths and Limitations)

The analysis was able to identify an optimal combination across
different training methods. It was performed and reviewed separately and
then the codes were combined in order to compare them adjusting the
logic of progression. The analysis was primarily done without black box
packages. This was the response to the feedback received in which the
performance of the model could be hindered by the lack of flexibility
the Contextual Package brings. This being said, choosing this approach
increased the amount of mistakes resulting from simple naming functions.
Additionally, the fact that the code was divided and written by
different coders increased the amount of errors in the methodology when
it was being computed on another device. Lastly, given the limited
computing power, the medium-sized dataset was chosen which restricts the
reliability of the results with its 80-armed counterpart.

Initially, all the algorithms were designed to be computed with the same
amount of simulations and size. However, each group member adjusted this
in the way they considered best in order to obtain significant results.
This is something that must be taken into account when assessing the
reliability of the analysis. Lastly, the "random" values chosen for the
variables which allowed it from the coder's perspective are object to
slight biases which limit the analysis to more specific results for
other parameters.

# XV Appendix 

### Parameter Tuning for Thompsom Sampling

```{r, parameter tuning alpha}
set.seed(123)
alphas <- c(1,1,1,1,1,1,1,1,1,1)
betas <-  c(10,20,30,40,50,60,70,80,90,100)

size_sim <- 4442
n_sim <- 10        

bandit_Zozo_20 <- OfflineReplayEvaluatorBandit$new(formula = click ~ combination_index,
                                                   data = df_filtered_agents,
                                                   randomize = FALSE)
results_list <- list()


for (i in seq_along(alphas)) {
  TS <- ThompsonSamplingPolicy$new(alpha = alphas[i], beta = betas[i])
  agent_TS_zozo_20 <- Agent$new(TS, bandit_Zozo_20)
  simulator_20 <- Simulator$new(agent_TS_zozo_20, 
                              horizon = size_sim, 
                              simulations = n_sim)

  history_TS_zozo_20 <- simulator_20$run()
  
  results <- history_TS_zozo_20$data %>%
    mutate(alpha = alphas[i], beta = betas[i])
  results_list[[i]] <- results 
}

df_TS_zozo_20 <- bind_rows(results_list)
df_TS_zozo_20 <- df_TS_zozo_20 %>%
  mutate(alpha_beta = paste0("alpha=", alpha, "_beta=", beta))

# Now group by alpha_beta and sim to calculate max_t
df_TS_zozo_20_max_t <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  summarize(max_t = max(t)) 
print(df_TS_zozo_20_max_t)

x <- min(df_TS_zozo_20_max_t$max_t)
max_obs <- x
df_history_agg <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  
  group_by(alpha_beta, t) %>%
  summarise(
    avg_cumulative_reward = mean(cumulative_reward), 
    se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim), 
    .groups = 'drop'
  ) %>%
  mutate(
    cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
    cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward
  ) %>%
  filter(t <= max_obs)
# Plot the average cumulative rewards over time for each alpha-beta pair
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  labs(title = "Average Cumulative Rewards Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

# Plot Average Cumulative Reward with 95% CI
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0),  
                  ymax = cumulative_reward_upper_CI, 
                  fill = alpha_beta), alpha = 0.1) + 
  labs(title = "Average Cumulative Rewards with 95% CI Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair', 
       fill = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

In this graph, the confidence intervals for the different pairs overlap,
which indicates that the observed differences between them may not be
statistically significant. This overlap is largely a result of the
limited sample size in our dataset. When the dataset is small,
variability tends to increase, making it more difficult to detect
meaningful distinctions between groups. As a result, the confidence
intervals widen, and it becomes harder to confidently assert that any
observed differences are not due to random chance.

```{r, parameter tuning beta}
set.seed(123)
alphas <- c(1,2,3,4,5,6,7,8,9,10)
betas <-  c(70, 70, 70, 70, 70, 70, 70, 70, 70, 70  )

size_sim <- 4442
n_sim <- 10        

bandit_Zozo_20 <- OfflineReplayEvaluatorBandit$new(formula = click ~ combination_index,
                                                   data = df_filtered_agents,
                                                   randomize = FALSE)
results_list <- list()


for (i in seq_along(alphas)) {
  TS <- ThompsonSamplingPolicy$new(alpha = alphas[i], beta = betas[i])
  agent_TS_zozo_20 <- Agent$new(TS, bandit_Zozo_20)
  simulator_20 <- Simulator$new(agent_TS_zozo_20, 
                              horizon = size_sim, 
                              simulations = n_sim)

  history_TS_zozo_20 <- simulator_20$run()
  
  results <- history_TS_zozo_20$data %>%
    mutate(alpha = alphas[i], beta = betas[i])
  results_list[[i]] <- results 
}

df_TS_zozo_20 <- bind_rows(results_list)
df_TS_zozo_20 <- df_TS_zozo_20 %>%
  mutate(alpha_beta = paste0("alpha=", alpha, "_beta=", beta))

# Now group by alpha_beta and sim to calculate max_t
df_TS_zozo_20_max_t <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  summarize(max_t = max(t)) 
print(df_TS_zozo_20_max_t)

x <- min(df_TS_zozo_20_max_t$max_t)
max_obs <- x
df_history_agg <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  
  group_by(alpha_beta, t) %>%
  summarise(
    avg_cumulative_reward = mean(cumulative_reward), 
    se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim), 
    .groups = 'drop'
  ) %>%
  mutate(
    cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
    cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward
  ) %>%
  filter(t <= max_obs)
# Plot the average cumulative rewards over time for each alpha-beta pair
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  labs(title = "Average Cumulative Rewards Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

# Plot Average Cumulative Reward with 95% CI
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0),  
                  ymax = cumulative_reward_upper_CI, 
                  fill = alpha_beta), alpha = 0.1) + 
  labs(title = "Average Cumulative Rewards with 95% CI Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair', 
       fill = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r, parameter tuning}
set.seed(123)
alphas <- c(1,2,3,4,5,6,7,8,9,10)
betas <-  c(90, 90, 90, 90, 90, 90, 90, 90, 90, 90)

size_sim <- 4442
n_sim <- 10        

bandit_Zozo_20 <- OfflineReplayEvaluatorBandit$new(formula = click ~ combination_index,
                                                   data = df_filtered_agents,
                                                   randomize = FALSE)
results_list <- list()


for (i in seq_along(alphas)) {
  TS <- ThompsonSamplingPolicy$new(alpha = alphas[i], beta = betas[i])
  agent_TS_zozo_20 <- Agent$new(TS, bandit_Zozo_20)
  simulator_20 <- Simulator$new(agent_TS_zozo_20, 
                              horizon = size_sim, 
                              simulations = n_sim)

  history_TS_zozo_20 <- simulator_20$run()
  
  results <- history_TS_zozo_20$data %>%
    mutate(alpha = alphas[i], beta = betas[i])
  results_list[[i]] <- results 
}

df_TS_zozo_20 <- bind_rows(results_list)
df_TS_zozo_20 <- df_TS_zozo_20 %>%
  mutate(alpha_beta = paste0("alpha=", alpha, "_beta=", beta))

# Now group by alpha_beta and sim to calculate max_t
df_TS_zozo_20_max_t <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  summarize(max_t = max(t)) 
print(df_TS_zozo_20_max_t)

x <- min(df_TS_zozo_20_max_t$max_t)
max_obs <- x
df_history_agg <- df_TS_zozo_20 %>%
  group_by(alpha_beta, sim) %>%
  mutate(cumulative_reward = cumsum(reward)) %>%  
  group_by(alpha_beta, t) %>%
  summarise(
    avg_cumulative_reward = mean(cumulative_reward), 
    se_cumulative_reward = sd(cumulative_reward, na.rm = TRUE) / sqrt(n_sim), 
    .groups = 'drop'
  ) %>%
  mutate(
    cumulative_reward_lower_CI = avg_cumulative_reward - 1.96 * se_cumulative_reward,
    cumulative_reward_upper_CI = avg_cumulative_reward + 1.96 * se_cumulative_reward
  ) %>%
  filter(t <= max_obs)
# Plot the average cumulative rewards over time for each alpha-beta pair
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  # Line for average cumulative reward
  labs(title = "Average Cumulative Rewards Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability

# Plot Average Cumulative Reward with 95% CI
ggplot(data = df_history_agg, aes(x = t, y = avg_cumulative_reward, color = alpha_beta)) +
  geom_line(size = 1.5) +  
  geom_ribbon(aes(ymin = pmax(cumulative_reward_lower_CI, 0),  
                  ymax = cumulative_reward_upper_CI, 
                  fill = alpha_beta), alpha = 0.1) + 
  labs(title = "Average Cumulative Rewards with 95% CI Over Time by Alpha-Beta Pairs", 
       x = 'Time', 
       y = 'Average Cumulative Reward', 
       color = 'Alpha-Beta Pair', 
       fill = 'Alpha-Beta Pair') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r, cleaning the environment 8}
rm(batch_size, batch_sizes, history_TS_zozo_20, n_sim, simulator, size_sim, TS, df_TS_zozo_20, agent_TS_zozo_20, bandit_Zozo_20, click_counts, max_t_list, max_t, df_cumulative_rewards, results_list, df_TS_zozo_20_agg, df_TS_zozo_20_max_t, p1, p2, name)
```
