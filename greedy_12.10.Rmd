---
title: "Greedy  - V"
output: html_document
date: "2024-10-06"
---

### 1. load libraries

```{r setup, include=FALSE}
 if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

```

### 2. Load dataset

```{r}
ZOZOdf <-  read_csv("C:/Users/Lenovo/Desktop/zozo_Contex_80 items.csv/zozo_Context_80items.csv")

ZOZO_for_sim <- ZOZOdf %>%
  select(item_id, click)

ZOZO_for_sim$index <- 1:nrow(ZOZO_for_sim)
```

### 3. Distribution

```{r, distribution }
click_distribution <- ZOZO_for_sim %>%
  group_by(click) %>%
  summarise(count = n())

```

### 4. epsilon-Greedy Algorithm: notation

```{r}
df_practice <- ZOZO_for_sim[sample(1:nrow(ZOZO_for_sim), 5000), ] #get random observations
eps <- 0.05 #later in code we find out it is the best performing one

policy_greedy <- function(df, eps, n_arms=80){
  random_uniform_variable <- runif(1, min=0, max=1)
  
  # in epsilon % of cases, pick a random arm (exploration)
  if (random_uniform_variable < eps){
    # Sample uniform random arm between 1:n_arms
    chosen_arm <- sample(1:n_arms, 1)
  } else {
     df_reward_overview <- df %>%
      drop_na() %>%
      group_by(item_id) %>%
      summarise(reward_size = sum(click, na.rm=TRUE),
                sample_size = n(),
                success_rate = reward_size / sample_size)
    
    # Pick the arm with the highest success_rate from df_reward_overview
    chosen_arm <- df_reward_overview$item_id[which.max(df_reward_overview$success_rate)]
  }
  
  # Return the chosen arm
  return(chosen_arm)
}
# Get the chosen arm from the function
chosen_arm_practice <- policy_greedy(df_practice, eps)
print(paste0('The chosen arm is: ', chosen_arm_practice))

```

#### 4.1 sim greedy - simulates perfomance of an epsiloj-greedy policy

##### Part 1: create two dataframes, one with data before start of policy, and one with data after

##### Part 2: apply epsilon-greedy algorithm, updating at interval

```{r}
##PART 1
sim_greedy <- function(df, n_before_sim, n_sim, epsilon, interval=1){
  
  # define the number of observations of all data available
  n_obs <- nrow(df)

  if(n_sim > (n_obs-n_before_sim)){
    stop("The indicated size of the experiment is bigger than the data provided - shrink the size ")
  }

  # find n_before_sim random observations to be used before start policy
  index_before_sim <- sample(1:n_obs, n_before_sim)
  
  df_before_policy <- df[index_before_sim,]
  
  # save dataframe with all the results at t - to begin with those before the policy
  df_results_at_t <- df_before_policy %>%
    select(item_id, click)
  
  # create dataframe with data that we can sample from during policy
  df_during_policy <- df[-index_before_sim,]
  
  # dataframe where the results of storing the policy are stored
  df_results_policy <- data.frame(matrix(NA, nrow = n_sim, ncol = 2))
  colnames(df_results_policy) <- c('item_id', 'click')


##Part 2
  for (i in 1:n_sim){
    # update at interval
    if((i==1) || ((i %% interval)==0)){
      # Select the arm with your policy_greedy function and define the current arm
      chosen_arm <- policy_greedy(df_results_at_t, epsilon)
      current_arm <- chosen_arm
    } else {
      # In the case that we do not update, take the current arm
      chosen_arm <- current_arm
    }

    # select from the data for experiment the arm chosen
    df_during_policy_arm <- df_during_policy %>%
      filter(item_id == chosen_arm)
    
    # randomly sample from this arm and observe the reward
    if (nrow(df_during_policy_arm) == 0) {
      warning("You have run out of observations from a chosen arm")
      sampled_arm <- sample(1:n_obs, 1, replace = TRUE)
      reward <- df$click[sampled_arm]
    } else {
      sampled_arm <- sample(1:nrow(df_during_policy_arm), 1)
      reward <- df_during_policy_arm$click[sampled_arm]
    }
    
    # get a vector of results from chosen arm (item_id, reward)
    result_policy_i <- c(chosen_arm, reward)
    
    # add to dataframe to save the result
    df_results_policy[i,] <- result_policy_i
    # update the data.frame 
    df_results_at_t <- rbind(df_results_at_t, result_policy_i)
  }

  # save results in list
  results <- list(df_results_of_policy = df_results_policy,
                  df_sample_of_policy = df[-index_before_sim,])
  
  return(results)
}
  
 

```

#### 4.2 Parameters

```{r}
n_before_sim <- 500
n_sim <- 4500

set.seed(0)
```

#### 4.3 comparing epsilons: 0.05, 0.1, 0.2, 0.3, 0.4, 0.5

```{r}
epsilon_values <- c(0.05, 0.1, 0.2, 0.3, 0.4, 0.5)
```

#### 4.3.1.loop for epsalons

```{r}
results_sim_list <- list()

# Loop through each epsilon value
for (eps in epsilon_values) {
 
  results_sim_list[[paste0("eps_", eps)]] <- sim_greedy(ZOZO_for_sim,
                                                        n_before_sim = n_before_sim,
                                                        n_sim = n_sim,
                                                        epsilon = eps,
                                                        interval = 1)
}
```

```{r}
df_result_sims_avg <- data.frame(t = 1:n_sim)

# Loop epsilon results and cumulative averages
for (eps in epsilon_values) {
  result <- results_sim_list[[paste0("eps_", eps)]]
  
  cumulative_avg_reward <- cumsum(result$df_results_of_policy$click) / (1:n_sim)
  
  df_result_sims_avg[[paste0("eps_", eps)]] <- cumulative_avg_reward
}

# Melt the dataframe for ggplot visualization
df_result_sims_avg_melted <- melt(df_result_sims_avg, id = 't')

```

##### Epsilon performance comparison

```{r}
# Create the plot for cumulative average rewards
cumulative_avg_reward_plot <- ggplot(data = df_result_sims_avg_melted, aes(x = t, y = value, col = variable)) +
  geom_line() +
  theme_bw() +
  xlab('Time') +
  ylab('Cumulative Average Reward') +
  scale_color_manual(name = expression(epsilon),
                     values = c("blue", "green", "red", "orange", "purple","brown"),
                     labels = as.character(epsilon_values)) +
  ggtitle("Cumulative Average Reward for Different Epsilon Values")

# Print the plot
print(cumulative_avg_reward_plot)
```

```{r}
# Create the plot for cumulative average rewards
cumulative_avg_reward_plot <- ggplot(data = df_result_sims_avg_melted, aes(x = t, y = value, col = variable)) +
  geom_line() +
  theme_bw() +
  xlab('Time') +
  ylab('Cumulative Average Reward') +
  scale_color_manual(name = expression(epsilon),
                     values = c("blue", "green", "red", "orange", "purple","brown"),
                     labels = as.character(epsilon_values)) +
  ggtitle("Cumulative Average Reward for Different Epsilon Values")

# Print the plot
print(cumulative_avg_reward_plot)
```


The graph displayed above shows the cumulative average rewards over time for different epsilon values (0.05, 0.1, 0.2, 0.3, 0.4, 0.5). By looking closely and more detailed at graph, it is noticeable that ,at the beginning, all of the epsilons declines in their rewards but then they are stabilizing. As the time passed, the epsilon curve (0.05) is outperforming the rest of the epsilon curve values, meaning that it has the highest cumulative average reward throughout the simulation,proving that epsilon 0.05 is the best epsilon value among displayed.

##### Best performed 0.05 + CI 95%

```{r}
# Function confidence intervals
calc_confidence_interval <- function(rewards, n) {
  mean_reward <- mean(rewards)
  std_error <- sd(rewards) / sqrt(n)
  lower_bound <- mean_reward - 1.96 * std_error
  upper_bound <- mean_reward + 1.96 * std_error
  return(c(lower_bound, upper_bound))
}

# Calculate cumulative average reward for epsilon = 0.05
result_eps_05 <- results_sim_list$eps_0.05
cumulative_avg_reward_05 <- cumsum(result_eps_05$df_results_of_policy$click) / (1:n_sim)

#confidence intervals 
lower_ci_05 <- sapply(1:n_sim, function(i) calc_confidence_interval(result_eps_05$df_results_of_policy$click[1:i], i)[1])
upper_ci_05 <- sapply(1:n_sim, function(i) calc_confidence_interval(result_eps_05$df_results_of_policy$click[1:i], i)[2])

df_eps_05 <- data.frame(
  t = 1:n_sim,
  avg_reward = cumulative_avg_reward_05,
  lower_ci = lower_ci_05,
  upper_ci = upper_ci_05
)

# Plot the cumulative average reward with confidence intervals for epsilon = 0.05

ggplot(df_eps_05, aes(x = t, y = avg_reward)) +
  geom_line(color = "blue") +  
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), fill = "light blue", alpha = 0.2) +  # Add confidence intervals
  theme_bw() +
  xlab('Time') +
  ylab('Cumulative Average Reward') +
  ggtitle(" Best performed  Eps = 0.05 ; 95% Confidence Interval")

```

### BATCHES SENSITIVITY
```{r, BATCHES SENTISITIVITY}

# Define different batch sizes (intervals between policy updates)
batch_sizes <- c(1, 5, 10, 20, 50, 100)

results_sim_batch_list <- list()

# Set epsilon to the best-performing value
epsilon <- 0.05

n_sim <- 4500

# Loop through each batch size
for (batch_size in batch_sizes) {
  results_sim_batch_list[[paste0("batch_", batch_size)]] <- sim_greedy(
    df = ZOZO_for_sim,
    n_before_sim = n_before_sim,
    n_sim = n_sim,
    epsilon = epsilon,
    interval = batch_size  # Varying the interval parameter
  )
}

# Create a dataframe for storing results
df_result_batches_avg <- data.frame(t = 1:n_sim)

# Loop through the batch size results and calculate cumulative averages
for (batch_size in batch_sizes) {
  result <- results_sim_batch_list[[paste0("batch_", batch_size)]]
  
  cumulative_avg_reward <- cumsum(as.numeric(result$df_results_of_policy$click)) / (1:n_sim)
  
  df_result_batches_avg[[paste0("batch_", batch_size)]] <- cumulative_avg_reward
}

# Melt the dataframe 
df_result_batches_avg_melted <- melt(df_result_batches_avg, id = 't')


```
```{r}
# Create the plot for cumulative average rewards across different batch sizes
ggplot(data = df_result_batches_avg_melted, aes(x = t, y = value, col = variable)) +
  geom_line() +
  theme_bw() +
  xlab('Time') +
  ylab('Cumulative Average Reward') +
  scale_color_manual(
    name = 'Batch Size',
    values = c("blue", "green", "red", "orange", "purple", "brown"),
    labels = as.character(batch_sizes)
  ) +
  ggtitle("Cumulative Average Reward for Different Batch Sizes")

```



