---
title: "Assignment 2"
author: "Tobi"
date: "2024-10-03"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Thompson Sampling for Multi-Armed Bandits

In this part of the assignment, we use the Thompson Sampling model for multi-armed bandit problem, which aims to explore all posterior distribution of each arm’s reward probability and then to select the arm with highest sampled probability value.  

### Download Libraries

```{r}

# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,
               ggplot2,
               devtools,
               BiocManager,
               contextual
               )

knitr::opts_chunk$set(echo = TRUE, eval = FALSE)

install.packages("devtools")
devtools::install_github("Nth-iteration-labs/contextual")
library(contextual)
```

### Download Dataset

```{r}

dataset <- read.csv("zozo_noContext_20items_tiny.csv") %>%
  select (item_id, click)

```

### Identify Number of Arms (Bandit)

In this dataset, each index item is identified as a separate arm, that is why we count the number of those items in order to determine the number of arms. 

```{r}

#Clothes are Bandits

n_bandits <- length(unique(dataset$item_id))

```

### Choosing an arm based on the Thompson Sampling Algorithm

The code presented below chooses an arm based on the Thompson Sampling Algorithm. Fistly, the alpha and beta are initialized per each arm, then the samples are drawn and the average reward for each arm is computed, finally the arm with the highest average reward is chosen. 

```{r}
# set the seed
set.seed(0)

# arm index
arm <- 1:n_bandits

# alpha per arm
alpha <- rep(1,n_bandits)

# beta per arm
beta <- rep(1, n_bandits)

# dataframe with alpha, beta per arm
df_alpha_beta <- data.frame(arm=arm,alpha = alpha, beta = beta)

# number of samples to draw
n_sample <- 100

# create dataframe with sampled values
df_sampled <- mapply(rbeta, n_sample, df_alpha_beta$alpha, df_alpha_beta$beta)

#IDENTIFY AVERAGE REWARDS
average_arm <- colMeans(df_sampled) #EASIER WAY

#CHOOSE THE HIGHEST REWARD AND ARM 
choice <- which.max(average_arm) #Choice of Arm

```

### Simulation on Zozo Data

```{r}
# set the seed
set.seed(0)

bandit_zozo<- OfflineReplayEvaluatorBandit$new(formula = click ~ item_id,
                                                   data = dataset,
                                                   randomize = FALSE)

# lets generate 20 simulations, each of size 10.000
size_sim <- 10000
n_sim<- 20

#Thompson Sampling policy object
TS          <- ThompsonSamplingPolicy$new()

# the contextual package works with 'agent' objects - which consist of a policy and a bandit
agent_TS <-  Agent$new(TS, bandit_zozo) 

simulator          <- Simulator$new(agent_TS, #Agent
                                      horizon= size_sim, #Size of each simulation
                                      do_parallel = TRUE, 
                                      simulations = n_sim, #Number of simulations
                                  )
  
history_TS_zozo            <- simulator$run()

#Results
df_TS_zozo <- history_TS_zozo$data %>%
  select(t, sim, choice, reward, agent) 

```

### Maximum Number of Simulations

```{r}
df_TS_zozo_max_t <- df_TS_zozo%>%
  group_by(sim) %>% # group by per agent
  summarize(max_t = max(t)) # get max t

```

### Plot of Average Cumulative Rewards for Thompson Sampling

In this part of the analysis, we aim to generate two graphs. Graph 1. shows the average cumulative reward (black line) over time, while Graph 2 upgrades Graph 1 by including the confidence intervals – upper and lower bond (grey shade area).  

```{r}
#Maximum_Obserbations
max_obs <- 500

#Data_Frame_Obserbations 
df_history_agg <- df_TS_zozo %>%
  group_by( sim) %>% # group by simulation
  mutate(cumulative_reward = cumsum(reward))%>% 
  group_by( t) %>% # group by timestep
  summarise(avg_cumulative_reward = mean(cumulative_reward), # average cumulative reward
            se_cumulative_reward = sd(cumulative_reward, na.rm=TRUE)/sqrt(n_sim)) %>% # SE + Confidence interval
  mutate(cumulative_reward_lower_CI =avg_cumulative_reward - 1.96*se_cumulative_reward,
         cumulative_reward_upper_CI =avg_cumulative_reward + 1.96*se_cumulative_reward)%>%
  filter(t <=max_obs)

# 1: Average cumulative rewards over time using the df_history_agg 

ggplot(data = df_history_agg, aes (x = t, y = avg_cumulative_reward)) + geom_line() 
 

#Plot_2 - Thompson Sampling Data with Confidence Interval at 95%

ggplot(data=df_history_agg, aes(x=t, y=avg_cumulative_reward)) +
  geom_line(size=1.5) + # create line
  geom_ribbon(aes(ymin=ifelse(cumulative_reward_lower_CI<0, 0,cumulative_reward_lower_CI) , 
                  ymax=cumulative_reward_upper_CI),
              alpha=0.1) +
  labs(x = 'Time', y='Cumulative Reward', color ='number of arms', fill='number of arms') +
  theme_bw() 

```
